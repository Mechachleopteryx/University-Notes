<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <title>CS240 | Anthony Zhang</title>
  <link rel="stylesheet" href="../css/base.css" type="text/css">
  <link rel="stylesheet" href="../css/note.css" type="text/css">
  <link rel="stylesheet" href="../highlight/styles/default.css">
  <link rel="stylesheet" href="../highlight/styles/paraiso-light.css">
  <script src="../highlight/highlight.pack.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
  <script src="../katex/katex.min.js" type="text/javascript"></script>
  <link rel="stylesheet" href="../katex/katex.min.css" />
  <script type="text/javascript">
  window.onload = function() {
    document.getElementsByClassName("status-banner")[0].style.display = "block";
    setTimeout(function() {
      renderMathElements(document.getElementsByClassName("math"));
      document.getElementsByClassName("status-banner")[0].style.display = "none";
    }, 50); // delay to allow status banner to show
  }

  function renderMathElements(mathElements) {
    var mathOptions = {
      macros: {
        "\\set": "\\left\\{ #1 \\right\\}",
        "\\tup": "\\left\\langle #1 \\right\\rangle",
        "\\abs": "\\left\\lvert #1 \\right\\rvert",
        "\\floor": "\\left\\lfloor #1 \\right\\rfloor",
        "\\ceil": "\\left\\lceil#1 \\right\\rceil",
        "\\mb": "\\mathbb{#1}",
        "\\rem": "\\operatorname{rem}",
        "\\ord": "\\operatorname{ord}",
        "\\sign": "\\operatorname{sign}",
        "\\imag": "\\bm{i}",
        "\\dee": "\\mathop{}\\!\\mathrm{d}",
        "\\lH": "\\overset{\\text{l'H}}{=}",
        "\\evalat": "\\left.\\left(#1\\right)\\right|",
        "\\sech": "\\operatorname{sech}",
        "\\spn": "\\operatorname{Span}",
        "\\proj": "\\operatorname{proj}",
        "\\prp": "\\operatorname{perp}",
        "\\refl": "\\operatorname{refl}",
        "\\magn": "\\left\\lVert #1 \\right\\rVert",
        "\\rank": "\\operatorname{rank}",
        "\\trace": "\\operatorname{trace}",
        "\\sys": "\\left[ #1 \\mid #2\\space \\right]",
        "\\range": "\\operatorname{Range}",
        "\\adj": "\\operatorname{adj}",
        "\\cof": "\\operatorname{cof}",
        "\\coord": "{\\left\\lbrack #1 \\right\\rbrack}_{#2}",
        "\\diag": "\\operatorname{diag}",
        "\\formlp": "\\operatorname{Form}(\\mathcal{L}^P)",
        "\\argmin": "\\operatorname{argmin}",
        "\\argmax": "\\operatorname{argmax}",
        "\\sgn": "\\operatorname{sgn}",

        // not yet available in KaTeX
        "\\bm": "\\mathbf", //wip: should be italic, but isn't
      },
    };
    for (var i=0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      try {
        katex.render(texText.data, mathElements[i], mathOptions);
      } catch (e) {
        console.error(e);
        console.log(mathElements[i]);
      }
    }
  }
  </script>
</head>
<body>
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-68271407-1', 'auto');
    ga('send', 'pageview');

  </script>
  <h1>Lecture Notes by <a href="/">Anthony Zhang</a>.</h1>
  <ul class="site_links">
    <li><a href="/blog/" class="page">blog</a></li>
    <span class="divider"></span>
    <li><a href="http://uberi.github.io/University-Notes" class="page">notes</a></li>
    <span class="divider"></span>
    <li><a href="/resume.pdf" class="page">résumé</a></li>
    <span class="divider"></span>
    <li><a href="https://github.com/Uberi" class="contact">github</a></li>
    <span class="divider"></span>
    <li><a href="https://www.linkedin.com/in/uberi/" class="contact">linkedin</a></li>
    <span class="divider"></span>
    <li><a href="mailto:me@anthonyz.ca" class="contact">email</a></li>
    <span class="divider"></span>
    <li><a href="https://www.facebook.com/anthony.zhang.user" class="contact">facebook</a></li>
    <span class="divider"></span>
    <li><a href="https://twitter.com/anthony926535" class="contact">twitter</a></li>
    <span class="divider"></span>
    <li><a href="https://keybase.io/uberi" class="info">public key</a></li>
  </ul>
<h1 id="cs240">CS240</h1>
<p>Data Structures and Data Management.</p>
<pre><code>Therese Beidl
Section 002
Email: beidl@uwaterloo.ca
Web: http://www.student.cs.uwaterloo.ca/~cs240
ISA: Joseph (Danny) Sitter
ISA Email: cs240@studennt.cs.uwaterloo.ca</code></pre>
<p>Section 1 and 2 are the regular classes, and section 3 is the enhanced section and has very different content.</p>
<p>Midterm at Thurs. Feb. 26 at 4:30 PM, worth 25%. Final exam worth 50%. Must pass weighted average of exam to pass the course. 5 assignments, each worth 5%.</p>
<h1 id="section">6/1/15</h1>
<p>Assignments is due on Tuesdays. Assignment 0 is due on Jan. 13.</p>
<p>Suppose we have a lot of data to keep track of. We could store it in an array/list, but depending on the type of the data, this might not be the best choice. Data structures should make it easy and efficient to perform the operations we need. For example, an English dictionary probably needs to be efficiently searched, but we don't really need to care about insertion and deletion since they're so rare.</p>
<p>The best data structure for something depends on the type of data we want to store. Our goal is to have a short running time and little memory.</p>
<p>In this course we will be performing theoretical analysis, developing ideas and pseudocode (and sometimes, implementations), and analyzing them using tools like big-O notation.</p>
<p>An Abstract Data Type is the idea of describing something by what it can do, not how it does it.</p>
<p>Required background includes arrays, linked lists, strings, stacks, queues, ADTs, recursive algorithms, trees, sorting algorithms (insertion, selection, quick, merge), binary search/BSTs, arithmetic series, geometric series, harmonic series (<span class="math inline">\frac 1 1 + \ldots + \frac 1 n = \ln n</span>).</p>
<p>In this course, <span class="math inline">\log n</span> is implicitly <span class="math inline">\log_2 n</span> - all logarithms are base 2 unless otherwise stated.</p>
<p>A <strong>problem</strong> is a description of a general situation and the desired outcome. For example, the sorting problem is &quot;given comparable values, put them in sorted order&quot;.</p>
<p>A <strong>problem instance</strong> is a particular input to a problem, like a particular array of numbers to sort for the sorting problem.</p>
<p>A <strong>problem solution</strong> is a change/process that, given the situation of a problem instance, results in the desired outcome. For example, the sorted array of numbers for the sorting problem.</p>
<p>We <strong>solve</strong> a problem by giving the correct algorithm for it. A solution is <strong>correct</strong> if it finds a solution for every possible input that can be given.</p>
<p>An <strong>algorithm</strong> is a finite description of a process that gives an answer (a solution that is not necessarily correct) for all possible instances of a problem.</p>
<p><strong>Efficiency</strong> usually refers to an algorithm's runtime, and sometimes its memory usage. It may also refer to things specific to the problem domain, like the number of comparisons done.</p>
<p>To solve a problem:</p>
<ol type="1">
<li>Design an algorithm.</li>
<li>Write down the main idea of the algorithm in plain prose. For example, &quot;for increasing <span class="math inline">i</span>, make <span class="math inline">A[0 \ldots i]</span> sorted by inserting <span class="math inline">A[i]</span> into a sorted <span class="math inline">A[0 \ldots i - 1]</span>&quot; for the sorting problem.</li>
<li><p>Optionally, write pseudocode - code that might not be a real language, and is something like a mix between prose and code. This is a more precise way of specifying an algorithm. Any consistent, precise, and clearly understandable language will be accepted as pseudocode.</p>
<pre><code>Preconditions: an array `A[0 ... n - 1]`$.
Postconditions: `A` is sorted
for i in 1 ... n - 1:
  value = A[i]
  j = i - 1
  while j &gt;= 0 and A[j] &gt; key:
    A[j + 1] = A[j]
    j = j - 1
  A[j + 1] = value</code></pre></li>
<li>Argue/prove that it is correct.</li>
<li>We can use formal correctness proofs, but this is often excessive to convince someone an algorithm is correct. Instead, we can simply give lots of invariants and prove that the algorithm terminates.</li>
<li>For example, for the above, we would say that for the inner loop, &quot;<code>A[j + 1 ... i]</code> contains items that are bigger than <code>value</code>, and is in sorted order&quot; is an invariant.</li>
<li>It is very important to prove that it terminates, especially recursive algorithms. We simply use the standard methods specified in CS245, but we can usually just write that each call or iteration gets some smaller input, and that small enough inputs cause the algorithm to terminate.</li>
<li>Analyze how good the algorithm is, in terms of efficiency and sometimes lower bounds.</li>
<li>For this we use time complexity/running-time analysis, memory usage analysis, and so on.</li>
<li>For example, the above code is known as Insertion Sort, which we already know to have a worst case time complexity of <span class="math inline">O(n^2)</span> and a best case time complexity of <span class="math inline">O(n)</span>.</li>
<li>Recall that formally, an algorithm being in <span class="math inline">O(f(n))</span> means that there exists a <span class="math inline">k &gt; 0</span> and <span class="math inline">m \ge 0</span> such that for all <span class="math inline">n &gt; m</span>, <span class="math inline">k \cdot f(n) &gt; T(n)</span> where <span class="math inline">T(n)</span> represents the number of constant time steps or amount of time that the algorithm needs to give an answer for an input of size <span class="math inline">n</span> - the running time function. This is written as <span class="math inline">T(n) \in O(f(n))</span>.</li>
<li><p>Repeat above steps until satisfactory algorithm is found.</p></li>
</ol>
<p>Only after this point would we implement/experimentally test the algorithm.</p>
<h1 id="section-1">8/1/15</h1>
<p>A <strong>timing function</strong> is a function <span class="math inline">S_p: \mathrm{Input} \to \mb{R}^+</span> where <span class="math inline">p</span> is the program we are running. For example, we can compare quicksort and mergesort by comparing <span class="math inline">S_{\text{mergesort}}(\tup{5, 6, 2, 3})</span> and <span class="math inline">S_{\text{quicksort}}(\tup{5, 6, 2, 3})</span>.</p>
<p>A timing function measures the amount of time a program takes for a given input.</p>
<p>Since computers are constantly changing and improving, comparing timing functions is not very meaningful since they will not stay consistent over differnt machines and over time as computers change. We want a machine-agnostic way to measure the amount of time a given program takes.</p>
<p>For example, Intel's core instruction set is RISC, with tons of extensions such as MMX, SSE, and FPU and GPU stuff. These add instructions such as <code>fsqrt</code> and similar. We assume that every instruction takes a finite, constant amount of time (interesting note).</p>
<p>We can instead measure the number of elementary operations, but it is difficult to define what an elementary operation actually is, and determining the exact number of elementary operations is a lot of work. For example, we might measure the number of clock cycles needed to run a program, but this would not make sense across things like RISC vs CISC machines. For example, if an architecture doesn't have a constant time division operation, we would probably need to implement one in software that would not have constant time.</p>
<p>In other words, we write our algorithms in pseudo-code, then count the number of primitive operations.</p>
<p>We now want to plot our timing function with respect to the size of the input <span class="math inline">\abs{\mathrm{Input}} \subseteq \mb{N}</span>. This allows us to figure out the behaviour of the function as the size of the input gets bigger. This is difficult since finding the timing function for arbitrary inputs is often difficult. Additionally, we don't necessarily care about the constants in the function output, just its behaviour on large inputs.</p>
<p>Usually we care about the worst case behaviour, to determine the worst possible behaviour in all cases. We may also be interested in the average case to see how things will perform in practice. Occasionally, we may also be interested in the best case - for example, in cryptography, we want to make sure the problem cannot be solved in less than some given amount of time.</p>
<p>The worst case behaviour is <span class="math inline">T_p(n) = \max \set{S_p(I) \middle| \abs{I} = n}</span>. The best case behaviour is <span class="math inline">T_p(n) = \max \set{S_p(I) \middle| \abs{I} = n}</span>. The average case behaviour is <span class="math inline">T_p(n) = \frac{\sum_{e \in R} i}{\abs{R}}</span> where <span class="math inline">R = \set{S_p(I) \middle| \abs{I} = n}</span>. We can now plot these functions to get the time bounds on our program.</p>
<p>The big-O notation is used to make life easier for computer scientists trying to calculate timing functions and to make comparing functions much easier. Refer to the CS136 notes for background. The <span class="math inline">O</span> in <span class="math inline">O(f(x))</span> is actually a capital omicron, bu nowadays the O is more common.</p>
<p>Comparing Big O is analogous to ordering in <span class="math inline">\mb{R}</span>. Just like how <span class="math inline">x \le y</span>, we might write <span class="math inline">f(x) \in O(g(x))</span>. What we care about is the trends of the function, not the actual numbers.</p>
<p>Big-O gives an upper bound behaviour on a function (analogous to <span class="math inline">x \le y</span>). Big-<span class="math inline">\Omega</span> gives a lower bound (analogous to <span class="math inline">x \ge y</span>). Big-<span class="math inline">\Theta</span> us the exact bounds - when the Big-<span class="math inline">O</span> is the same as the Big-<span class="math inline">\Omega</span> (analogous to <span class="math inline">x = y</span>).</p>
<p>There is also Little-<span class="math inline">o</span>, which is a non-inclusive upper bound (analogous to x &lt; y), and Little-<span class="math inline">\omega</span>, which is a non-inclusive lower bound (analogous to <span class="math inline">x &gt; y</span>). For little-o, instead of <span class="math inline">\exists c &gt; 0, \exists n_0 &gt; 0, n &gt; n_0 \implies f(n) \le cg(x)</span>, we have <span class="math inline">\forall c &gt; 0, \exists n_0 &gt; 0, n &gt; n_0 \implies f(n) &lt; cg(x)</span>.</p>
<p>There are also incomparable functions, like one that goes up and down arbitrarily. As a result, functions can only be partially ordered.</p>
<h1 id="section-2">13/1/15</h1>
<p>Big-O does not care about the values of the functions - only about how it grow as input values get large.</p>
<p>Prove that <span class="math inline">2n^2 + 3n + 11 \in O(n^2)</span>:</p>
<blockquote>
<p>We want to prove that <span class="math inline">2n^2 + 3n + 11 \le cn^2</span> for all <span class="math inline">n &gt; n_0</span>.<br />
Let <span class="math inline">n_0 = 10</span> and <span class="math inline">n \ge n_0</span>. Clearly <span class="math inline">2n^2 + 3n + 11 \le 2n^2 + 3n^2 + 11n^2</span> when <span class="math inline">n \ge 1</span> and <span class="math inline">2n^2 + 3n^2 + 11n^2 = 16n^2</span>.<br />
Then 16 is a possible value for <span class="math inline">c</span> and <span class="math inline">2n^2 + 3n + 11 \in O(n^2)</span>.</p>
</blockquote>
<p><span class="math inline">O(n^2 + \log n)</span> is not correct since it is not fully simplified.</p>
<p>Common time complexities include <span class="math inline">\Theta(1)</span> (constant), <span class="math inline">\Theta(\log n)</span> (logarithmic), <span class="math inline">\Theta(n)</span> (linear), <span class="math inline">\Theta(n \log n)</span> (pseudo-linear), <span class="math inline">\Theta(n^2)</span> (quadratic), <span class="math inline">\Theta(n^3)</span> (cubic), <span class="math inline">\Theta(n^k)</span> (polynomial), <span class="math inline">\Theta(2^n)</span> (exponential). In the real world, everything above <span class="math inline">O(n \log n)</span> tends to be rather bad performance.</p>
<p>It is not always the case that an algorithm with better time complexity than another is always better in all circumstances. For example, despite insertion sort being <span class="math inline">O(n^2)</span> and merge sort being <span class="math inline">O(n \log n)</span>, we often use insertion sort when sorting smaller lists since it is faster in practice when the input is small. Many practical sorting algorithms therefore drop down into insertion sort when the input is small. This is also why most people use insertion sort when sorting things in real life - it is very fast for real-world inputs, which are usually small.</p>
<p>Let <span class="math inline">L = \lim_{n \to \infty} \frac{f(n)}{g(n)}</span>. If <span class="math inline">L = 0</span>, <span class="math inline">f(n) \in o(g(n))</span>. If <span class="math inline">0 &lt; L &lt; \infty</span>, <span class="math inline">f(n) \in \Theta(g(n))</span>. Otherwise, <span class="math inline">L = \infty</span> and <span class="math inline">f(n) \in \omega(g(n))</span>. This is a useful way to prove orders for functions that would otherwise be difficult to prove by first principles. This is the <strong>limit order rule</strong>.</p>
<p>For example, if we want to prove that <span class="math inline">n^2 \log n \in o(n^3)</span>, we can do <span class="math inline">\lim_{n \to \infty} \frac{n^2 \log n}{n^3} = \lim_{n \to \infty} \frac{\log n}{n} \lH \lim_{n \to \infty} \frac{\frac 1 n}{1} = 0</span>, and use the fact that <span class="math inline">L = 0</span> to conclude that <span class="math inline">n^2 \log n \in o(n^3)</span>.</p>
<p>We can aso use this to prove that <span class="math inline">(\log n)^a \in o(n^b)</span> for any <span class="math inline">a, b &gt; 0</span>.</p>
<p>Also, <span class="math inline">f(n) \in \Omega(g(n)) \iff g(n) \in \Omega(f(n))</span>, <span class="math inline">f(n) \in O(g(n)) \iff g(n) \in \Omega(f(n))</span>, <span class="math inline">f(n) \in o(g(n)) \iff g(n) \in \omega(f(n))</span>, <span class="math inline">f(n) \in \Theta(g(n)) \iff g(n) \in O(f(n)) \wedge g(n) \in \Omega(f(n))</span>, <span class="math inline">f(n) \in o(g(n)) \implies f(n) \in O(g(n))</span>, <span class="math inline">f(n) \in o(g(n)) \implies f(n) \notin \Omega(g(n))</span>, <span class="math inline">f(n) \in \omega(g(n)) \implies f(n) \in \Omega(g(n))</span>, and <span class="math inline">f(n) \in \omega(g(n)) \implies f(n) \notin O(g(n))</span>. We can use these to simplify order notation expressions.</p>
<p>Also, <span class="math inline">O(f(n) + g(n)) = O(\max(f(n), g(n))) = \max(O(f(n)), O(g(n)))</span>. Also, orders are transitive - <span class="math inline">f(n \in O(g(n))) \wedge g(n) \in O(h(n)) \implies f(n) \in O(h(n))</span>.</p>
<p>In our pseudocode, we often use <span class="math inline">x \leftarrow 5</span> or <code>x := 5</code> to denote assignment, since <code>=</code> is ambiguous - it could potentially mean equality or assignment.</p>
<p>Math with orders works a lot like normal math: <span class="math inline">\Theta(1) \cdot (1 + \ldots + n) = \Theta(1) \cdot \frac{n^2 + n}{2} = \Theta(n^2 + n) = \Theta(n^2)</span>.</p>
<h1 id="section-3">15/1/15</h1>
<p>A priority queue is a data structure that stores items with keys that represent the priorities that the values have. It has two operations - insertion with a given key and deletion of the item with the maximum priority.</p>
<p>There are a number of ways we could implement a priority queue - an unsorted dynamic array (amortized <span class="math inline">O(1)</span> insert, <span class="math inline">O(n)</span> deletion), a sorted dynamic array, and something known as a heap. We don't want to use sorted order, since that makes insertion slow. We also dont't want to use unsorted order, since that makes deletion slow.</p>
<p>A <strong>heap</strong> is a data structure that allows us to implement a priority queue in a very efficient way. The main idea is that we would like to know very easily what the maximum element is, so we make all the other elements follow from it.</p>
<p>The basic idea behind the heap is that it is a tree where each element keeps track of candidates that can replace them when they're deleted, as children. The maximum item would therefore be the root.</p>
<p>Most commonly, we use binary heaps, which is a binary tree. Recall that this is a data structure consisting of nodes and links between node, such that each node has 2 or fewer children, and 1 or fewer parents.</p>
<p>Formally, a <strong>binary max-heap</strong> is a binary tree that satisfies the structural property and the max-heap property. There are no rules about the order of children, unlike a binary search tree.</p>
<p>The <strong>structural property</strong> is that all levels except the lowest level must be completely filled. Each node must have two children, except the lowest level, where nodes can have fewer than two children but <strong>must fill up from the left</strong> - in the lowest level, the left sibling of a node must have 2 children before we can insert a child into it.</p>
<p>The <strong>max-heap property</strong> is that for any node <span class="math inline">x</span> in the heap, the key of <span class="math inline">x</span> is less than or equal to the key of the parent of <span class="math inline">x</span>. As a result, the maximum node of a heap is always the root node. There is also a min-heap property where the key of <span class="math inline">x</span> is greater than or equal to the key of the parent of <span class="math inline">x</span>, used for min-heaps. In other words, the parent of any node in a heap must be bigger or equal to that node.</p>
<p>Insertion into a heap is easy because we can simply do a standard tree insertion, with certain constraints to ensure it satisfies the structural property and the heap property. This operation is <span class="math inline">\Theta(\log n)</span> because the structural property ensures that the height of the tree is <span class="math inline">\Theta(\log n)</span>.</p>
<p>Also, the height of a binary heap is <span class="math inline">\ceil{\log_2 n}</span>. The height of a tree is 1 less than the number of levels it has (1 less than the maximum depth). Levels start at 1 and increase with depth.</p>
<p>Proof:</p>
<blockquote>
<p>Let <span class="math inline">x</span> be a heap with <span class="math inline">n</span> nodes and height <span class="math inline">h</span>.<br />
Clearly, any binary tree of height <span class="math inline">h</span> must have <span class="math inline">n \le 2^{h + 1} - 1</span>, since each level <span class="math inline">i</span> has less than or equal to <span class="math inline">2^{i - 1}</span> nodes, and the total is <span class="math inline">n \le \sum_{i = 1}^{h + 1} 2^{i - 1} \le \frac{2^{h + 1} - 1}{2 - 1}</span>.<br />
Since <span class="math inline">n + 1 \le 2^{h + 1}</span>, <span class="math inline">h \ge \log(n + 1) - 1</span>.<br />
Clearly, <span class="math inline">n \ge 2^h</span>, by the structural property, since the lowest level is always <span class="math inline">h + 1</span> so each level <span class="math inline">i</span> before it has exactly <span class="math inline">2^{i - 1}</span> nodes, and the total for all levels except the lowest is <span class="math inline">2^h - 1</span>. Since the lowest level has 1 or more nodes, the total is <span class="math inline">n \ge (2^h - 1) + 1</span>, or <span class="math inline">2^h</span>.</p>
</blockquote>
<h1 id="section-4">20/1/15</h1>
<p>Binary heaps are trees, so we can store them as a tree structure with references and nodes. However, a more efficient way to store binary trees is as an array:</p>
<ul>
<li><span class="math inline">A</span> is an array representing the heap.</li>
<li><span class="math inline">A[0]</span> contains the root node.</li>
<li><span class="math inline">A[1], \ldots, A[2]</span> store the children of the root node.</li>
<li><span class="math inline">A[3], \ldots, A[6]</span> store the children of the children of the root nodes, and so on.</li>
</ul>
<p>Essentially, we first have the first level, then the second level, and so on. Formally, the left and right children of a node stored in the array at index <span class="math inline">i</span> are <span class="math inline">lc(i) = 2i + 1</span> and <span class="math inline">rc(i) = 2i + 2</span> (in a 0-indexed array). Likewise, the parent of any node is <span class="math inline">p(i) = \floor{\frac{i - 1} 2}</span>. This works because the leftmost node in level <span class="math inline">l</span> has index <span class="math inline">\sum_{k = 1}^{l - 1} 2^{k - 1}</span>.</p>
<p>In fact, this left and right parent system works for any binary tree, and even for <span class="math inline">n</span>-ary trees with a simple extension. However, it is not space efficient unless the tree obeys something like the structural property. The structural property ensures that the array is mostly filled, possibly except for part that stores the last level.</p>
<h3 id="heaps-as-priority-queues">Heaps as Priority Queues</h3>
<p>We will represent our heap as an array <span class="math inline">A</span> containing <span class="math inline">n</span> values, automatically expanded or shrunk as needed.</p>
<p>To <strong>insert</strong> into a heap, we first find the only location that the structural property allows us to insert a new node <span class="math inline">i</span> into. This could possibly violate the heap property since it is possible that <span class="math inline">i &gt; p(i)</span>, so if <span class="math inline">i &gt; p(i)</span>, we swap <span class="math inline">i</span> and <span class="math inline">p(i)</span> to get <span class="math inline">p(i)&#39;</span> and <span class="math inline">i&#39;</span>, where <span class="math inline">p(p(i)&#39;) = i&#39;</span>.</p>
<p>Now the heap property is satisfied for <span class="math inline">i&#39;</span> and <span class="math inline">p(i)&#39;</span>, and it isn't possible for <span class="math inline">p(i)&#39;</span>'s sibling to be greater than <span class="math inline">i&#39;</span> since it was less than the original parent <span class="math inline">p(i)</span>. However, it is possible that <span class="math inline">i&#39;</span> is now greater than it's new parent <span class="math inline">p(i&#39;)</span>, so we swap again if <span class="math inline">i&#39; &gt; p(i)&#39;</span>.</p>
<p>We repeat swapping the originally inserted node with its parents until either it is less than its parent, or it is the new root node. This is called &quot;bubbling up until it holds&quot;.</p>
<p>In pseudocode:</p>
<pre><code>def insert(key):
    # wip: we resize the array here if necessary
    
    A[n] = key # $n$ is the index of the next free space in the heap - we keep track of this in the heap structure
    j = n
    n += 1 # set $n$ to the next empty space in the array
    while j &gt; 0 and A[p(j)] &lt; A[j]:
        swap A[j] and A[p(j)]
        j = p(j)</code></pre>
<p>Clearly, this takes <span class="math inline">O(\log n)</span> time worst case since every run of the loop runs on a lower height on the tree. The bubbling up process would potentially need to visit every level in the tree if inserting an element greater than the root element of the heap.</p>
<p>To <strong>delete the maximum</strong> of a heap, our goal is to remove the root element of the heap. When we do this, we now have a blank space at the root node of the heap. We fix this by moving the last node in the heap (the rightmost node on the lowest level) into the position the root originally occupied. This allows us to satisfy the structural property again.</p>
<p>However, now the heap property is not satisfied. We fix this now by swapping the root node (originally the last node) <span class="math inline">i</span> with the bigger of its children <span class="math inline">lc(i)</span> and <span class="math inline">rc(i)</span>, which we will represent as <span class="math inline">c(i)</span>, to get <span class="math inline">c(i)&#39;</span> and <span class="math inline">i&#39;</span> such that <span class="math inline">p(i&#39;) = c(i)&#39;</span>. Now we have satisfied the heap property between <span class="math inline">i&#39;</span> and <span class="math inline">c(i)&#39;</span> and between <span class="math inline">i&#39;</span>'s sibling and <span class="math inline">c(i)&#39;</span>, but it is possible that <span class="math inline">i&#39;</span>'s children could be greater than it.</p>
<p>Therefore, we repeatedly swap the originally last node with its larger child until it is greater (or equal to) than both of its children, or it is a leaf node.</p>
<p>In pseudocode:</p>
<pre><code>def delete_max():
    max = A[0]
    n -- # 1 less than the next free space is the last node in the heap, which we will consider as a free space after we move it to the root
    # if memory management is needed, free `A[0]` here
    A[0] = A[n]
    
    # the following is the bubble_down operation, which bubbles values down until they satisfy the heap property
    j = 0
    while j &lt;= n - 1:
        max_index = j
        if lc(j) exists and A[lc(j)] &gt; A[j]: # existance means that lc(j) &lt;= n - 1
            max_index = lc(j)
        if rc(j) exists and A[rc(j)] &gt; A[max_index]: # existance means that rc(j) &lt;= n - 1
            max_index = rc(j)
        if max_index == k: break # the heap property is already satisfied, so stop bubbling
        swap A[j] and A[max_index] # swap the current node with its child, which is larger than it
        j = max_index
    
    # if necessary, shrink the array here
    return max</code></pre>
<p>Clearly, this takes <span class="math inline">O(h)</span> time worst case where <span class="math inline">h</span> is the height of the heap underneath the starting element, the root, which is <span class="math inline">O(\log n)</span> since <span class="math inline">h</span> is roughly <span class="math inline">\log n</span>. The worst case is when the last node of the heap is also the smallest one.</p>
<p><strong>Priority queue sort</strong> (PQSort) is a sorting algorithm where we insert all our elements into a priority queue and then take them out again in order. PQSort needs <span class="math inline">n</span> insertions and <span class="math inline">n</span> deletions of maximums. Since each operation is <span class="math inline">O(\log n)</span>, PQSort is <span class="math inline">O(n \log n)</span>, like mergesort. When the priority queue is implemented with a heap, we also call it <strong>heapsort</strong>.</p>
<p>Heapsort is pretty nice because we can sort in-place (by building a heap directly) and without a lot of recursion, but it can have poor real-world behaviour because the worst case heap insertion occurs when we are inserting nodes one by one that are already in ascending order, and already-sorted data occurs very often in real-world applications.</p>
<p>Also, heapsort is not stable. If a sorting algorithm is <strong>stable</strong>, then if different nodes have equal keys, they appear in the same order in the output as they did in the input.</p>
<p>Also, we often measure the runtime of sorting algorithms in terms of the number of key comparisons made (<span class="math inline">A[i] &lt; A[j]</span>). This is because we often allow users to specify their own key computing functions, which could potentially be aribtrarily exxpensive. Everything else is simply proportional or a constant on top of the number of key comparisons, in terms of runtime.</p>
<p>Bubbling downward is basically the operation of repeatedly swapping an element with its largest child until it is greater or equal to both. Bubbling up is basically repeatedly swapping an element with its parent until it is less than or equal to its parent.</p>
<p>We may also want to <strong>build heaps</strong> - given an array <span class="math inline">D</span> with <span class="math inline">n</span> values, we want to build a head from that. This is relatively trivial in theory - simply go through the array and insert it into a heap. Clearly, this is <span class="math inline">O(n \log n)</span> since insertion is <span class="math inline">O(\log n)</span> and we need <span class="math inline">n</span> of them.</p>
<p>Clearly, this takes roughly <span class="math inline">\sum_{k = 0}^{n - 1} \log(k + 1)</span> time since <span class="math inline">k</span> starts off at 0 and grows by 1 upon every insertion. Clearly, there are at least <span class="math inline">\frac n 2</span> terms in that sum where <span class="math inline">k + 1 \ge \frac n 2</span>, so <span class="math inline">\sum_{k = 0}^{c - 1} \log(k + 1) \ge \frac n 2 \log \frac n 2</span>, so the operation is <span class="math inline">\Omega(n \log n)</span>, and by the above, <span class="math inline">\Theta(n \log n)</span>.</p>
<p>A better option is to build the heap in place, to save memory. Clearly, an array always satisfies the structural property since there's no way not to, so all we have to do is enforce the heap property in the array.</p>
<p>We will enforce the heap property by bubbling downward on every level of the node starting from the last node that has children (the last node on the second-to-last level). Our invariant will be that for all <span class="math inline">k &gt; i</span>, the subheap rooted at <span class="math inline">k</span> satisfies the heap property (we can say this because we bubble down from right to left). We will show that this takes less than or equal to <span class="math inline">2c</span> key comparisons:</p>
<pre><code>for i in floor(n / 2) - 1 ... 0:
    bubble_down(i)</code></pre>
<p>We could bubble down for <span class="math inline">n - 1</span> down to 0, since that visits every node, but since the ones at the end are all leaf nodes and bubbling down is a no-op, we just need to bubble down for all nodes from <span class="math inline">p(n - 1) = \floor{\frac{n - 2} 2} = \floor{\frac n 2} - 1</span> down to 0.</p>
<p>Proof:</p>
<blockquote>
<p>Without loss of generality, assume <span class="math inline">n = 2^t - 1</span> where <span class="math inline">t</span> is the number of levels - all levels are full.<br />
Clearly, any node on a level <span class="math inline">l</span> needs at most <span class="math inline">2(t - l)</span> comparisons for its bubble down, since each level until the bottom needs 2 comparisons each and there <span class="math inline">t - l</span> levels between the current level and the bottom.<br />
Clearly, there are <span class="math inline">2^{l - 1}</span> nodes on each level, which means that there are <span class="math inline">\sum_{l = 1}^{t + 1} 2^{l - 1} 2(t - l)</span> comparisons in total.<br />
Clearly, <span class="math inline">\sum_{l = 1}^p 2^{l - 1} 2(p - l) = t\sum_{l = 1}^p 2^l - \sum_{l = 1}^p l2^l) \le 2^t - 2t - 2 \le 2^t - 1 = 2n</span> (note that <span class="math inline">\sum_{x = 1}^n x2^x = 2^{n + 1} n - 2^{n + 1} + 2</span>). ;wip: wat So there are <span class="math inline">2n</span> or fewer comparisons in total.</p>
</blockquote>
<p>In other words, we can build a heap in-place from an array in <span class="math inline">O(n)</span> time.</p>
<p>Also, if we prove that the running time of a function is of a certain order at infinity, then it is proven for all <span class="math inline">n</span>.</p>
<h1 id="section-5">22/1/15</h1>
<p>Another proof that in-place heapify is linear in time would be to use induction to show that the number of comparisons done for each node that is <span class="math inline">i</span> levels above the lowest is <span class="math inline">2i</span>, and that there are <span class="math inline">\frac{N + 1}{2^{i + 1}}</span> nodes in that level. Therefore, the total number of comparisons is <span class="math inline">\sum_{i = 1}^{h + 1} \frac{N + 1}{2^{i + 1}} 2i = (N + 1)\sum_{i = 1}^{h + 1} \frac{i}{2^i} \le (N + 1)\sum_{i = 1}^\infty \frac{i}{2^i} = 2(N + 1)</span>, by the series rules.</p>
<h2 id="selection-problem">Selection Problem</h2>
<p>Given an array <span class="math inline">A</span> of <span class="math inline">N</span> integers and an integer <span class="math inline">k \in \set{0, \ldots, N - 1}</span>, find the <span class="math inline">k</span>th smallest item in <span class="math inline">A</span>. If <span class="math inline">k = 0</span>, the result is the minimum. If <span class="math inline">k \approxeq \frac N 2</span>, the result is the median. If <span class="math inline">k = N - 1</span>, the result is the maximum.</p>
<p>We could solve this by sorting the array, which would be <span class="math inline">O(n \log n)</span>, or we could build a min-heap or max-heap and remove the appropriate number of items, both <span class="math inline">O(n \log n)</span>.</p>
<p>There is also a better solution called <strong>quickselect</strong>, based on the divide and conquer approach - we divide the problem into smaller subproblems, and then solve these subproblems. The basic idea is to recursively choose a pivot, partition the array into an array where every element is less than or equal to that pivot and an array greater than that pivot, then recurse on one of the smaller arrays to find the solution:</p>
<pre><code>def quick_select(A, k):
    if len(A) == 1: return A[0] # only one element, so must be the answer
    x = the pivot element # an element of the array that we choose to split on - how we choose it is up to us
    partition `A` into two sections (possibly in-place) `A[0:i - 1]` and `A[i + 1, n - 1]` such that `A[0:i - 1]` contains all elements that are less or equal to the pivot (excluding the pivot itself), and `A[i + 1, n - 1]` contains all elements that are greater, and `A[i]` is the pivot `x`
    if k == i: return x # the pivot element is the `k`th smallest element, so we found the answer
    if k &lt; i: return quick_select(A[0:i - 1], k) # the answer is in the left partition, so recurse on the smaller problem with the same index
    if k &gt; i: return quick_select(A[i + 1, n - 1], k - i - 1)</code></pre>
<p>Partitioning can be done in-place in <span class="math inline">O(n)</span> time worst-case:</p>
<pre><code>def partition(A, i):
    swap A[0] and A[i] # we want to get `x` out of the way first, so we put it in `A[0]`
    
    i, j = 1, len(A) - 1
    while True: # this is linear since `i` can only increase and `j` can only decrease, so we have at most one comparison with `A[0]` per element&gt;
        # find an element that belongs in the left partition, and an element that belongs in the right partition
        while i &lt; len(A) and A[i] &lt;= A[0]: i += 1 # go right until `A[i]` is greater than `x`
        while A[j] &gt; A[0]: j -= 1 # go left until `A[j]` is less than or equal to `x`
        
        if j &lt; i: # all of the left side is less or equal to `x` and all the right side is greater, so the partitioning is done
            # the place where the elements that are less or equal and the elements that are greater meet is the boundary between partitions
            swap A[0] and A[j] # put the pivot back into its place between the two partitions, since A[j] is less or equal to `x`
            break
        
        # at this point, `i` is greater than or equal to `j`, where `i` is the index of an element that is on the right but belongs on the left, and `j` represents the index of an element that is on the left but belongs on the right
        swap A[i] and A[j] # by swapping, we are moving each element into their correct partition</code></pre>
<p>The worst case runtime for quickselect is <span class="math inline">T(N) = \begin{cases} c_1 &amp;\text{if } N = 1 \\ T(N - 1) + c_2 N &amp;\text{if } N &gt; 1 \end{cases} = c_2 N + c_2 (N - 1) + \ldots + c_2 2 + c_1 = O(N^2)</span> where <span class="math inline">c_1, c_2</span> are constants - in the worst case, quickselect takes quadratic time. However, in the best case it takes just <span class="math inline">O(n)</span>, where the pivot is the <span class="math inline">k</span>th smallest element.</p>
<p>The average case runtime is <span class="math inline">T_{avg}(N) = \frac{\sum_{\text{each input } i \text{ of size } N} T(i)}{\text{number of inputs with size } N}</span>. This is not feasible to calculate directly, but we can actually describe every input instance as a permutation <span class="math inline">P</span>, since we only care about the order of numbers, not the values themselves. We will analyze the worst case of <span class="math inline">k</span>, where we need the maximum amount of recursion, in order to not have to consider it in our calculations.</p>
<p>With these assumptions in place, we can now write it as <span class="math inline">T_{avg}(N) = \frac{\sum_{\text{each permutation } i \text{ of } N \text{ elements}} T(i)}{N!}</span>. We now split the sum into those where the pivot ended up being roughly in the middle of the array after partitioning, and those where the pivot did not: <span class="math inline">\frac{\sum_{\text{each permutation } i \text{ of } N \text{ elements where } i \in [\frac 1 4 N, \frac 3 4 N)} T(i)}{N!} + \frac{\sum_{\text{each permutation } i \text{ of } N \text{ elements where } i \notin [\frac 1 4 N, \frac 3 4 N)} T(i)}{N!}</span>.</p>
<p>Note that when the pivot ends up being roughly in the middle, we are recursing on an input that is roughly half the size of the original input, which is the best case scenario for splitting input. When the pivot is not near the middle, in the worst case of <span class="math inline">k</span> the input we are recursing on could be almost as large as the original input.</p>
<p>Clearly, if <span class="math inline">i \in [\frac 1 4 N, \frac 3 4 N)</span>, which is <span class="math inline">\frac 1 2</span> of the time, then when we recurse in quickselect, our new input size is proportionally smaller, at most <span class="math inline">\frac 3 4</span> of the original size, so <span class="math inline">T(N) = T(\frac 3 4 N)</span>. If <span class="math inline">i \notin [\frac 1 4 N, \frac 3 4 N)</span>, which is also <span class="math inline">\frac 1 2</span> of the time, then the largest array we might recurse on could be roughly the same size as the original array, so <span class="math inline">T(N) = T(N - 1) + c_2 N</span>.</p>
<p>Therefore, on average <span class="math inline">T(N) = \begin{cases} c_1 &amp;\text{if } N = 1 \\ \frac 1 2 T(\frac 3 4 N) + \frac 1 2 T(N - 1) + c_2 N &amp;\text{if } N &gt; 1 \end{cases} = O(n)</span> where <span class="math inline">c_1, c_2</span> are constants. This means that quickselect is, on average, a linear time operation, with a <span class="math inline">O(n^2)</span> worst case and a <span class="math inline">O(n)</span> best case.</p>
<p>Also, exponential runtime includes all bases, but each base is distinct: <span class="math inline">O(4^n)</span> is not the same as <span class="math inline">O(2^n)</span>.</p>
<h1 id="section-6">27/1/15</h1>
<h2 id="quicksort">Quicksort</h2>
<p>Quicksort sorts by partitioning the input into two based on a pivot, then sorting each partition recursively. This can all be done in place:</p>
<pre><code>def quicksort(A, left, right):
    # postcondition: `A[left]`, ..., `A[right]` is sorted
    if right &gt;= left: return # the array is already sorted since it has 1 or fewer elements
    pivot = find_pivot(A, left, right)
    i = partition(A, left, right, pivot) # `i` is the new position of the original `pivot` element
    quicksort(A, left, i - 1)
    quicksort(A, i + 1, right)</code></pre>
<p>Quicksort is similar to quickselect, but we sort both sides when we recurse. In practice, quicksort is very fast, though in theory heapsort and mergesort are better.</p>
<p>Let <span class="math inline">T(n)</span> be the runtime for sorting <span class="math inline">n</span> elements, which is <span class="math inline">right - left + 1</span>. Clearly, <span class="math inline">T(n) = \begin{cases} O(1) &amp;\text{if } n \le 1 \\ T(\text{size of left partition}) + T(\text{size of right partition}) + O(n) &amp;\text{if } n &gt; 1 \end{cases}</span>.</p>
<p>We can also write this more formally:</p>
<blockquote>
<p>Let <span class="math inline">c_1, n_0</span> exist such that everything in the function except the recursive calls takes <span class="math inline">c_1 n_0</span> time or less for all <span class="math inline">n \ge n_0</span>. Let <span class="math inline">c_2 = \max(T(0), \ldots, T(n_0))</span>. Let <span class="math inline">c = \max(c_1, c_2)</span>.<br />
Then <span class="math inline">T(n) \le c_2 \le c \le cn \le T(\text{size of left partition}) + T(\text{size of right partition}) + cn</span> if <span class="math inline">2 \le n \le n_0</span>. So <span class="math inline">T(n) \le \begin{cases} c &amp;\text{if } n \le 1 \\ T(\text{size of left partition}) + T(\text{size of right partition}) + cn &amp;\text{if } n &gt; 1 \end{cases}</span>.</p>
</blockquote>
<p>In the worst case the size of one of the partitions will be <span class="math inline">n - 1</span>, when the pivot is the largest or smallest element each time - sorted input. So <span class="math inline">T(n) = T(1) + T(n - 1) + O(1) = O(n^2)</span>.</p>
<p>In the best case the size of each partition will be roughly equal, when the pivot is one of the median elements. So <span class="math inline">T(n) = 2T\left(\frac n 2\right) + O(n) = O(n \log n)</span>.</p>
<p>The average case proof is a bit more difficult:</p>
<blockquote>
<p>Clearly, the average is simply take the average of all possible pivot combinations: <span class="math inline">T_{avg}(n) = \frac{\sum_{i = 0}^{n - 1} \left(T_{avg}(i) + T_{avg}(n - i + 1) + cn\right)}{n} = \frac 1 n \sum_{i = 0}^{n - 1} T_{avg}(i) + \frac 1 n \sum_{i = 0}^{n - 1} T_{avg}(n - i + 1) + cn = \frac 2 n \sum_{i = 0}^{n - 1} T_{avg}(i) + cn</span>.<br />
We claim that given <span class="math inline">D = \max\left(T(0), T(1), \frac{T(2){2 \ln 2}}, 18c\right)</span>, <span class="math inline">T_{avg}(n) \le D \max(1, n \ln n)</span> for all <span class="math inline">n</span>. This can be proven uing induction over <span class="math inline">n</span>, and <span class="math inline">T_{avg}(n) \le D\frac 2 n \sum_{i = 2}^{n - 1} i \ln i + \frac 2 n D + cn</span>.<br />
Clearly, <span class="math inline">D\frac 2 n \sum_{i = 2}^{n - 1} i \ln i + \frac 2 n (T(0) + T(1)) + cn \le D\frac 2 n \sum_{i = 2}^{n - 1} i \ln i + \frac 2 3 (D + D) + cn</span> because <span class="math inline">n \ge 3</span>.<br />
So <span class="math inline">D\frac 2 n \sum_{i = 2}^{n - 1} i \ln i + \frac 2 3 (D + D) + cn \le \frac 1 2 n^2 \ln n - \frac 1 4 n^2 \le \frac{2D} n \frac 1 2 n^2 \ln n - \frac{2D} n \frac 1 4 n^2 + cn \le D n \ln n - \frac D {18} n + cn = O(n \log n)</span>.<br />
Basically, <span class="math inline">\frac 2 n \sum_{i = 0}^{n - 1} T(i) + cn \in O(n \log n)</span>.</p>
</blockquote>
<p>Quicksort and quickselect are bad in the worst case but good in the average case. In real life, how good it is will depend on what kind of input we run it on. The average case assumes that the every input is equally likely to occur, but it is not necessarily true. For example, sorting almost sorted arrays is a very common use case in real life, which, given our pivot selection technique, would often give the worst case behaviour.</p>
<p>Instead, we can force the instances to become equally likely, to get something like average case behaviour. We do this by using randomization - eliminating the bias caused by real world input by adding a random factor to the algorithm.</p>
<p>For quicksort, we can do this by selecting a random pivot instead of just always taking the first element in the array as the pivot. This makes all elements equally likely, so the probability of choosing a bad pivot is always <span class="math inline">\frac 1 n</span>, regardless of what the input is. The average case is now <span class="math inline">O(n \log n)</span> regardless of what kind of input we have.</p>
<h1 id="section-7">29/1/15</h1>
<p>We will prove that all comparison sorting algorithms are <span class="math inline">\Omega(n \log n)</span>. However, there are other sorting algorithms that can also run in <span class="math inline">O(n)</span> time, which we will also look at.</p>
<p>A comparison sorting algorithm is one that is based purely on key comparisons - we can only compare two keys to see if one is greater, less, or equal to another, and no other operations are available.</p>
<p>We don't really have tools for analyzing arbitrary algorithms. However, for comparison-based algorithms we have <strong>decision trees</strong>:</p>
<p>Consider an example, sorting three elements <span class="math inline">A[0..2]</span>:</p>
<pre><code>                                     A[0] \le A[1]
                                    / false       \ true
                       A[1] \le A[2]               same as left side, but with A[0] and A[1] exchanged
                      / false       \ true
A[0] \le A[1] \le A[2]               A[0] \le A[2]
                                    / false       \ true
              A[0] \le A[2] \le A[1]               A[0] \le A[2] \le A[1]</code></pre>
<p>Decision trees are binary trees that represent every possible decision path in the input, where each leaf is an output and each nde represents a decision. This decision tree represents the absolute minimum number of comparisons that we have to use in order to correctly sort three values</p>
<p>The worst case number of comparisons is the length of the longest path from the root to the leaf - the height of the tree. The length of a path is the number of edges in it.</p>
<p>Using these, we can now construct a proof:</p>
<blockquote>
<p>No matter what comparison sorting algorithm we use to sort, it is based on comparisons and is therefore representable using decision trees.<br />
When we sort <span class="math inline">n</span> items, there are <span class="math inline">n!</span> possible outputs - every permutation of the input is a possible output.<br />
Clearly, any binary tree of height <span class="math inline">h</span> must have at most <span class="math inline">2^h</span> leaves, so any binary tree with <span class="math inline">l</span> leaves must have height at least <span class="math inline">\log_2 l</span>.<br />
So the height <span class="math inline">h</span> of the decision tree is at least <span class="math inline">\log_2 n!</span>.<br />
Clearly, <span class="math inline">h \ge \log_2 n! = \log_2 (n(n - 1) \cdots 1) \ge \log_2 n + \log_2 (n - 1) + \ldots + \log_2 \frac n 2 \ge \frac n 2 \log_2 \frac n 2 = \frac n 2 \log_2 n + \frac n 2 \log_2 \frac 1 n = \frac n 2 \log_2 n - \frac n 2</span>.<br />
So <span class="math inline">h \in \Omega(n \log n)</span>.</p>
</blockquote>
<p>However, non-comparison sorting algorithms can be faster. <strong>Bucket sort</strong> is how most people will sort things by keys like names or grades. Basically, we have one pile for each possible value, make a pile for each value, and put them together:</p>
<pre><code>let $0 \le A[i] \le R - 1$ be an integer ($R$ is the radix)
create an array $R$ of empty lists $L[0], \ldots, L[R - 1]$
for element in A:
    append `element` t `L[element]`
concatenate all the lists $L[0], \ldots, L[R - 1]$ together into another list `L`
the sorted result is now `L`</code></pre>
<p>This is a stable sort. Since we iterate through the array, and the lists take a constant time to create, bucket sort is <span class="math inline">O(n + R)</span> in the worst case.</p>
<p>This is not a comparison sorting algorithm because it uses more information than just comparisons - it also relies on assumptions about the keys all being in the finite set of possible values.</p>
<p><strong>Key-index/count sort</strong> is similar to bucket sort (and is also stable), but doesn't need to use lists, which are something we try to avoid. The idea is that we know where the elements of <span class="math inline">L[1]</span> will be in the final result based on the number of elements in <span class="math inline">L[0]</span>, where the elements of <span class="math inline">L[2]</span> based on where <span class="math inline">L[1]</span> is and the number of elements in it, and so on:</p>
<pre><code># compute length of each list
list_lengths = [0] * R
for element in A:
    list_lengths[element] += 1

# compute position of each array in the result
list_positions = [0] * R
for i in range(1, R):
    list_positions[i] = list_positions[i - 1] + list_lengths[i - 1]

# put each value into the correct position
initialize `result` as a list with the same length as `A`
for element in A:
    result[list_positions[element]] = element
    list_positions[element] += 1</code></pre>
<p>This can also be done in place, and we can combine <code>list_length</code> into <code>list_positions</code>. This is still <span class="math inline">O(n + R)</span> since we have to initalize the array of size <code>R</code> to 0.</p>
<p>In practice, we wouldn't want to use these sorts for any real-world inputs. This is because although it is <span class="math inline">O(n)</span>, it is often still slower than a good comparison sort for real-world number values.</p>
<p>These sorts also work for occasions when we have too many buckets to be practical, like names. We could first sort by the first letter/digit of the name/grade, then recursively sort each of the buckets based on the second letter/digit, and so on. This technique is also called <strong>MSD-radix-sort</strong>. When <span class="math inline">R = 2</span>, this is very similar to quicksort.</p>
<p><strong>LSD-radix-sort</strong> is a bit better, since we don't have to keep track of all the indices by which we're sorting recursively:</p>
<pre><code>for d from the length of the largest value down to 1:
    sort `A` by the `d`th significant digit with a stable sorting algorithm</code></pre>
<p>The time complexity for this is simply <span class="math inline">O(m(n + R))</span> where <span class="math inline">m</span> is the largest possible number of digits and <span class="math inline">R</span> is the radix in which we are sorting (number of possible characters). Typically, the number of digits or letters is bounded, so the depth of recursion is bounded by a constant and the worst case time complexity is <span class="math inline">O(n + R)</span>.</p>
<h1 id="section-8">3/2/15</h1>
<p>An empty tree has height -1. A tree with one node has height 0. The height of a tree is 1 more than the larger of the heights of the two subtrees.</p>
<p>A <strong>dictionary</strong> is an abtract datatype that associates keys to values. Essentially, it stores key-value pairs, and supports insertion of a key value pair, searching for a key-value pair by key, and deleting a key-value pair by key.</p>
<p>We assume that all the keys are unique and can be compared in constant time, and each pair takes up constant space.</p>
<p>A naive way to implement dictionaries is to use unsorted arrays/lists (<span class="math inline">O(1)</span> insert, <span class="math inline">O(n)</span> search, <span class="math inline">O(n)</span> delete) or sorted arrays/lists (<span class="math inline">O(n)</span> insert, <span class="math inline">O(\log n)</span> search, <span class="math inline">O(n)</span> delete).</p>
<p>A <strong>binary search tree</strong> is a binary tree where for every node, every key in the left subtree is smaller than it and the every key in the right subtree are larger. We don't have to care about the duplicate keys since we assume keys are unique.</p>
<p>Implementing dictionaries using binary search trees is possible by constructing the BST where the value of each pair is its key (<span class="math inline">O(h)</span> insertion, <span class="math inline">O(h)</span> search, <span class="math inline">O(h)</span> deletion). It is always true that <span class="math inline">h \in \Omega(\log n)</span> and <span class="math inline">h \in O(n)</span>.</p>
<p>We want to make sure <span class="math inline">h</span> is <span class="math inline">O(\log n)</span>, by <strong>rebalancing</strong> the binary search tree as we perform operations on it. We do this by imposing additional conditions on the tree, and then showing that we can do all operations while maintaining these conditions and taking <span class="math inline">O(h)</span> time or less, and that these conditions ensure that <span class="math inline">h \in O(\log n)</span>.</p>
<p>There are many possible sets of conditions that we can use to make sure the binary tree is always balanced. One of these are those used in <strong>AVL-trees</strong>, which imposes just one condition: for any node, the height of the left and right subtree differ by at most 1 - the structural condition.</p>
<p>AVL trees were one of the first ever self-balancing binary trees, and although they work, they are rather slow in practice.</p>
<p>When we store a node in an AVL tree, we will also <strong>store the height of the tree formed by that node and its children in that node</strong>. This allows us to efficiently do the calculations we need later for insertions and deletions.</p>
<p>Searching in an AVL tree is exactly the same as in a normal BST. Since we don't modify the tree, we also can't violate the strutural condition.</p>
<p>For insertion, we need to insert the key-value pair at its proper spot, then check if the tree is still balanced and rebalance if not. Inserting into the correct subtree is the same as in a normal BST, but we also need to update the tree heights we are storing with the nodes - the newly inserted node gets height 0, and we recalculate the height of its parent. If the height changed, we recurse and recalculate the height of its parent, and so on.</p>
<p>While we are recalculating heights, we also check if the heights of the children of the ancestor differ by at most 1. If they differ by more, then <strong>we need to rebalance on that node</strong> - to make sure it is both a BST and tree heights don't differ by more than 1. We basically have 4 possible rebalancing operations, and it has been proven that at least one of them will correctly rebalance the tree.</p>
<p>The first is <strong>right rotation on node Z</strong>. Given a BST with nodes in the form <span class="math inline">Z(Y(X(A, B), C), D)</span>, we can rearrange it as <span class="math inline">Y(X(A, B), Z(C, D))</span> while still maintaining the BST properties, since <span class="math inline">A &lt; X &lt; B &lt; Y &lt; C &lt; Z &lt; D</span>. Basically, we rotate <span class="math inline">Y</span> and <span class="math inline">Z</span> clockwise/right, changing only those elements so the entire operation is constant time.</p>
<p>The second is <strong>left rotation on node Z</strong>, which is similar but we are rotating in the opposite direction. Given a BST with nodes in the form <span class="math inline">Z(A, Y(B, X(C, D)))</span>, we an rearrange it as <span class="math inline">Y(Z(A, B), X(C, D))</span> while still maintaining the BST properties.</p>
<p>The third is <strong>double right rotation on node Z</strong>. Given a BST with nodes in the form <span class="math inline">Z(Y(A, X(B, C)), D)</span>, we can rearrange it as <span class="math inline">X(Y(A, B), Z(C, D))</span>. Basically, we are bringing <span class="math inline">X</span> up to replace <span class="math inline">Z</span>. This is called a double right rotation because it can be implemented as left rotate on <span class="math inline">Y</span> and right rotate on <span class="math inline">Z</span>.</p>
<p>The fourth is the <strong>double left rotation on node Z</strong>. Given a BST with nodes in the form <span class="math inline">Z(A, Y(X(B, C), D))</span>, we can rearrange it as <span class="math inline">X(Z(A, B), Y(C, D))</span>.</p>
<p>Basically, we try each operation in turn, and check if each one is balanced. If so, then we can stop trying the others.</p>
<h1 id="section-9">5/2/15</h1>
<p>Operations on AVL trees can now be defined:</p>
<pre><code>def insert(k, v):
    insert `k` and `v` into the tree as a new leaf, without rebalancing
    let `node` be the new node that was inserted
    while `node` is not the root node:
        p, s = parent(node), sibling(node)
        if abs(s.height - node.height) &gt;= 2: # node unbalanced, rebalance it
            rebalance(node)
            node = p
        else: # node balanced, update the height
            new_height = max(node.height, s.height) + 1
            if (new_height == p.height): break # the height has stopped changing and we can stop rebalancing
            p.height = new_height

def delete(k):
    delete the node `k`, without rebalancing
    let `node` be the node that was deleted
    while `node` is not the root node:
        p, s = parent(node), sibling(node)
        if abs(s.height - node.height) &gt;= 2: # node unbalanced, rebalance it
            rebalance(node)
            node = p
        else: # node balanced, update the height
            new_height = max(node.height, s.height) + 1
            if new_height == p.height: break # the height has stopped changing and we can stop rebalancing
            p.height = new_height

def rebalance(node):
    let `y` be the larger child of `node`
    let `x` be the larger child of `y` # this child must exist since the height differs by more than 1, so `node` must have grandchildren
    perform the rebalancing operation according to the four rebalancing cases, based on whether `y` and `x` are left and right children
    update the heights of `node`, `y`, and `x`</code></pre>
<p>The height <span class="math inline">h</span> of an AVL-tree is always at most <span class="math inline">O(\log n)</span>. Proof:</p>
<blockquote>
<p>We want to prove that that <span class="math inline">h \le \log_c n = O(\log n)</span> for a constant <span class="math inline">c</span>.<br />
We want to find <span class="math inline">c</span> and <span class="math inline">n</span> for <span class="math inline">n \ge c^h</span> - given a fixed height, we want to find the smalles number of nodes we can have.<br />
Clearly, for height 0, there must be at least 1 node. Clearly, for height 1 there must be at least 2 nodes. Clearly, for height 2 there must be at least 4 nodes. Clearly, for height 3 there must be at least 7 nodes.<br />
By induction, we can show that the minimum number of <span class="math inline">n</span>, represented <span class="math inline">n(h)</span> must be <span class="math inline">n(0) = 1, n(1) = 2, n(h) = 1 + n(h - 1) + n(h - 2)</span>, since each minimal tree must have the smaller minimal trees as its children, and we want to use one as large as possible while the other is as small as possible.<br />
Interesingly, the values of <span class="math inline">n(h)</span> is simply 1 less than each Fibonacci number. The Fibonacci numbers grow exponentially, so <span class="math inline">n(h)</span> does too.<br />
Clearly, <span class="math inline">n(h) \ge \sqrt{2}^h</span>, since <span class="math inline">n(h) = n(h - 1) n(h - 2) + 1 \ge n(h - 2) + n(h - 2) = 2n(h - 2) \ge 2^2 n - 4 \ge \ldots \ge 2^i n(h - 2i), i &gt; 1</span>, so <span class="math inline">n(h) \ge 2^{\ceil{\frac h 2}}</span>.<br />
So any AVL-tree of height <span class="math inline">h</span> has <span class="math inline">\sqrt{s}^h</span> or more nodes, so <span class="math inline">h \le \log_{\sqrt 2} n = O(\log n)</span>.</p>
</blockquote>
<p>Clearly, the worst case time for insertion/deletion/search is <span class="math inline">O(h)</span>, and since <span class="math inline">h \in O(\log n)</span> and each rebalancing operation takes constant time, insertion/deletion/search is always <span class="math inline">O(\log n)</span>.</p>
<h3 id="trees">2-3-trees</h3>
<p>A 2-3-tree is similar to AVL-trees in concept, but we no longer have to be a binary tree - a node may have 2 or 3 children. The 2-3-tree could be said to be a generalization of the AVL-tree.</p>
<p>Each node in a 2-3 tree is either a node with one key-value pair and two children, or a node with two key-value pairs and three children (the left child is less than key 1, the middle child is between key 1 and 2, and the right child is greater than key 2, and key 2 must be greater than key 1). Additionally, all empty subtrees must be on the same level.</p>
<p>Searching is done by a generalized version of the standard BST search - when we are comparing, we simply need to check how many key-value pairs the node has and check the cases accordingly.</p>
<p>Insertion into a 2-3 tree is done by searching for the node on the lowest level that would have had the key as a child if it was inserted directly. If it has only one key-value pair, then add our key-value pair to it to make it a node with two key-value pairs. Otherwise, we add the key-value pair to it to make a node with three key-value pairs (an overflow), then split it into three nodes with one key-value pair each and move the middle one (the one that is neither largest or smallest) up to the parent, so the node becomes two nodes and the parent gets an extra key-value pair. If the parent now has three nodes when we do this, we recursively perform this splitting and moving up operation on the parent, all the way up until no nodes have three key-value pairs.</p>
<p>Deletion in a 2-3 tree is done by searching for the key-value pair that we want to delete. We then swap the key-value pair with its successor/predecessor (the descendant key-value pairs that are closest to the current node's value), like with deletion in a normal BST. If the successor/predecessor's node has 2 keys, then we just delete the successor/predecessor to get a node with a single key-value pair. Otherwise, we have an underflow. If possible, we <strong>transfer</strong> by moving a key-value pair from the parent down into the leaf, and move a key-value pair from a sibling up into the parent to complete the deletion. Otherwise, we <strong>fuse</strong> by moving a key-value pair from the parent down into a sibling. If we do this, it might be necessary to recurse and fuse on the parent, and so on.</p>
<h1 id="section-10">10/2/15</h1>
<p>Insertion, deletion, and searching in a 2-3 tree are all <span class="math inline">O(h)</span> where <span class="math inline">h</span> is the height of the tree. The height of a 2-3 tree is always <span class="math inline">\Theta(\log n)</span>. Proof:</p>
<blockquote>
<p>Let <span class="math inline">n</span> be the number of key-value pairs in the tree. We want to find the smallest possible <span class="math inline">n</span> for any given <span class="math inline">h</span>, in order to get an upper bound on <span class="math inline">h</span> given <span class="math inline">n</span>.<br />
Clearly, the first level has at least 1 key-value pair and at least 2 children. Clearly, the second level has at least 2 key-value pairs and at least 4 children. By induction, it can be proved that we have at least <span class="math inline">n \ge 2^{i - 1}</span> key-value pairs on each level <span class="math inline">i</span>, with at least <span class="math inline">2^i</span> children.<br />
So the total number of key-value pairs is <span class="math inline">n \ge 2^{h + 1} - 1</span>, and <span class="math inline">h \le \log_2(n + 1) - 1</span>. So <span class="math inline">h \in O(\log n)</span>.<br />
Clearly, the first level has at most 1 key-value pair and at most 3 children. Clearly, the second level has at most 6 key-value pairs and at most 9 children. By induction, it can be proved that we have at most <span class="math inline">n \le 2 \cdot 3^{i - 1}</span> key-value pairs on each level <span class="math inline">i</span> (2 key-value pairs per node, <span class="math inline">3^{i - 1}</span> nodes per level), with at most <span class="math inline">3^i</span> children.<br />
So the total number of key-value pairs is <span class="math inline">n \le 3^{h + 1} - 1</span>, and <span class="math inline">h \ge \log_3(n + 1) - 1</span>. So <span class="math inline">h \in \Omega(\log n)</span>.<br />
So <span class="math inline">h \in \Theta(\log n)</span>.</p>
</blockquote>
<p>We can also generalize 2-3 trees into <span class="math inline">a</span>-<span class="math inline">b</span> trees. These are trees where the nodes can have between <span class="math inline">a</span> and <span class="math inline">b</span> children, except the root node, which can have between 2 and <span class="math inline">b</span> children. The number of children for each node is 1 more than the number of key-value pairs it has. Children are subtrees that are between two neighboring keys. All empty subtrees must be on the same level.</p>
<p>The height of an <span class="math inline">a</span>-<span class="math inline">b</span> tree is <span class="math inline">h \in \Omega(\log_b(n)), h \in O(\log_a(n))</span>. Searching takes <span class="math inline">O(\log_2 b \log_a n)</span>.</p>
<h2 id="external-memory">External Memory</h2>
<p>It is quite common that we have more data than we can fit into the main memory, and we store it in external memory. Accessing external memory is generally far slower.</p>
<p>We now use a new computation model in which we only <strong>count how many times we access external memory</strong> - the number of <strong>disk transfers</strong>. We will also often consider sequential access to be faster than random access access.</p>
<p>A <strong>B-tree</strong> of order <span class="math inline">M</span> is a <span class="math inline">\ceil{\frac M 2}</span>-<span class="math inline">M</span> tree - a tree with between <span class="math inline">\ceil{\frac M 2}</span> and <span class="math inline">M</span> children per node (except the root). We choose an <span class="math inline">M</span> so that one node of the B-tree will fit into main memory. Clearly, <span class="math inline">h \in O(\log_{\frac M 2} n)</span>. Storing our data as a B-tree is an excellent way to minimise disk transfers, since searching takes <span class="math inline">h</span> or fewer disk transfers, and insertion/deletion takes <span class="math inline">2h</span> or fewer.</p>
<h1 id="section-11">12/2/15</h1>
<p>As a result, we can use balanced binary trees to implement a dictionary with <span class="math inline">\Theta(\log n)</span> runtime for all supported operations.</p>
<p>Any comparison-based implementation of searching for a key <span class="math inline">k</span> is <span class="math inline">\Omega(\log n)</span>. We can prove this as follows:</p>
<blockquote>
<p>Clearly, we can only use comparison between <span class="math inline">k</span> and existing keys <span class="math inline">a_0, \ldots, a_n</span>.<br />
Clearly, in the decision tree for the algorithm there must be at least one leaf per key, since any key might be looked up. Clearly, there must also be a leaf for the case where <span class="math inline">k</span> is not in the dictionary.<br />
So the number of leaves is at least <span class="math inline">n + 1</span>. So the height of the decision tree must be <span class="math inline">h \ge \log(n + 1)</span>.<br />
Clearly, the height of the decision tree determines the worst-case number of comparisons that need to be made.<br />
So the worst case comparisons for the algorithm is <span class="math inline">\Omega(\log n)</span>.</p>
</blockquote>
<p>So logarithmic time for searching is the best we can do if we just have comparisons. However, if we exploit other properties of keys, we can do better.</p>
<p>If we assume keys of our dictionary are integers, and that they have a small lower and upper bound <span class="math inline">l \le k &lt; h</span>, then we can use <strong>key-indexed search</strong>. Basically, we create an array of size <span class="math inline">h - l</span> and map every key <span class="math inline">k</span> to the index <span class="math inline">k - l</span> in the array. This means we have <span class="math inline">\Theta(1)</span> time for all operations. However, it can be very wasteful of space for sparse dictionaries, and the assumption doesn't occur very often in practice.</p>
<h2 id="hash-tables">Hash Tables</h2>
<p>What we could potentially do is to map keys to the indices of a much smaller table - hashing the keys. We keep an unordered array <span class="math inline">T</span> (a <strong>hash table</strong>), where each entry is known as a <strong>bucket</strong> or <strong>slot</strong>. <span class="math inline">T</span> has size <span class="math inline">M</span>, which we can choose ourselves.</p>
<p>We now want to map our keys to the indices <span class="math inline">0, \ldots, M - 1</span> - a <strong>hash function</strong> <span class="math inline">h(k): \text{keys} \to \set{0, \ldots, M - 1}</span>, that can be computed in <span class="math inline">O(1)</span>. So now any key <span class="math inline">k</span> is stored at <span class="math inline">T[h(k)]</span>. An example of a hashing function over integer keys is <span class="math inline">h(k) = k \pmod{M}</span>.</p>
<p>However, it is possible for multiple keys to hash to the same value (a <strong>collision</strong>). There are multiple ways to resolve this. One way is to use a linked list or something similar at each slot, to store all the colliding key-value pairs. Unfortunately, this means that in the worst case, when all the keys collide, we have <span class="math inline">O(n)</span> search and deletion, or <span class="math inline">O(\log n)</span> if we used balanced binary trees at each slot instead of linked lists.</p>
<p>Our goal is therefore to minimise collisions on average. An ideal hash function must therefore be <strong>uniform</strong> (all slots are equally likely to be chosen for any key). We will assume that we have such a function, although in practice they are very hard to do correctly.</p>
<p>The <strong>load factor</strong> is <span class="math inline">\alpha = \frac{n}{M}</span> - the ratio of keys to slots. We control <span class="math inline">\alpha</span> because we control <span class="math inline">M</span>. Essentially, we can resize the array when we need to to reduce the load factor. We generally want to keep the load factor small to avoid collisions, but not so small that we waste too much memory.</p>
<p>Alternatively, we can find an different slot for <span class="math inline">k</span> in the same array rather than using a linked list for each slot (this is known as <strong>open addressing</strong>). This is more efficient because we avoid the overhead of linked lists, but is still has the poor <span class="math inline">O(n)</span> worst case behaviour for all operations.</p>
<p>Basically, we now have a sequence of slots <span class="math inline">h(k, 0), h(k, 1), \ldots, h(k, M - 1)</span> using our slightly modified hash function (this is a <strong>probe sequence</strong>), and we search for an empty slot in the sequence <span class="math inline">T[h(k, 0)], \ldots, T[h(k, n)]</span> when we want to insert or delete.</p>
<p>When we delete we need to make sure we fill in the gap in the sequence that opened up when we deleted a key-value pair, or else we will break the sequence. What we could do is after deletion, insert a sentinel value that represents a deleted element into that slot. This sentinel value is interpreted as an element by search and delete (but not insert), and is not considered when deciding whether to shrink the array (that means we might need to keep track of two load factors).</p>
<p>A common way to define the probe sequence is <strong>linear probing</strong>, when <span class="math inline">h(k, i) = h(k) + i \pmod{M}</span>. Ideally, a probe sequence is a permutation of <span class="math inline">0, \ldots, M - 1</span> - we want to ensure that we try all slots and don't try each one more than once.</p>
<p>For open addressing, we must ensure that the load factor is less than 1 (it can't be 1 since otherwise it will break search). It is generally a good idea to increase <span class="math inline">M</span> when the load factor gets up to 0.5 or so.</p>
<p>Linear hashing is inefficient because it ends up making big runs of non-empty slots - it clusters a lot of values together, and any key that hashes innto the cluster increases its size, which means that searching will often encounter long runs that it will need to go through. A good probe sequence should hop around the array more to avoid further collisions. <strong>Quadratic probing</strong> does this a little better using <span class="math inline">h(k, i) = h(k) + c_1 i + c_2 i^2</span>, but it is hard to choose the constants such that it makes a permutation. Another technique is to use double hashing, where <span class="math inline">h(k, i) = h_1(k) + i h_2(k)</span> and <span class="math inline">h_2(k) \ne 0</span>, but getting a hash function that is a permutation is difficult.</p>
<h1 id="section-12">24/2/15</h1>
<p><strong>Cuckoo hashing</strong> is more or less the best hashing strategy around. It uses a very different insertion strategy: we definitely insert a key at slot <span class="math inline">h(k)</span>. If that slot was occupied prior to insertion, we kick the old one out and insert the new one.</p>
<p>This strategy has two independent hash functions, <span class="math inline">h_1(k)</span> and <span class="math inline">h_2(k)</span>:</p>
<pre><code>def insert(key, value):
    original_index = h_1(key) # the index of the new key-value pair
    index = original_index
    while T[index] is not None: # slot is not already empty, kick out old value
        old_key, old_value = T[index]
        T[index] = (key, value) # store the current key-value pair, displacing the original key-value pair in there
        index = h_1(old_key) if index == h_2(old_key) else h_2(old_key) # switch the index to the alternate slot for the newly displaced pair
        key, value = old_key, old_value # repeat the whole kicking out and inserting process for the newly displaced key-value pair
    T[original_index] = (key, value) # insert the new key-value pair in case it didn&#39;t kick any other pairs out earlier</code></pre>
<p>It is possible for insertion to go into a cycle, such as if the hash functions were equal. To avoid an infinite loop, we check if the length of the displacement sequence (the number of times the while loop runs) and check if it equals the table size. If this is the case, we rebuild the table with a larger size, and new hashing functions.</p>
<p>Insertion into a hash table using cuckoo hashing has an expected runtime of <span class="math inline">O\left(\frac \alpha {(1 - 2\alpha)^2}\right)</span>, where <span class="math inline">\alpha</span> is the load factor and assuming the hashing functions are uniformly distributed. So when the load factor starts going above 0.5, the expected runtime goes to infinity. We should always rebuild the table before this.</p>
<p>For all the hashing strategies we've seen so far, insertion, deletion, and search has an expected runtime of <span class="math inline">O(1)</span>, as long as we keep the load factor below a constant (1 for open addressing, 0.5 for cuckoo hashing).</p>
<p>Midterm covers everything up to here, includes identities sheet, questions like run algorithm on some input, compare algorithms by things like memory or time, develop new algorithms, proving runtimes.</p>
<h3 id="rebuilding-tableshashing">Rebuilding tables/Hashing</h3>
<p>We want to occasionally rebuild the table when the load factor gets too big or too small, to avoid collisions and save memory. Basically, we monitor <span class="math inline">\alpha</span> during insertions and deletions. If <span class="math inline">\alpha</span> gets too big, we make a new, larger array (usually about double the size of the previous one, or less), then insert all the elements of the old one into the new one, and creating new hash functions. If <span class="math inline">\alpha</span> gets too small, we do the same thing, but with a smaller array (usually about half the size of the previous table).</p>
<p>If we do both the growing and shrinking operations, we can ensure that the hash table also uses <span class="math inline">\Theta(n)</span> space.</p>
<p>Obviously, this is very slow, but it only happens very rarely. These rebuilding operations are amortized so that insertion and deletion are <span class="math inline">O(1)</span> on average.</p>
<p>In practice, open addressing is the most commonly used collision resolution strategy. We also want to use cheaply computable functions, unlike some theoretical ones that are very expensive to compute. A hash function should not ignore any part of the key, and should mix up patterns in data.</p>
<p>A common hash function for integer keys is <span class="math inline">h(k) = k \pmod M, M \in \mb{N}</span>. This function is the division method. However, this works very badly if the data is human-readable and <span class="math inline">M</span> is a power of 10 (since many real world numbers end with 0), and if <span class="math inline">M</span> is a power of 2, since powers of two are very commonly used in computer arithmetic. Ideally, <span class="math inline">M</span> would be a prime number. A common value is <span class="math inline">2^i - 1</span>, but this also is bad for certain patterns of numbers.</p>
<p>Another common hash function for integer keys is <span class="math inline">h(k) = \floor{(Ak \mod 1) \times M}, A \in \mb{R}</span>. This function is the <strong>multiplication method</strong>. For this, we want to choose an <span class="math inline">A</span> that destroys patterns in the input, though good values are hard (<span class="math inline">\phi = \frac{\sqrt{5} - 1} 2</span> is a good value in practice). This technique is good because the table size doesn't affect the quality of hashing.</p>
<p>Keys are not always integers. To hash strings, we often use the ascii values, and then use each of them in a certain base. For example, &quot;APPLE&quot; becomes <span class="math inline">65, 80, 80, 76, 69</span>, and we use this as the digits in a certain radix <span class="math inline">R</span>, to get <span class="math inline">h(k) = (65R^4 + 80R^3 + 80R^2 + 76R^1 + 69R^0) \mod M</span>. Since the value before the modulo is so large, it is hard to compute. We can do the equivalent function <span class="math inline">h(k) = ((((((((65 \mod M)R + 80) \mod M)R + 80) \mod M)R + 76) \mod M)R + 69) \mod M</span> instead.</p>
<h1 id="section-13">26/2/15</h1>
<p>In external memory, a B-tree uses <span class="math inline">O(n \log_M n)</span> disk transfers. Ideally, we want to use hashing in external memory much like in internal memory to speed up our accesses.</p>
<p>External memory is organized in <strong>pages</strong>, which are chunks that each fit in internal memory. We can easily read pages of external memory at a time.</p>
<p>We could use open addressing, but this would result in a lot of disk transfers. We could use hashing with chaining/closed addressing, but potentially if there are long chains one chunk would not fit in internal memory and we would need to store it in external memory again, which wastes space.</p>
<h2 id="directories">Directories</h2>
<p>There is a good way to do hashing with external memory, a data structure known as a <strong>directory</strong>, using a technique called <strong>extendible hashing</strong>.</p>
<p>Let <span class="math inline">h(k) \in \set{0, \ldots, 2^L - 1}</span> - all hash values are binary strings of size <span class="math inline">L</span>. The directory contains an array, stored in internal memory, of size <span class="math inline">2^d</span> where <span class="math inline">d \le L</span> (<span class="math inline">d</span> is called the <strong>order</strong> of the directory).</p>
<p>Each slot in the array points to the a block in external memory, a chunk of fixed size. Each slot of the array has an index that can be represented as a binary string of length <span class="math inline">d</span>. The idea is that every key-value pair with a hash value that begins with binary string <span class="math inline">x_1 \ldots x_d</span> is stored at the index represented by <span class="math inline">x_1 \ldots x_d</span>. For example, a key-value pair with hash <span class="math inline">110000</span> in a directory of order 2 is stored at index <span class="math inline">11</span>, or 3 in decimal.</p>
<p>Each block stores an integer <span class="math inline">k_d \le d</span>, the <strong>local depth</strong>, which represents how many bits of the hashes of the keys will be the same, starting from the beginning. A block also stores key-value pairs such that the first <span class="math inline">k_d</span> bits of the hashes of each key are the same. The local depth also shows how many references there are to the block, as <span class="math inline">2^{d - k_d}</span>. For example, a block with local depth 2 stores key-value pairs with hashes that all begin with the same first two bits.</p>
<p>Blocks can store their key-value pairs however they want, like hash tables or BSTs, since they are loaded into internal memory before we work with them. We will assume that searching within the block is <span class="math inline">O(\log S)</span> where <span class="math inline">S</span> is the maximum number of key-value pairs a block can store.</p>
<p>Now, to lookup a key-value pair, we take the first <span class="math inline">d</span> bits of the key hash as an array index, then read the block at that array index into internal memory and search for the key-value pair within that block. For example, a key-value pair with hash <span class="math inline">010001</span> stored in a directory of order 3 is in the array at index <span class="math inline">010</span>, or 2 in decimal.</p>
<p>To insert a key-value pair, we take the first <span class="math inline">d</span> bits of the key hash as an array index, then read the block at that array index into internal memory. If there is room in the block, we insert the key-value pair and are finished. Otherwise, if the block contains the entries for multiple slots (<span class="math inline">k_d &lt; d</span>), we split the block into two blocks with local depth <span class="math inline">k_d + 1</span>, and make sure the array points to the correct blocks, then try to insert again. Otherwise, we have to rebuild the directory's entire array by doubling the array and populating it again, so <span class="math inline">d</span> is incremented.</p>
<p>To delete a key-value pair, we do something similar to insertion by reading the block and deleting the key, but merge two blocks together if possible. Merging involves decrementing <span class="math inline">k_d</span> on one block, copying key-value pairs over, then deleting the other block and updating references in the directory's array.</p>
<p>Clearly, lookup is <span class="math inline">O(1)</span> and only ever needs 1 disk transfer. Insertion could potentially take <span class="math inline">O(2^d)</span> time (to rebuild to array), but this happens very rarely, and only needs 2-3 disk transfers.</p>
<p>The advantage of extendible hashing is that we never have to re-update all the blocks in external memory, or rehash them. This makes it very well suited for working with external memory.</p>
<p>An empty directory starts off with all slots pointing to the same block, and the <span class="math inline">k_d</span> of that block is 0. Basically, all of the slots are combined into one. It is possible for multiple slots to point to the same block, when <span class="math inline">k_d &lt; d</span>. Insertion and deletion can cause blocks to become combined and split combined blocks.</p>
<p>So far, we have seen various ways of implementing dictionaries: unsorted arrays, sorted arrays, hash tables, AVL-trees, in increasing order of difficulty to implement.</p>
<h1 id="section-14">3/3/15</h1>
<h2 id="heuristics-and-self-rearranging-arrays">Heuristics and Self-rearranging Arrays</h2>
<p>In the real world, sometimes algorithms with poor worst case time complexity will actually perform better.</p>
<p>For dictionaries implemented using unordered arrays or linked lists, search will always need <span class="math inline">O(n)</span> worst case. However, a successful search takes <span class="math inline">O(\text{index of } k \text{ within the array/linked list})</span>.</p>
<p>The idea is to place keys that are more frequently searched for should be at the beginning of the array/linked list. In the real world, the 80-20 rule of thumb says that about 80% of searches will be for 20% of the keys.</p>
<p>If we know the frequencies of search for the keys beforehand, we can just put them in the array/linked list sorted by decreasing frequency. This allows to to minimize <span class="math inline">\sum_{k \in \text{ keys of the dictionary}} \text{frequency of search} \times \text{index of } k</span>. This is called <strong>static ordering</strong>.</p>
<p>We often don't know the frequencies. Instead, we could change the order of the keys with every search. Every time we search for a key and find it, we move it toward the beginning.</p>
<p>One such movement strategy is always moving the found item to the front, and shifting the array to the right by 1, the <strong>move-to-front heuristic</strong>. There is also the <strong>transpose heuristic</strong>, which always moves the found item 1 item toward the front of the array if not already at the front (by swapping it with its left neighbor). These are all <span class="math inline">O(1)</span>, except the move-to-front heuristic on arrays, which is <span class="math inline">O(n)</span>. The move-to-front heuristic tends to be better in practice.</p>
<p>For ordered arrays, search will always need <span class="math inline">O(\log n)</span> worst case. However, in real life we often have a general idea where the key is. For example, if we are looking for a name beginning with Z in a phone book, we would start near the end.</p>
<p>The idea is to guide the search by where we expect the key to be. If the current bounds in our search are <span class="math inline">l</span> and <span class="math inline">r</span> in the sorted array <span class="math inline">A</span> (assumed to contain cardinal values), then we would expect the a key <span class="math inline">k</span> to be at index <span class="math inline">l + (r - l)\frac{k - A[l]}{A[r] - A[l]}</span>, assuming the values are all uniformly spaced. This is called <strong>interpolation search</strong>:</p>
<pre><code>def interpolation_search(key, A, left = 0, right = len(A)):
    if left &gt;= right: return A[l] if A[l] == k else None
    middle = left + (right - left) * ((key - A[left]) / (A[right] - A[left]))
    if A[middle] &gt;= key: return interpolation_search(key, A, left, middle)
    return interpolation_search(key, A, middle + 1, right)</code></pre>
<p>This is now greater than <span class="math inline">O(\log n)</span> in the worst case, but it works pretty well in real life.</p>
<p>We also have <strong>galloping</strong>, which is a search technique for sorted arrays in which we start from the beginning, and skip ahead by increasing increments. For example, we might skip ahead by 1 at first, then 2, then 4, then 8, and so on, until we are at or past the desired key, in which case we would do a binary search or similar on the bounds. This is good for huge arrays (and even works on infinite-size arrays, as long as they are sorted) because we can start searching before the entire array is finished loading, and is actually <span class="math inline">O(\log n)</span> if we increase the search index exponentially.</p>
<p>A <strong>skip list</strong> is a data structure used for implementing dictionaries using ordered linked lists. Basically, the idea is that we store additional references to nodes within the list, so we can skip to it directly when we need to, like with an ordered array. We would essentially store lists <span class="math inline">L_1, \ldots, L_k</span>, where <span class="math inline">L_k</span> is a subset of <span class="math inline">L_{k - 1}</span> of proportional size, <span class="math inline">L_1</span> contains every node, and <span class="math inline">L_k</span> includes only 1. Each node has multiple fields for the next pointers of each level of list, so at each node we can go to the next element of any of the lists that include it. The goal is to have each list be a uniformly spaced sample of the elements of the lower list. This is a lot like a tree with variable arity and fixed height.</p>
<p>To search, we would first search left to right through the top list. Right before we pass the key's value or reach the end, we drop down a level and try searching left to right again, repeating the search and drop down until we have found the key or are at the lowest level and try to drop down (key not found).</p>
<p>To insert, we search for where <span class="math inline">k</span> should be, insert <span class="math inline">k</span> into the lowest list <span class="math inline">l_1</span>, and then randomly choose how many lists we choose to insert it into with decreasing probability (probability of inserting into <span class="math inline">i</span> lists above should be <span class="math inline">\frac 1 {2^i}</span>). So at each level, we would have a 50% chance of also inserting it into the parent level.</p>
<p>This takes <span class="math inline">O(n)</span> expected space and <span class="math inline">O(\log n)</span> expected runtime for search, insert, and delete.</p>
<h1 id="section-15">5/3/15</h1>
<p>Our abstract datatype dictionaries we have insertion, deletion, and search. However, there are often other ueful operations in many situations.</p>
<h2 id="range-queries">Range Queries</h2>
<p>A <strong>range query</strong> is an operation <span class="math inline">\text{Range}(x_1, x_2)</span> that returns all the key-value pairs <span class="math inline">k, v</span> such that <span class="math inline">x_1 \le k \le x_2</span> - all the key-value pairs in a range.</p>
<p>For implementations of dictionaries using unordered arrays, hash tables, heaps, and other data structures that use arrays that are not sorted, we can't do better than just searching through the entire array. For implementations that are sorted, like sorted arrays or BSTs, we can do binary search for the range endpoints, and build the result along the way, in <span class="math inline">O((x_2 - x_1) + \log n)</span> time.</p>
<p>We don't really care about the <span class="math inline">x_2 - x_1</span> term since we can't do anything about it - it is the cost of producing the output. We will only consider the terms that depend on <span class="math inline">n</span>.</p>
<p>To do range queries on a BST, we can do a modified tree traversal to flatten the desired subtrees:</p>
<pre><code>def range_query(node, lower, upper):
    if node == None: return []
    if node.value &lt; lower or node.value &gt; upper: return []
    return range_query(node.left) + [(node.key, node.value)] + range_query(node.right)</code></pre>
<p>Range queries are extremely useful for building databases and working with multidimensional data. Suppose the items stored in a dictionary all each have <span class="math inline">d</span> attributes, and that the attributes are numbers - the values of the items are <span class="math inline">d</span>-tuples of numbers.</p>
<p>Now we want to do a <span class="math inline">d</span>-dimensional range query to search by multiple attributes.</p>
<p>One way to do 2D range queries is to use a quadtree. This is a data structure that stores points as a tree-like structure, where each node has 4 children that split a 2D rectangle into 4 quadrants. To build one, we find a bounding box of all the points (preferably, a power of 2), and then recursively split boxes into 4 smaller boxes as quandrants as necessary until every non-split box contains 0 or 1 points. To draw a quadtree, we label the edges as one of <span class="math inline">NE, NW, SW, SE</span> (and draw them in this order from left to right), label leaves as empty or the item it stores, and label non-leaf nodes with the bounding boxes they represent.</p>
<p>Searching a quadtree is similar to searching a binary tree, just with more cases for which child to recurse into. Insertion is similar to binary tree insertion as well, but when we find the leaf where the new point should go, we place it down if the leaf is empty, replace the leaf if the points are equal, or split the node into quadrants if it already has a point (putting the old point into the correct quadrant) and attempt to insert the new key into the correct child.</p>
<p>As a result, insertion can split a node many times. In fact, the height of a quadtree is unbounded with respect to <span class="math inline">n</span>. Potentially, we could have an arbitrary number of quadrant splits depending on how close together points are (closeness of new point to any existing points determines the max number of splits upon insertion). The height is actually <span class="math inline">O\left(\log \frac{\text{max distance between points}}{\text{min distance between points}}\right)</span>.</p>
<h1 id="section-16">10/3/15</h1>
<p>Range search in quadtrees is similar to range search in binary trees, but with bounds on both the X and Y coordinates of the points:</p>
<pre><code>def box_query(node, left, top, right, bottom, node_width, node_height):
    if node is None: return []
    if node.is_leaf: return [(node.key, node.value)]
    if node_width &lt; left or right &lt; 0: return [] # outside of the box bounds
    if node_height &lt; top or bottom &lt; 0: return [] # outside of the box bounds
    return range_query(node.top_left, left, top, right, bottom, node_width / 2, node_height / 2) +
           range_query(node.top_right, left + width / 2, top, right, bottom, node_width / 2, node_height / 2) +
           range_query(node.bottom_left, left, top + height / 2, right, bottom, node_width / 2, node_height / 2) +
           range_query(node.bottom_right, left + width / 2, top + height / 2, right, bottom, node_width / 2, node_height / 2)</code></pre>
<p>We can also use a coloring algorithm that colors each node white (none of the children are in the box), black (all of the children are in the box), or gray (not sure, or neither). We recurse on gray nodes, ignore white nodes, and output all descendants of black nodes.</p>
<p>The runtime is therefore <span class="math inline">O(g + \frac w {O(g)} + \frac b {O(g)})</span>, where <span class="math inline">g, w, b</span> represents gray, white, and black, repectively. The number of gray nodes is unbounded, and therefore the runtime is as well.</p>
<p>A <strong>k-d tree</strong> is a data structure similar to quadtrees. Where in quadtrees we could split nodes into 4 equal parts when inserting, in k-d trees, we split the region into 2 parts (alternatingly horizontally and vertically, or through each dimension in higher dimensional k-d trees) that each have roughly the same number of points, and the size of both regions can be different.</p>
<p>The partitions/splits are placed at the median of all the points inside the node. Calculating the median can be done in <span class="math inline">O(n)</span> time, but is rather difficult to do in practice. Leaf nodes are those that have no split, and store points.</p>
<p>The height of a k-d tree is always <span class="math inline">O(\log n)</span> since each split cuts the search space roughly in half, so search is possible in <span class="math inline">O(\log n)</span> (compare coordinate of search to coordinate of split, then recurse if necessary). The tree uses <span class="math inline">O(n)</span> space.</p>
<p>Insertion is done by finding the leaf in which we'd insert the node, insert it so the node has 2 elements, then split the node and put the 2 elements into the 2 leaves, all <span class="math inline">O(\log n)</span> overall. However, this means the tree is no longer balanced, and the medians are no longer at the medians. What we could do instead is to keep track of how unbalanced the tree is, and if it gets too bad, we just rebuild the tree entirely. There are theoretical implementations that can rebalance a k-d tree after insertion, but they are largely impractical.</p>
<p>Deletion is done by finding the leaf containing the node, removing it, and then combining the node with its parent, so it is <span class="math inline">O(\log n)</span>. There are no ways to rebalance a k-d trees after a deletion.</p>
<p>Building a k-d tree from a set of elements is possible in <span class="math inline">\Theta(n \log n)</span>, by finding the median of all the elements (in <span class="math inline">O(n)</span>), partitioning the elements into two sets, then recursively rebuilding those sets into k-d trees.</p>
<p>Range query in a k-d tree is very similar to range queries in quadtrees, but we need to ensure we keep the query bounds and the current node's bounds up to date. Overall, this is a <span class="math inline">O(n) + O(\sqrt n)</span> operation, and there is a theorem to prove it.</p>
<p>Proof:</p>
<blockquote>
<p>We color the tree with our range query values - white for nodes that are entirely outside the query bounds, black for nodes entirely inside, and gray for all others, then ignore white nodes, return black nodes, and recurse on gray nodes.<br />
Clearly, the time complexity of range queries is <span class="math inline">O(n)</span> for returning all the black nodes, plus the number of grey nodes. We want to prove that there are <span class="math inline">O(\sqrt n)</span> grey nodes.<br />
Gray nodes are possible when parallel edges of the query rectangle intersect one edge of the node, when parallel edges of the query rectangle intersect parallel edges of the node, when the query rectangle is entirely inside the node, and when perpendicular edges of the query rectangle intersect perpendicular edges of the node.<br />
Clearly, every gray node must have at least one intersection - there are more intersections than gray nodes.<br />
Clearly, the largest number of intersection points between a line and a rectangle is proportional to the largest number of intersection points between a rectangle and a rectangle, so they have the same order.<br />
Let <span class="math inline">Q(n)</span> be the largest number of intersection points between a horizontal/vertical line and the node's rectangle or its children's rectangles. Clearly, the number of gray nodes is <span class="math inline">O(Q(n))</span>.<br />
Clearly, <span class="math inline">Q(n) \le 2 + Q(\frac n 4) + Q(\frac n 4) = O(\sqrt n)</span>, since there must be at least 2 points intersected in the node's rectangle, and the line can intersect at most <span class="math inline">Q(\frac n 4)</span> points in the descendents.<br />
Since there are <span class="math inline">O(\sqrt n)</span> intersections, there are <span class="math inline">O(\sqrt n)</span> gray nodes.&lt; So there are at most <span class="math inline">O(\sqrt n)</span> recursions, and the time complexity is <span class="math inline">O(n) + O(\log n)</span> as required.</p>
</blockquote>
<p>We can use a strategy where we don't always alternate which dimension to split along to improve the rectangle shapes (ideally they would all be squares), which can sometimes reduce tree height.</p>
<p>We can also reduce height by storing points in interior nodes, which means that the points are on the split lines. If not, then we can also split at non-point coordinates - all split coordinates are valid.</p>
<h1 id="section-17">12/3/15</h1>
<p>Range trees are another way to implement dictionaries that use points as keys. This tree has the advantage of not having the bad time complexities of quadtrees and kd-trees. Also, it allows us to do range queries in a much simpler way.</p>
<p>Basically, a range tree stores points in a balanced BST by the X coordinate of each point. Every node in the tree also contains a balanced BST, which contains the node's points and the points of all of the node's descendents, by the Y coordinate of each point. When we compare points, we compare the X coordinates, then the Y coordinates if the X coordinates are equal.</p>
<p>This means that points will be stored multiple times in the same tree. Each point will appear in the tree at most once per level of the tree, and is proportional to the height. Therefore, range trees need O(n n) memory, since each node needs <span class="math inline">O(\log n)</span> memory.</p>
<p>Searching a range tree is as easy as using binary search to find a point in the main tree with the same X coordinate, then search in the node's tree for the one with the same Y coordinate, all possible in <span class="math inline">O(\log n)</span>.</p>
<p>Inserting is simply inserting the new point into the main tree, then going upwards and inserting the point into the tree for each ancestor node of the new node and rebalancing those trees. However, we can't rebalance the main tree because that would ruin the node trees, so we basically have to rebuild it every so often. Deletion is a similar operation.</p>
<p>To do a range query within <span class="math inline">(x_1, x_2)</span> to <span class="math inline">(x_2, y_2)</span>, we do a range query in the main tree for <span class="math inline">x_1</span> to <span class="math inline">x_2</span>. Now we color the tree using this query: the nodes that are on the paths of the boundary nodes in the range query are gray, the nodes that are entirely inside the range query are black, and the ones that are entirely outside are white. We ignore white nodes, do an explicit test for whether the point is in the range for gray nodes, and know that the point is in the range for black nodes. For points that we know are in the X range, we can now do a range query for <span class="math inline">y_1</span> to <span class="math inline">y_2</span> for each point, and combined, this is the result of the original range query. Since there are <span class="math inline">O(\log n)</span> of each type of node, and the range query in the node tree is <span class="math inline">O(n)</span>, range queries are <span class="math inline">O(\log^2 n) + O(n)</span> (and can actually be a bit better with certain tricks).</p>
<h2 id="tries">Tries</h2>
<p>Dictionaries for words are relatively common. There are dedicated data structures that make it very simple to associate data with words. Recall that words are sequences of characters, and characters are elements of a finite set, an alphabet (commonly <span class="math inline">\Sigma = \set{0, 0}</span> or ASCII values). For this purpose we can use a <strong>trie</strong>.</p>
<p>A trie is an <span class="math inline">n</span>-ary tree with edges labelled with characters from <span class="math inline">\Sigma</span>, or the end of word symbol (by convention, we usually use $). Basically, words are stored as the sequence of edges in paths from the root node to leaf nodes character by character, where the <span class="math inline">i</span>th level corresponds to the <span class="math inline">i</span>th character (or the end of word symbol) of a word. Tries do not contain any unnecessary edges. At the leaf nodes we could store the whole word, or any data we want:</p>
<pre><code>     / word: 1
    /$
  / \1
 /   \
/1    word: 11
\0
 \   word: 0
  \ /$</code></pre>
<p>One variation of this is to store words at internal nodes instead of using the end of word symbol. This might be beneficial if using an end of word symbol makes implementation difficult. We could also not store the words at all, and instead keep track of all the edge labels as we recurse downward to reconstruct the word.</p>
<p>Searching for a word is as easy as iterating through each character and taking the edge labelled by that character as the new tree to search in. If a leaf node is reached, we found the word. Otherwise, one of the labelled edges we want don't exist and the word is not in the trie. The runtime of this operation is <span class="math inline">O(\abs{w}\abs{\Sigma})</span>, where <span class="math inline">w</span> is the word and <span class="math inline">\Sigma</span> is the alphabet.</p>
<p>To insert a node into a trie, we follow edges corresponding to characters in the word (creating them as necessary) until we arrive at some node, then create an edge labelled with the end of word symbol to a leaf node and store the data that leaf node. To delete a node from a trie, we find where it should be in the trie, then delete that node and its ancestors up to but not including the first one with 2 or more children, to remove unnecessary edges. Insertion is <span class="math inline">O(\abs{w}\abs{\Sigma})</span>, and deletion is <span class="math inline">O(\abs{w})</span>.</p>
<p>We can also compress tries by allowing edges to store more than one character. This allows us to write them in a cleaner way, but still uses the same asymptotic memory.</p>
<p>A <strong>Patricia trie</strong> is a special type of trie in which nodes are also labelled with the index of the character to test, in addition to edges labelling characters. In a normal trie, the level of the node determines the index of the haracter we test; a node at level 5 means we take edges corresponding to the 5th character of the word. In a Patricia trie, we can test any character at any node. If the index is greater than the index of the end of word symbol in the word, then we take that to mean that the word is not in the Patricia trie.</p>
<p>Patricia tries can actually compress tries - we can simply make tries that avoid checking characters that are the same in all the descendents (for example, if all the descendents have the second character be 0, then we can just skip checking the second character in that subtree), and since we store the entire word at each leaf node, we can't get to a leaf node by following a word that isn't actually in the trie.</p>
<p>Patricia tries use less space in practice, though don't really improve time complexities.</p>
<h1 id="section-18">17/3/15</h1>
<h2 id="pattern-matching">Pattern Matching</h2>
<p>Pattern matching is the problem of finding strings inside other strings, like what <code>grep</code> does.</p>
<p>Formally, the problem is checking for the existance and finding the position of the string <span class="math inline">P[0 \ldots m - 1] \in \Sigma^*</span> within the string <span class="math inline">T[0 \ldots n - 1] \in \Sigma^*</span>, where <span class="math inline">\Sigma</span> is a set of characters. Here, <span class="math inline">P</span> is the <strong>pattern/needle</strong>, <span class="math inline">T</span> is the <strong>text/haystack</strong>, and <span class="math inline">\Sigma</span> is the <strong>alphabet</strong>.</p>
<p>Generally speaking, <span class="math inline">T</span> will be much larger than <span class="math inline">P</span>. Although we care about time complexity with respect to the length of <span class="math inline">P</span>, we care much more about time complexity with respect to <span class="math inline">T</span>.</p>
<p>A <strong>substring</strong> of <span class="math inline">T</span> is a string <span class="math inline">S</span> such that <span class="math inline">S = T[i \ldots j]</span> for some <span class="math inline">i, j</span>. A <strong>prefix</strong> is a string <span class="math inline">S</span> such that <span class="math inline">S = T[0 \ldots i]</span> for some <span class="math inline">i</span>. A <strong>suffix</strong> is a string <span class="math inline">S</span> such that <span class="math inline">S = T[i \ldots n - 1]</span> for some <span class="math inline">i</span>.</p>
<p>An <strong>occurrence</strong> of <span class="math inline">P</span> in <span class="math inline">T</span> is an index <span class="math inline">s</span> such that <span class="math inline">T[s \ldots s + (m - 1)] = P[0 \ldots m - 1]</span>. If <span class="math inline">s</span> is an occurrence of <span class="math inline">P</span> in <span class="math inline">T</span>, then we say <span class="math inline">P</span> occurs in <span class="math inline">T</span> at shift <span class="math inline">s</span>. Note that the equality operator over strings doesn't take constant time - checking if an index <span class="math inline">s</span> is an occurrence takes <span class="math inline">\Theta(m)</span> time in the worst case.</p>
<p>The brute force algorithm for pattern matching is just to try every possible shifts and return the first one that works:</p>
<pre><code>def find(needle, haystack):
    for s in range(len(haystack) - len(needle)):
        for i, c in enumerate(needle):
            if c != haystack[s + i]: break
        else: return True
    return False</code></pre>
<p>Clearly, this is <span class="math inline">\Theta(m(n - m + 1))</span> in the worst case. For small, constant <span class="math inline">m</span> this is actually relatively fast, especially in practice.</p>
<p>However, we are clearly doing a lot of unnecessary comparisons. For example, if we have a pattern &quot;gattaca&quot; and text &quot;gattaccgattact&quot;, we would fail the match at the second &quot;c&quot;, but since we know we didn't match, we could just skip 7 characters ahead and start at the second &quot;g&quot;, avoiding the need to check all the shifts in between. For longer patterns, this gives huge performance gains.</p>
<p>The idea is to improve runtime by using preprocessing - doing a lot of work upfront, but then making later searches very fast. We could preprocess the text (suffix trees do this), or preprocess the pattern (DFAs, KMP, Boyer-Moore, Rabin-Karp all do this).</p>
<p>String matching using DFAs is always <span class="math inline">O(n)</span>. An NFA is easy to construct, but the DFA is not - converting an NFA to a DFA results in exponentially increasing numbers of states. Instead, we have to design the DFA directly. For example, if we are matching <span class="math inline">P = abac</span> in <span class="math inline">\Sigma = \set{a, b, c}</span>, then we have the following DFA:</p>
<pre><code># one transition per each character of the pattern
start &gt; a &gt; a
a &gt; b &gt; ab
ab &gt; a &gt; aba
aba &gt; c &gt; abac
abac &gt; accept

# transitions from each state to the state with the longest value that is a suffix of the current state
a &gt; c &gt; start
a &gt; a &gt; a
ab &gt; b, c &gt; start
aba &gt; a &gt; a
aba &gt; b &gt; ab</code></pre>
<p>The basic algorithm is to add a state for each prefix of <span class="math inline">P</span>, and for each combination of prefix <span class="math inline">w</span> and next character <span class="math inline">x</span>, add a transition from the state <span class="math inline">w</span> to the state with the longest prefix <span class="math inline">v</span> that is a suffix of <span class="math inline">wx</span>.</p>
<p>Finding the DFA is possible in quadratic time, or cubic using a naive solution. However, this is far too slow in practice.</p>
<p>Instead, we can use <span class="math inline">\epsilon</span>-NFAs, that we make deterministic. We enforce that we only take the <span class="math inline">\epsilon</span> transition if there are no other possible transitions. Basically, we have the standard matcher, but at each state we have an <span class="math inline">\epsilon</span> transition that tells us where to go if the <span class="math inline">i</span>th character doesn't match. Since there is only one epsilon transition, with our rule the <span class="math inline">\epsilon</span>-NFA is deterministic.</p>
<p>For example, the <span class="math inline">\epsilon</span>-NFA for <span class="math inline">P = abac</span> appears as follows:</p>
<pre><code># one transition per each character of the pattern
start &gt; a &gt; a
a &gt; b &gt; ab
ab &gt; a &gt; aba
aba &gt; c &gt; abac
abac &gt; accept

# epsilon transitions to the failure states
a &gt; &gt; start
ab &gt; &gt; start
aba &gt; &gt; a

# transitions from start to start for all non-matching first characters
start &gt; b &gt; start
start &gt; c &gt; start</code></pre>
<p>This is the <strong>Knuth-Morris-Pratt algorithm</strong> (KMP) - using <span class="math inline">\epsilon</span>-NFAs to match strings.</p>
<p>The idea is to describe the <span class="math inline">\epsilon</span>-NFA using a <strong>failure function</strong> <span class="math inline">\pi(k)</span> - the state to go to given that we have seen <span class="math inline">P[0 \ldots k - 1]</span> in the text and the next character of <span class="math inline">T</span> is not <span class="math inline">P[k]</span>. We can represent the failure function using a table where the columns are <span class="math inline">k</span> values and the rows are the prefixes with lengths equal to those <span class="math inline">k</span> values and the corresponding values of <span class="math inline">\pi(k)</span>.</p>
<p>In code, we can represent each state by the length of the prefix it represents, where the start state is 0 and the state representing the prefix &quot;aba&quot; represents 3. This looks like the following:</p>
<pre><code>def kmp(T, P):
    compute the failure function $\pi(k): 1 \ldots m - 1 \to 0 \ldots m - 1$ as `failure_transition`
    state = 0
    i = 0
    while i &lt; len(T):
        if T[i] == P[state]: # match/forward transition
            state += 1 # move forward in pattern
            i += 1 # move forward in text
            if state == len(P): return True
        else: # mismatch/backward transition
            if state &gt; 0: state = failure_transition[state] # follow epsilon transition
            else: i += 1 # at start state, stay at start state and move forward in text</code></pre>
<p>Clearly, each run of the loop either does a forward transition, which consumes a character and therefore can happen at most <span class="math inline">n</span> times, or we do a backward transition, which is always followed by a forward transition (since the state we transition to must be able to consume the next character), and therefore can happen at most <span class="math inline">n</span> times. As a result, the loop can run at most <span class="math inline">O(n + n) = O(n)</span> times, and is therefore the algorithm's worst case time complexity.</p>
<h1 id="section-19">19/3/15</h1>
<p>The <span class="math inline">\epsilon</span>-NFA for the failure function has states for each character of the pattern, from 1 to the length of the pattern, and the start state is 0. Each state is therefore associated with an index, and the character at that index in the pattern. Each state is also associated with a prefix, which is all the characters of the pattern from the start of the pattern to the state's index. Each state has a transition to its next state by its character.</p>
<p>If we are in state <span class="math inline">j</span>, then we have already matched <span class="math inline">P[0 \ldots j - 1]</span>. Formally, we are always in the state <span class="math inline">\max\left(\set{j \middle| P[0 \ldots j - 1] \text{ is a suffix of the text input read so far}}\right)</span>. Basically, the longest prefix of the pattern that is a suffix of the currently read input.</p>
<p>The failure function, <span class="math inline">\pi(k) = j</span>, describes the epsilon transition at a state - the state we should hop back to if matching the next character failed to match. Since we hop backward, we have <span class="math inline">j &lt; k</span>.</p>
<p>Clearly, this is the same as <span class="math inline">\max\left(\set{j \middle| j &lt; k, P[0 \ldots j - 1] \text{ is a suffix of } P[0 \ldots k - 1]}\right)</span> - the epsilon transition doesn't consume any characters. Since <span class="math inline">j &lt; k</span>, we can just say the failure function's value is <span class="math inline">\max\left(\set{j \middle| P[0 \ldots j - 1] \text{ is a suffix of } P[1 \ldots k - 1]}\right)</span>.</p>
<p>In other words, the failure function goes to the <strong>rightmost state whose prefix is a proper suffix of the current state's prefix</strong> (a proper suffix is a suffix that is not equal to the string).</p>
<p>When we step the <span class="math inline">\epsilon</span>-NFA on <span class="math inline">P[1 \ldots k - 1]</span>, the state it is in at the end is the rightmost state whose text value is a proper suffix of the current state's text value. This is a suffix because the <span class="math inline">\epsilon</span>-NFA would go back to a previous state if it wasn't, is a proper suffix because <span class="math inline">P[1 \ldots k - 1]</span> is shorter than <span class="math inline">P[0 \ldots k - 1]</span> (the current node's prefix), and is the longest prefix because the <span class="math inline">\epsilon</span>-NFA always matches characters whenever possible, avoiding <span class="math inline">\epsilon</span>-transitions unless there is no other option. Therefore, it satisfies all the conditions to be the value of the failure function at <span class="math inline">k</span>.</p>
<p>Note that we don't yet know what the <span class="math inline">\epsilon</span>-transitions for the <span class="math inline">\epsilon</span>-NFA are yet. However, we do know that <span class="math inline">\pi(0)</span> is undefined, and <span class="math inline">\pi(1) = 0</span>. Given this, we can use strong induction to find the rest.</p>
<p>That means that if we run the <span class="math inline">\epsilon</span>-NFA on <span class="math inline">P[1 \ldots m - 1]</span>, then right before we read the <span class="math inline">n</span>th character to step the NFA, the state of <span class="math inline">\epsilon</span>-NFA is the value of <span class="math inline">\pi(n + 1)</span>. Now we have everything we need to calculate a lookup table for <span class="math inline">\pi(k)</span>:</p>
<pre><code>def calc_kmp_table(P):
    table = []
    table[0] = -1 # state 0 has no epsilon transition, so this is undefined
    table[1] = 0 # state 1 always fails to the start state
    input_position = 1 # start at the second character
    current_state = 0
    while input_position + 1 &lt; len(P): # `input_position + 1` is the state for which we are calculating the failure function at
        if P[input_position] == P[current_state]: # continue prefix matching
            table[input_position + 1] = current_state # store the rightmost state whose prefix is a proper suffix of the target state&#39;s prefix
            input_position += 1 # move ahead in the input
            current_state += 1 # move to the next state
        elif current_state != 0: # only non-start states have epsilon transitions
            current_state = table[current_state] # go back to current state&#39;s epsilon transition, which exists by inductive hypothesis
        else: # at start state and prefix doesn&#39;t match
            table[input_position + 1] = 0 # no prefixes, go back to start state
            input_position += 1 # move ahead in the input, since input is an unnacceptable first character
    return table</code></pre>
<p>Clearly, this takes <span class="math inline">O(m)</span> time, by the same argument as the one for the matching routine. So KMP uses <span class="math inline">O(m)</span> time to compute the failure function, and <span class="math inline">O(n)</span> time to match the string, so takes <span class="math inline">O(m + n)</span> time overall. Note that it does not depend on the size of the alphabet. Theoretically speaking, KMP has the best time complexity, but in practice it is often slower than certain other algorithms with worse time complexities.</p>
<p>The <strong>Boyer-Moore</strong> algorithm is basically the brute force algorithm plus a few tricks. The first is the <strong>looking glass heuristic</strong>: when we are comparing the pattern to the current part of the string by testing whether <span class="math inline">P[0 \ldots m - 1] = T[s \ldots s + m - 1]</span>, we compare from <span class="math inline">m - 1</span> to 0 (the characters are compared backward).</p>
<p>Another trick is the <strong>character-jump heuristic</strong>: if we are checking a pattern and <span class="math inline">T[s + i] \ne P[i]</span>, then we find the character <span class="math inline">T[s + i]</span> in <span class="math inline">P</span> and shift the pattern over to that character. For example, if we want to match &quot;bacaca&quot; in &quot;bacabaaacaacaba&quot;, we try to compare using the looking-glass heuristic and notice that the second to last character &quot;b&quot; doesn't match the &quot;c&quot; in the pattern. Clearly, the next match cannot occur before we find another &quot;b&quot; for &quot;bacaca&quot; to match, so we shift over by 4 to start matching the pattern on &quot;baaacaacaba&quot;.</p>
<p>Finally, there is the <strong>good-suffix heuristic</strong>: if we are checking a pattern and <span class="math inline">T[s + i] != P[i]</span>, then we shift by a value that matches <span class="math inline">P[i + 1 \ldots m - 1]</span>. The shift is the largest <span class="math inline">k</span> such that <span class="math inline">P[k + 1 \ldots m - 1 + (k - j)] = P[j + 1 \ldots m - 1]</span> and <span class="math inline">P[k] \ne P[j]</span>.</p>
<p>The shifts in the above two heuristics only depend on the pattern itself. As a result, we can precompute these before we match any text input at all. The code looks something like the following:</p>
<pre><code>def boyer_moore(T, P):
    last = [-1] * len(P);  for i, c in enumerate(P): last[c] = i # calculate shifts for character-jump heuristic
    suffix = compute_suffix_function(P) # good suffix heuristic (`suffix[j]` is the amount to shift by if we see $P[j + 1 \ldots m - 1]$, can be computed in $O(n^3)$)
    i = m - 1;
    while i &lt; n: # compare $T[i - (m - 1) \ldots i]$ with $P$
        j = m - 1
        while j &gt;= 0 and T[i] == P[j]: i, j = i - 1, j - 1
        if j == -1: return True
        i = i + m - 1 - min(suffix(j), last[T[i]])</code></pre>
<p>Without the good suffix heuristic, the function is very simple to implement. However, it makes the runtime <span class="math inline">O(m + n + \abs{\Sigma})</span>. The basic idea behind Boyer-Moore is that it skips as much text as possible - some characters of the string aren't even processed at all. In practice, the algorithm skips around a quarter of characters in English text.</p>
<p>The <strong>Rabin-Karp algorithm</strong> preprocesses the text rather than the pattern, though the preprocessing depends on the pattern. Basically, we need a hash function that takes <span class="math inline">O(1)</span> time given <span class="math inline">T[i \ldots i + (m - 1)]</span>, <span class="math inline">P</span>, and <span class="math inline">h(T[i - 1 \ldots i - 1 + (m - 1)])</span> - a hash function that can be computed in constant time using the previous result.</p>
<p>The idea is to eliminate many substring comparisons using the hash function. The hash function can be computed at the same time as we are matching in the string.</p>
<p>For example, assume we are using binary as the alphabet. For each substring of <span class="math inline">T</span> with length <span class="math inline">m</span>, we count the number of 1's, which can be done in <span class="math inline">O(n)</span> time overall and constant memory. Then, we only need to compare <span class="math inline">P</span> to those substrings that contain the same number of 1's as <span class="math inline">P</span> does.</p>
<h1 id="section-20">24/3/15</h1>
<p><strong>Suffix trees</strong> are another approach to pattern matching, and preprocesses the text rather than the pattern, like Rabin-Karp. This means that it is well suited for when we are searching for many different patterns in the same text.</p>
<p>A <strong>suffix trie</strong> is an uncompressed version of a suffix tree.</p>
<p>The main idea is that if <span class="math inline">P</span> is a substring of <span class="math inline">T</span>, then <span class="math inline">P</span> is a prefix of a suffix of <span class="math inline">T</span>. In other words, if we store a trie containing all the possible suffixes of <span class="math inline">T</span>, then <span class="math inline">P</span> is a substring of <span class="math inline">T</span> if and only if <span class="math inline">P</span> is within the trie starting at the beginning - the trie lets us check for prefixes of all the suffixes at once. So for <code>T = \text{test}</code>, we would store the empty string, <code>t</code>, <code>st</code>, <code>est</code>, and <code>test</code> in the trie.</p>
<p>However, this trie would waste a lot of memory. A <strong>suffix tree</strong> is simply a compressed version of this trie. Basically, when we have a node with only one child that also only has one child, we can merge the three nodes into 2. For example, if we have a structure like <span class="math inline">t(e(s(s, t)))</span> (the trie with &quot;tess&quot; and &quot;test&quot;), we can change this to <span class="math inline">tes(s, t)</span>.</p>
<p>This is much shorter to draw, but as we observed earlier for tries this doesn't save any space because we have to store the edge labels and leaves anyways. However, note that all labels and leaves are simply substrings of the text, and therefore can be represented using pairs of numbers representing the substring's start and end positions within the text.</p>
<p>The suffix tree is therefore a compressed trie of suffixes of text, where labels and words are represented using indices representing substrings of the text. For example, &quot;bananaban&quot; becomes <span class="math inline">a(ban\$(aban\$, n(a(ban\$(anaban\$)), \$(an\$)))), ban(anaban\$(bananaban\$), \$(ban\$)), n(a(ban\$(naban\$), naban\$(nanaban\$)), \$(n\$)), \$(\$)</span>, where $ is the end-of-word char and <span class="math inline">x(\ldots)</span> means that there is an edge labelled <span class="math inline">x</span> to a node <span class="math inline">\ldots</span> and plain words are leaves.</p>
<p>Clearly, <span class="math inline">T</span> has <span class="math inline">\abs{T}</span> suffixes and therefore <span class="math inline">\abs{T}</span> suffix tree leaves. Therefore, there are <span class="math inline">n - 1</span> or fewer internal nodes since each node has 2 or more children (if there was only 1 child then it would be merged in). Since nodes and edges take <span class="math inline">O(1)</span> space, suffix trees need <span class="math inline">O(n)</span> space.</p>
<p>Clearly, a naive solution for constructing the suffix tree is <span class="math inline">O(n^2)</span> - take each suffix, insert it into a trie, then compress. However, it is possible to do this in <span class="math inline">O(n)</span> time, though this is very complicated (fully covered in CS482).</p>
<p>Pattern matching using suffix trees is very easy:</p>
<pre><code>def suffix_tree_search(T, P):
    build the suffix tree with root node as `S`
    pattern_position = 0
    while True:
        if pattern_position &gt;= len(pattern): return True
        if S.is_leaf: return True
        for edge in S:
            if edge.startswith(P[pattern_position:pattern_position + len(edge)]):
                pattern_position += len(edge)
                S = S[edge]
                break
        else:
            return False</code></pre>
<p>In summary:</p>
<table>
<thead>
<tr class="header">
<th style="text-align: left;">Property</th>
<th style="text-align: left;">Brute Force</th>
<th style="text-align: left;">KMP</th>
<th style="text-align: left;">Boyer-Moore</th>
<th style="text-align: left;">Rabin-Karp</th>
<th style="text-align: left;">Suffix Trees</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Preprocessing time</td>
<td style="text-align: left;"><span class="math inline">O(1)</span></td>
<td style="text-align: left;"><span class="math inline">O(m)</span></td>
<td style="text-align: left;"><span class="math inline">O(m + \abs{\Sigma})</span></td>
<td style="text-align: left;"><span class="math inline">O(n)</span></td>
<td style="text-align: left;"><span class="math inline">O(n)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">Search time</td>
<td style="text-align: left;"><span class="math inline">O(mn)</span></td>
<td style="text-align: left;"><span class="math inline">O(n)</span></td>
<td style="text-align: left;"><span class="math inline">O(n)</span></td>
<td style="text-align: left;"><span class="math inline">O(mn)</span></td>
<td style="text-align: left;"><span class="math inline">O(m)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;">Space</td>
<td style="text-align: left;"><span class="math inline">O(1)</span></td>
<td style="text-align: left;"><span class="math inline">O(m)</span></td>
<td style="text-align: left;"><span class="math inline">O(m + \abs{\Sigma})</span></td>
<td style="text-align: left;"><span class="math inline">O(n)</span></td>
<td style="text-align: left;"><span class="math inline">O(n)</span></td>
</tr>
</tbody>
</table>
<p>KMP has the best asymptotic complexity overall. Boyer-Moore is often sublinear and best in practice. Rabin-Karp is also very fast in practice, especially with a good hash function. Suffix trees are very good when we are searching for different patterns in the same text, while all the others are good for same patterns in different texts.</p>
<h2 id="text-compressionencoding">Text Compression/Encoding</h2>
<p>Suppose we want to transmit text between two parties. Most communication channels work with bit strings, so we would have to encode the text into a bitstring when we send it, and decode the bitstring when we receive it.</p>
<p>We want an <strong>encoding/compression algorithm</strong> that converts a word into a bitstring, and a <strong>decoding/decompression algorithm</strong> that converts a bitstring back into the original word.</p>
<p>The main objective here is to make the resulting bitstrings to be as short as possible, in order to save space.</p>
<p>There is <strong>character-by-character/fixed-length encoding</strong>, which converts each character into a fixed-length bit strings, like ASCII. However, this can't represent every character. The UTF-32 standard, for example, uses 32 bits per character in order to represent every language properly.</p>
<p>There is also <strong>variable-length encoding</strong>, where characters could be mapped to bit strings of varying lengths. For example, Morse code (Morse code actually uses 4 symbols - dot, dash, end of character, end of word), UTF-8, and UTF-16. With this, the length of the encoding is simply the sum of the frequency of each character times the length of the encoding for that character.</p>
<p>The problem with variable-length encoding is that it is easier to accidentally design one where there exists a bitstring that has two possible decoded strings. This happens whenever the code for one character is a prefix of the code for another (the trie with all the character encodings has no words at interior nodes).</p>
<p>A variable-length encoding where no code for one character is a prefix of the code for another character is a <strong>prefix code</strong>.</p>
<p>We want to use the smallest possible bitstring encoding for encoded text given frequencies for each of the characters. There is a simple algorithm to do this if we are given the frequency function <span class="math inline">f(c)</span>.</p>
<p>First, we find the two characters <span class="math inline">c_1, c_2</span> with the smallest frequencies - the ones that occur least often. We then add these two as children to an internal node of a trie (with edges labelled 0 and 1), and treat that new internal node as if it were a character with frequency <span class="math inline">f(c_1) + f(c_2)</span> (and no longer consider <span class="math inline">c_1, c_2</span> as characters anymore).</p>
<p>We then repeatedly select the new two characters/internal nodes with the smallest frequencies and repeat this process. When we only have 1 character/internal node, we have created a trie with the best possible prefix encoding. This is the basic idea behind <strong>Huffman encoding</strong>.</p>
<h1 id="section-21">26/3/15</h1>
<p>We can use the following process to get the smallest encoding trie:</p>
<pre><code>def smallest_encoding(frequency_function, character_list):
    let `heap` be a min-heap of tries # this is to allow us to efficiently select the lowest frequencies later
    for character in character_list:
        let `new_node` be a trie leaf node with `character` as its value and `frequency_function(character)` as its tag
        insert `new_node` into the heap indexed by its tag
    # we could also make the above more efficient by building heap in place
    
    while len(heap) &gt;= 2:
        smallest1, smallest2 = heap.pop_min(), heap.pop_min()
        let `new_node` be a trie internal node with `smallest1` and `smallest2` as children (order doesn&#39;t matter), and `smallest1.tag + smallest2.tag` as its tag
        insert `new_node` into the heap indexed by its tag
    # at this point, the heap contains only one trie, which represents the best prefix code</code></pre>
<p>This is called <strong>Huffman encoding</strong>. Clearly, the heap starts off size <span class="math inline">d = \abs{\Sigma}</span>, and in every iteration of the while loop we decrease the size of the heap by 2 and then increase it by 1, so we have <span class="math inline">d</span> runs of the while loop, which itself takes <span class="math inline">O(\log n)</span> time. Therefore, Huffman encoding takes <span class="math inline">O(d \log d)</span> time.</p>
<p>Suppose we have <span class="math inline">f(s) = 2, f(e) = 2, f(v) = 1, f(n) = 3, f(_) = 1, f(b) = 1, f(a) = 3</span>, from the string &quot;seven bananas&quot;. Using the above process, we might get something like <span class="math inline">b \to 000, s \to 001, e \to 010, v \to 0110, _ \to 0111, n \to 10, a \to 11</span>.</p>
<p>There is a theorem that states that for any given set of frequencies, the above algorithm finds a prefix code that yields the shortest possible encoding in all bitstring-based prefix codes.</p>
<p>The problem with Huffman encoding is that we also need to send the dictionary (mapping bitstrings to characters) along with the encoded value, or the frequencies and tie-breaking rules. This adds a lot of overhead for smaller inputs, which occurs very often in practice.</p>
<p>Additionally, we have to go through the input twice - once to get the frequencies, and once to actually encode it. This is bad if we are working with huge amounts of data that can't be randomly accessed. We could potentially use the standard English text frequencies, but that would work poorly for non-English inputs. One approach is to use a fixed-size sample of the input to get the frequencies, but this also could perform poorly if the sample is non-representative.</p>
<p>There are two approaches to text compression. One is to encode multiple characters at once, and use the text to guide which characters to group together for encoding.</p>
<h3 id="run-length-encoding">Run-length Encoding</h3>
<p>Suppose we want to compress a bitstring (so <span class="math inline">\Sigma = \set{0, 1}</span>). If we have a string like &quot;1111100000001100000000&quot;, run-length encoding can compress this into 15728.</p>
<p>In real-world data, we often have long runs of 0 and long runs of 1. We can encode these runs with the length of each run - &quot;1111100000001100000000&quot; would become 5728, because there are 4 runs, with length 5, 7, 2, and 8, respectively. However, this also represents the string &quot;0000011111110011111111&quot;, so we need to prepend the first bit to disambiguate.</p>
<p>However, we still need to actually represent the run lengths as a bitstring in order for this actually to be an encoding. Note that the run lengths can get arbitrarily large, so we can't just directly implement them using a fixed number of bits. We could solve this by breaking overly long runs of 1 into two runs of 1 separated by a zero-length run of 0.</p>
<p>If we know the length of the input, we can just encode that value into the output, by putting that many 0 bits before every run length:</p>
<pre><code>def run_length_encode(value):
    write out the first bit of the string
    while not at the end of the string:
        let `k` be the length of the next run
        write a 0 bit `floor(log(k) / log(2))` times
        write `k` as binary and let `bits` be the number of bits written</code></pre>
<p>To decode this, we can do the following:</p>
<pre><code>def run_length_decode(value):
    current_bit = value[0]
    i = 1
    while i &lt; len(value) - 1:
        l = 0; while value[i] == 0: l += 1; i += 1
        k = int(value[i:i + l + 1], 2)
        write out `current_bit` `k` times
        flip `current_bit`</code></pre>
<p>Note that run-length encoding can actually result in a longer bitstring than the original. There is a threshold for average run-length such that if the average run length is below it, the output will actually be larger than the input. As a result, run-length encoding is really suited only for applications where we expect long runs of the same valule, like compressing some types of images.</p>
<h3 id="lempel-ziv-welch-compression">Lempel-Ziv-Welch Compression</h3>
<p>This is used extensively in the real world, since it is so effective. The basic idea is to create the dictionary while scanning and encoding the input.</p>
<p>We will assume that the input is an ASCII string, and the output is a sequence of 12-bit bitstrings.</p>
<p>We have a dictionary <span class="math inline">d(k) \to v</span> where <span class="math inline">k</span> is an ASCII string and <span class="math inline">v</span> is a 12-bit bitstring. It is initialized with all the ASCII characters mapped to their values - NUL to DEL mapped to 0 to 127.</p>
<p>Suppose our text is &quot;YO!_YOU!_YOUR_YOYO!&quot;. First, we read in a character to get &quot;Y&quot;, and find that it is in the dictionary, so we output the value, 0x089. Then, we read in the next character to get &quot;YO&quot;, which is not in the dictionary. We then add &quot;YO&quot; to the dictionary, mapped to the next available code, which is 128, and clear all read input except the last character read, so when we read the next character, we get &quot;O!&quot; rather than &quot;YO!&quot; or &quot;!&quot;.</p>
<p>We then repeat this process of reading inputs, checking if they're in the dictionary, and outputting the code if it is and resetting if it isn't, until the input is exhausted. At the end we might print out the code for the one character in the read input if there is one.</p>
<p>We can implement this as follows:</p>
<pre><code>def lzw(value):
    populate a dictionary `dictionary` populated with ASCII values
    while True:
        let `prefix` be the longest prefix of the remaining input that is in `dictionary`
        output the 12-bit number `dictionary[prefix]`
        let `c` be the result of peeking at the next character in the input
        dictionary[prefix + c] = len(dictionary) # `len(dictionary)` is always the next free bitstring code</code></pre>
<p>We can actually store the dictionary very efficiently using a trie, since our alphabet is fixed to ASCII.</p>
<h1 id="section-22">31/3/15</h1>
<p>LZW decoding does not require a dictionary - we can figure it out from the data itself, as long as we know the starting dictionary (for ASCII text, this is mapping from characters to their corresponding ASCII codes). The idea is to build the dictionary again as we are decoding the string.</p>
<p>We can decode by looking up the next code in the encoded input, and adding the previous string plus the first character of the next string to the dictionary, at the next code number.</p>
<p>For example, if we have <span class="math inline">67, 65, 78, 32, 66, 129, 133, 83</span>, then we know which strings all values below 128 correspond to. As a result, we know that the string is &quot;CAN B<span class="math inline">129, 133</span>S. As we are doing this, we are building the dictionary. By the time we get to 129, we have, in addition to the initial mappings, &quot;CA&quot; to 128, &quot;AN&quot; to 129, &quot;N &quot; to 130, &quot; B&quot; to 131, and &quot;BA&quot; to 132 - all the pairs of characters. However, 133 is special because we still don't have an entry in the dictionary for it.</p>
<p>This can be implemented as follows:</p>
<pre><code>def lzw_decode(encoded): # encoded is an array of 12-bit numbers
    let `D` be a trie populated with the codes for all the ASCII characters
    code = encoded[0]
    result = &quot;&quot;
    value = D[code]
    result += value
    for code in encoded[1:]: # go through the rest of the characters
        previous_value = value
        if code != len(D): # the code is still in the dictionary
            value = D[code]
        else: # unseen code, not in dictionary so we construct it from previous value
            value = previous_value + previous_value[0] # the value is the previous value appended with the first character of the previous value
        result += value
        D[len(D)] = previous_value + value[0] # the new substring is the previous value appended with the first character of the current value
    return result</code></pre>
<p>Note that the dictionary is a trie, where all nodes store a code (including the interior nodes, but not the root). If a string is in the trie, then due to the algorithm so are all the non-empty prefixes are also in the trie.</p>
<p>The basic idea behind LZW encoding is to find the longest prefix in the dictionary, output its code, and then add this prefix plus the next char to the dictionary. The runtime of finding the longest prefix is <span class="math inline">\Theta(n)</span> where <span class="math inline">n</span> is the number encoded characters so far, and since our search ended at the correct place in the trie to add a child, adding the new entry to the dictionary is <span class="math inline">\Theta(1)</span>. Therefore, <strong>the overall time complexity of LZW encoding is <span class="math inline">\Theta(n)</span> where <span class="math inline">n</span> is the length of the input string</strong>.</p>
<p>In practice, a trie takes a huge amount of memory - we often use a hashing solution instead. Also, since we have a 12-bit code we can't have more than <span class="math inline">2^{12}</span> codes, so we need to make sure we don't use that many in the encoding. If we do, we need to make sure to tell the decoder that there is more text to encode after that code.</p>
<p>For decoding, all our code numbers are consecutive, so we can just store our dictionary as an array for <span class="math inline">\Theta(1)</span> access (and if we initialize it to <span class="math inline">2^{12}</span> elements, we won't even need to expand it).</p>
<p>Looking codes up in the dictionary takes <span class="math inline">\Theta(1)</span> time, but adding the looked up string to the result, takes <span class="math inline">O(n)</span> time where <span class="math inline">n</span> is the length of that string. Therefore, <strong>The overall time complexity of LZW decoding is <span class="math inline">\Theta(n)</span> where <span class="math inline">n</span> is the length of the decoded string</strong>.</p>
<p>In fact, we don't even need to store entire prefix strings in the dictionary while decoding - we can simply store the code of its prefix (or nothing, for the initial entries) and the last character (since the string without its last character is guaranteed to be in the dictionary). Then, when we need to look up the string by code, we just decode the stored prefix, and append the last character. This keeps our time complexity the same, but lets us store our dictionary entries in O(1) time.</p>
<p>Huffman encoding is the best 01-prefix code, but we need to send the dictionary and it needs 2 passes - it's used in PDFs. Run-length encoding is very simple, but bad on text (though good for bitmap images), and is rarely used in practice. LZW is the best compression for medium-length English text, and is used in things like JPEG and MP3.</p>
<p>Huffman encoding is always the same length or shorter than the original (without the dictionary), but all the others can possibly be longer.</p>
<p>The bzip2 compression scheme uses multiple algorithms together. The main idea is to transform the text before trying to compress it, in order to get a better compression ratio. The text is first put through a Burrows-Wheeler transform, then the move-to-front heuristic, and then Huffman encoded, before being transmitted. Upon receipt, the encoded value is put through reverse Huffman, reverse move-to-front, and then reverse Barrows-Wheeler transform.</p>
<p>The <strong>move-to-front</strong> heuristic is a way of encoding text using a dictionary. Starting with an ASCII dictionary, we apply the move-to-front method with the character to the dictionary - we move the character represented by the current code to the front of the dictionary for each character, the goal being to use the smallest possible code for each character to improve compressability later.</p>
<h1 id="section-23">2/4/15</h1>
<p>The <strong>Burrows-Wheeler transform</strong> is a transform where we compute all the cyclic shifts of a string, sort them lexicographically, write them out one below the other in a square matrix, and then output the last <strong>column</strong> of the matrix, as well as the row index within the grid that corresponds to the original text that is unshifted. If the text ends with a sentinel value, then we don't need to output the shift as well.</p>
<p>The advantage of doing this transform is that equal characters will generally get more grouped together, which makes for better compressibility. Any pattern that occurs frequently means that the first character appears consecutively in the output of the transform. The goal of this is to get a lot of repeating characters. This works really well with the move-to-front heuristic because with repeating characters, it gets a smaller encoding.</p>
<p>To do a <strong>reverse Burrows-Wheeler transform</strong>, we partially restore the matrix. First, we fill in the last column of a matrix with the given encoded value.. Note that the first column of the matrix is just a sorted version of the last column, since it is the most significant character we were sorting by while encoding. Therefore, we can fill in the first column as well.</p>
<p>Now we know the first character of the original text. Note that the first column is a cyclic shift of the word rightward by 1 character. Therefore, given any character, we know what the next character is (for duplicate characters, make sure we label each one with unique tags to avoid confusing them) - we look up a character in the last row, and the character after it is the corresponding one in the first row. So starting from the first character, we can follow the next character until we have the correct number of characters.</p>
<p>Basically, we sort the value to get all the length 2 substrings, and then use those substrings to calculate the original value.</p>
<p>The runtime for this is <span class="math inline">O(n + \abs{\Sigma})</span> - we can sort using count sort/bucket sort (since we have a fixed number of characters to sort) in <span class="math inline">O(\abs{Sigma} + n)</span> time, and recover the length 2 substrings and the original text in <span class="math inline">O(n)</span> time.</p>
<p>Decoding is pretty fast, but the encoding still seems rather inefficient. We can do better. Let <span class="math inline">T</span> be the original string, and let <span class="math inline">S = TT</span>. Clearly, all substrings of length <span class="math inline">\abs{T}</span> in <span class="math inline">S</span> are cyclic shifts of <span class="math inline">T</span>. Clearly, there is a bijection between substrings of length <span class="math inline">\abs{T}</span> and suffixes of length <span class="math inline">\abs{T}</span> or greater, and the suffixes will sort in the same order as the substrings. Therefore, we can just sort the suffixes instead of the substrings, which will make things a lot simpler.</p>
<p>Now we want to sort these suffixes. This can easily be done by building a suffix tree/trie, and deleting the suffixes that are too short (or just don't add them in the first place while building the tree). We can build this in <span class="math inline">O(n)</span> time using some clever algorithms. Now, every prefix in the trie of length <span class="math inline">\abs{T}</span> is a length <span class="math inline">\abs{T}</span> substring of the word.</p>
<p>Now we can traverse the trie in order, and for each leaf, we output the character at index <span class="math inline">\abs{T} - 1</span>. Since the suffix tree traversal is equivalent to going through each length <span class="math inline">\abs{T}</span> substring in sorted order, we basically calculate the substring the suffix corresponds to and output the relevant character. Alternatively, we could just store the character at index <span class="math inline">\abs{T} - 1</span> in the leaf while we are building the suffix tree, and directly output the leaves while traversing. With this new, faster scheme, we can perform BWT in <span class="math inline">O(n)</span> time.</p>
<p>To summarize, bzip2 compression applies BWT to improve character grouping, applies MTF to get smaller numbers, and then applies Huffman encoding to get a variable length bitstring encoding for the text. Decoding is simply the opposite process. Bzip2 compresses really well, but is rather slow in practice. Overall, it is slightly over linear time since almost all the parts have linear runtime.</p>
<h2 id="course-overview">Course Overview</h2>
<ul>
<li>Algorithm design - process, proving correctness.</li>
<li>Time complexity - upper/lower bounds, proving runtimes.</li>
<li>Advanced sorting - quicksort, heapsort, radix sort, bucket/count sort, etc.</li>
<li>Priority queues, randomized algorithms, etc.</li>
<li>Dictionaries - hashing, self-balancing trees, implementation, dictionaries for points and for words.</li>
<li>Text processing - pattern matching, compression, etc.</li>
</ul>
<p>There isn't always a best solution for problems - in many cases, there will be different solutions that are better in different cases.</p>
<p>Final is in same format as midterm, questions ask to run algorithm on some inputs, apply knowledge to solve problems, or create algorithms.</p>
<div class="status-banner" style="display: none; position: fixed; bottom: 0; left: 0; right: 0; text-align: center;">
    <div style="display: inline-block; padding: 0.8em 2em 0.5em 2em; background: black; color: white; font-size: 2em;">
        Rendering <svg xmlns="http://www.w3.org/2000/svg" height="1.4em" viewbox="0 0 1200 500" style="vertical-align: text-bottom"><title>LaTeX logo</title><g transform="matrix(45 0 0 45 40 40)" fill="white"><path d="M5.5 4.4C5.5 4.4 5.2 4.4 5.2 4.4 5.1 5.4 5 6.7 3.2 6.7 3.2 6.7 2.4 6.7 2.4 6.7 1.9 6.7 1.9 6.6 1.9 6.3 1.9 6.3 1.9 1 1.9 1 1.9 0.6 1.9 0.5 2.9 0.5 2.9 0.5 3.2 0.5 3.2 0.5 3.2 0.5 3.2 0.2 3.2 0.2 2.8 0.2 1.9 0.2 1.5 0.2 1.1 0.2 0.3 0.2 0 0.2 0 0.2 0 0.5 0 0.5 0 0.5 0.2 0.5 0.2 0.5 1 0.5 1 0.6 1 0.9 1 0.9 1 6.2 1 6.2 1 6.6 1 6.7 0.2 6.7 0.2 6.7 0 6.7 0 6.7 0 6.7 0 7 0 7 0 7 5.2 7 5.2 7 5.2 7 5.5 4.4 5.5 4.4z"/><path d="M5.3 0.2C5.3 0 5.2 0 5.1 0 5 0 4.9 0 4.9 0.2 4.9 0.2 3.3 4.2 3.3 4.2 3.2 4.4 3.1 4.7 2.5 4.7 2.5 4.7 2.5 5 2.5 5 2.5 5 4 5 4 5 4 5 4 4.7 4 4.7 3.7 4.7 3.5 4.6 3.5 4.4 3.5 4.3 3.5 4.3 3.6 4.2 3.6 4.2 3.9 3.4 3.9 3.4 3.9 3.4 5.9 3.4 5.9 3.4 5.9 3.4 6.3 4.4 6.3 4.4 6.3 4.4 6.3 4.5 6.3 4.5 6.3 4.7 5.9 4.7 5.8 4.7 5.8 4.7 5.8 5 5.8 5 5.8 5 7.7 5 7.7 5 7.7 5 7.7 4.7 7.7 4.7 7.7 4.7 7.6 4.7 7.6 4.7 7.1 4.7 7.1 4.7 7 4.5 7 4.5 5.3 0.2 5.3 0.2zM4.9 0.9C4.9 0.9 5.8 3.1 5.8 3.1 5.8 3.1 4 3.1 4 3.1 4 3.1 4.9 0.9 4.9 0.9z"/><path d="M13.3 0.2C13.3 0.2 7.2 0.2 7.2 0.2 7.2 0.2 7 2.5 7 2.5 7 2.5 7.3 2.5 7.3 2.5 7.4 0.9 7.6 0.5 9.1 0.5 9.3 0.5 9.5 0.5 9.6 0.6 9.8 0.6 9.8 0.7 9.8 0.9 9.8 0.9 9.8 6.2 9.8 6.2 9.8 6.5 9.8 6.7 8.8 6.7 8.8 6.7 8.4 6.7 8.4 6.7 8.4 6.7 8.4 7 8.4 7 8.8 6.9 9.8 6.9 10.3 6.9 10.7 6.9 11.7 6.9 12.2 7 12.2 7 12.2 6.7 12.2 6.7 12.2 6.7 11.8 6.7 11.8 6.7 10.7 6.7 10.7 6.5 10.7 6.2 10.7 6.2 10.7 0.9 10.7 0.9 10.7 0.7 10.7 0.6 10.9 0.6 11 0.5 11.3 0.5 11.5 0.5 13 0.5 13.1 0.9 13.2 2.5 13.2 2.5 13.5 2.5 13.5 2.5 13.5 2.5 13.3 0.2 13.3 0.2z"/><path d="M18.7 6.7C18.7 6.7 18.4 6.7 18.4 6.7 18.2 8.2 17.9 8.9 16.2 8.9 16.2 8.9 14.9 8.9 14.9 8.9 14.4 8.9 14.4 8.8 14.4 8.5 14.4 8.5 14.4 5.9 14.4 5.9 14.4 5.9 15.3 5.9 15.3 5.9 16.3 5.9 16.4 6.2 16.4 7 16.4 7 16.6 7 16.6 7 16.6 7 16.6 4.4 16.6 4.4 16.6 4.4 16.4 4.4 16.4 4.4 16.4 5.2 16.3 5.5 15.3 5.5 15.3 5.5 14.4 5.5 14.4 5.5 14.4 5.5 14.4 3.2 14.4 3.2 14.4 2.8 14.4 2.8 14.9 2.8 14.9 2.8 16.2 2.8 16.2 2.8 17.7 2.8 18 3.3 18.1 4.7 18.1 4.7 18.4 4.7 18.4 4.7 18.4 4.7 18.1 2.5 18.1 2.5 18.1 2.5 12.5 2.5 12.5 2.5 12.5 2.5 12.5 2.8 12.5 2.8 12.5 2.8 12.7 2.8 12.7 2.8 13.5 2.8 13.5 2.9 13.5 3.2 13.5 3.2 13.5 8.4 13.5 8.4 13.5 8.8 13.5 8.9 12.7 8.9 12.7 8.9 12.5 8.9 12.5 8.9 12.5 8.9 12.5 9.2 12.5 9.2 12.5 9.2 18.2 9.2 18.2 9.2 18.2 9.2 18.7 6.7 18.7 6.7z"/><path d="M21.7 3.1C21.7 3.1 23 1.1 23 1.1 23.3 0.8 23.6 0.5 24.5 0.5 24.5 0.5 24.5 0.2 24.5 0.2 24.5 0.2 22.1 0.2 22.1 0.2 22.1 0.2 22.1 0.5 22.1 0.5 22.5 0.5 22.7 0.7 22.7 0.9 22.7 1 22.7 1.1 22.6 1.2 22.6 1.2 21.5 2.8 21.5 2.8 21.5 2.8 20.2 0.9 20.2 0.9 20.2 0.9 20.1 0.8 20.1 0.8 20.1 0.7 20.4 0.5 20.8 0.5 20.8 0.5 20.8 0.2 20.8 0.2 20.4 0.2 19.7 0.2 19.3 0.2 19 0.2 18.4 0.2 18 0.2 18 0.2 18 0.5 18 0.5 18 0.5 18.2 0.5 18.2 0.5 18.8 0.5 19 0.5 19.2 0.8 19.2 0.8 21 3.6 21 3.6 21 3.6 19.4 6 19.4 6 19.2 6.2 18.9 6.7 17.9 6.7 17.9 6.7 17.9 7 17.9 7 17.9 7 20.3 7 20.3 7 20.3 7 20.3 6.7 20.3 6.7 19.8 6.7 19.7 6.4 19.7 6.2 19.7 6.1 19.7 6.1 19.8 6 19.8 6 21.2 3.9 21.2 3.9 21.2 3.9 22.8 6.3 22.8 6.3 22.8 6.3 22.8 6.3 22.8 6.4 22.8 6.5 22.6 6.7 22.2 6.7 22.2 6.7 22.2 7 22.2 7 22.5 6.9 23.2 6.9 23.6 6.9 24 6.9 24.5 7 24.9 7 24.9 7 24.9 6.7 24.9 6.7 24.9 6.7 24.7 6.7 24.7 6.7 24.2 6.7 24 6.6 23.8 6.3 23.8 6.3 21.7 3.1 21.7 3.1z"/></g></svg> math...
    </div>
</div>
<div class="license">
  <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-sa/4.0/80x15.png" /></a> This work by <a xmlns:cc="http://creativecommons.org/ns#" href="https://uberi.github.io/" property="cc:attributionName" rel="cc:attributionURL">Anthony Zhang</a> is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.
  Copyright 2013-2017 Anthony Zhang.
</div>
</body>
</html>
