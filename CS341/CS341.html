<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <title>CS341 | Anthony Zhang</title>
  <link rel="stylesheet" href="../css/base.css" type="text/css">
  <link rel="stylesheet" href="../css/note.css" type="text/css">
  <link rel="stylesheet" href="../highlight/styles/default.css">
  <link rel="stylesheet" href="../highlight/styles/paraiso-light.css">
  <script src="../highlight/highlight.pack.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
  <style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
  </style>
  <script src="../katex/katex.min.js" type="text/javascript"></script>
  <link rel="stylesheet" href="../katex/katex.min.css" />
  <script type="text/javascript">
  window.onload = function() {
    document.getElementsByClassName("status-banner")[0].style.display = "block";
    setTimeout(function() {
      renderMathElements(document.getElementsByClassName("math"));
      document.getElementsByClassName("status-banner")[0].style.display = "none";
    }, 50); // delay to allow status banner to show
  }

  function renderMathElements(mathElements) {
    var mathOptions = {
      macros: {
        "\\set": "\\left\\{ #1 \\right\\}",
        "\\tup": "\\left\\langle #1 \\right\\rangle",
        "\\abs": "\\left\\lvert #1 \\right\\rvert",
        "\\floor": "\\left\\lfloor #1 \\right\\rfloor",
        "\\ceil": "\\left\\lceil#1 \\right\\rceil",
        "\\mb": "\\mathbb{#1}",
        "\\rem": "\\operatorname{rem}",
        "\\ord": "\\operatorname{ord}",
        "\\sign": "\\operatorname{sign}",
        "\\imag": "\\bm{i}",
        "\\dee": "\\mathop{}\\!\\mathrm{d}",
        "\\lH": "\\overset{\\text{l'H}}{=}",
        "\\evalat": "\\left.\\left(#1\\right)\\right|",
        "\\sech": "\\operatorname{sech}",
        "\\spn": "\\operatorname{Span}",
        "\\proj": "\\operatorname{proj}",
        "\\prp": "\\operatorname{perp}",
        "\\refl": "\\operatorname{refl}",
        "\\magn": "\\left\\lVert #1 \\right\\rVert",
        "\\rank": "\\operatorname{rank}",
        "\\trace": "\\operatorname{trace}",
        "\\sys": "\\left[ #1 \\mid #2\\space \\right]",
        "\\range": "\\operatorname{Range}",
        "\\adj": "\\operatorname{adj}",
        "\\cof": "\\operatorname{cof}",
        "\\coord": "{\\left\\lbrack #1 \\right\\rbrack}_{#2}",
        "\\diag": "\\operatorname{diag}",
        "\\formlp": "\\operatorname{Form}(\\mathcal{L}^P)",
        "\\argmin": "\\operatorname{argmin}",
        "\\argmax": "\\operatorname{argmax}",
        "\\sgn": "\\operatorname{sgn}",

        // not yet available in KaTeX
        "\\bm": "\\mathbf", //wip: should be italic, but isn't
      },
    };
    for (var i=0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      try {
        katex.render(texText.data, mathElements[i], mathOptions);
      } catch (e) {
        console.error(e);
        console.log(mathElements[i]);
      }
    }
  }
  </script>
</head>
<body>
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-68271407-1', 'auto');
    ga('send', 'pageview');

  </script>
  <h1>Lecture Notes by <a href="/">Anthony Zhang</a>.</h1>
  <ul class="site_links">
    <li><a href="/blog/" class="page">blog</a></li>
    <span class="divider"></span>
    <li><a href="http://uberi.github.io/University-Notes" class="page">notes</a></li>
    <span class="divider"></span>
    <li><a href="/resume.pdf" class="page">résumé</a></li>
    <span class="divider"></span>
    <li><a href="https://github.com/Uberi" class="contact">github</a></li>
    <span class="divider"></span>
    <li><a href="https://www.linkedin.com/in/uberi/" class="contact">linkedin</a></li>
    <span class="divider"></span>
    <li><a href="mailto:me@anthonyz.ca" class="contact">email</a></li>
    <span class="divider"></span>
    <li><a href="https://www.facebook.com/anthony.zhang.user" class="contact">facebook</a></li>
    <span class="divider"></span>
    <li><a href="https://twitter.com/anthony926535" class="contact">twitter</a></li>
    <span class="divider"></span>
    <li><a href="https://keybase.io/uberi" class="info">public key</a></li>
  </ul>
<h1 id="cs341">CS341</h1>
<p>Algorithms.</p>
<pre><code>Doug Stinson
Section 001
https://www.student.cs.uwaterloo.ca/~cs341/</code></pre>
<h1 id="section">14/9/15</h1>
<p>Most materials will be on LEARN. Questions and answers can be posted on Piazza.</p>
<p>Study of the design and the analsis of algorithms - in particular, the correctness (proved via formal proofs) and efficiency (proved using time complexity analysis).</p>
<p>Useful metrics for algorithms are asymptotic complexity (best case/average case/worst case time/space complexity), number of specific computations used (like comparisons in sorting algorithms), whether an algorithm is the most efficient for a particular problem, etc.</p>
<p>Interesting things to know are lower bounds on possible algorithms to solve a particular problems, problems that cannot be solved by any algorithm (undecidability), and problems that cannot be solved efficiently by any algorithm, but can be solved (NP-hardness).</p>
<p>Useful design strategies we will go over are divide and conquer, greedy algorithms, dynamic programming, breadth-first and depth-first search, local search, and linear programming.</p>
<p>An example of an algorithm design is the maximum problem: given an array of integers <span class="math inline">A</span>, find the maximum integer in <span class="math inline">A</span>. A simple solution to this is as follows:</p>
<pre><code>def find_max(array):
    current = array[0]
    for elem in array[1:]:
        if elem &gt; current: current = elem
    return elem</code></pre>
<p>This seems to be obviously correct, but we can also use program verification to prove it correct. In this case, we will use induction to prove the loop invariant - at the end of each iteration of the loop, <code>current</code> is equal to the largest element encountered so far in the array. We'd check the base case for arrays of length 1, then prove the inductive hypothesis to formally verify the program.</p>
<p>Clearly, this algorithm is <span class="math inline">\Theta(n)</span>, since it loops over the entire array once. Formal time complexity analysis can also be done by noting that the loop body takes <span class="math inline">\Theta(1)</span> time, and the loop runs <span class="math inline">\Theta(n)</span> times.</p>
<p>We also know that this algorithm is asymptotically optimal, at least in terms of the number of comparisons done. Since a correct solution to the Maximum problem must look at every element in the array, it must therefore do at least <span class="math inline">n - 1</span> comparisons. Since each comparison takes <span class="math inline">\Theta(1)</span> time, the best possible algorithm must take <span class="math inline">\Omega(n)</span> time. Let's formally prove this:</p>
<blockquote>
<p>Suppose there was an algorithm that could determine the maximum element of an array using fewer than <span class="math inline">n - 1</span> comparisons.<br />
Let <span class="math inline">G</span> be a graph such that each vertex in <span class="math inline">G</span> corresponds to an element in <span class="math inline">A</span>, and each comparison done by a run of the algorithm between any two elements results in an edge between those two elements.<br />
Clearly, there are <span class="math inline">n - 2</span> edges or less, and <span class="math inline">n</span> vertices.<br />
Therefore, there are at least 2 components in <span class="math inline">G</span>, since the graph cannot be connected.<br />
Clearly, the solution could be in any of the components, and can only be found by comparing them.<br />
Therefore, the algorithm cannot exist.</p>
</blockquote>
<p>An algorithm to find the minimum and the maximum element of an array can also trivially be designed, using <span class="math inline">2n - 2</span> comparisons and <span class="math inline">\Theta(n)</span> time. However, it is possible to design another algorithm that does fewer than <span class="math inline">2n - 2</span> comparisons (though the time complexity might be worse).</p>
<p>One way to do this is to consider elements two at a time - we compare elements one pair at a time, the larger of the pair with the maximum, and the smaller of which with the minimum. That means we need 3 comparisons per pair, or <span class="math inline">\ceil{\frac 3 2 n} - 2</span> in total, a ~25% improvement:</p>
<pre><code>def find_min_max(array):
    if array[0] &lt; array[1]: min, max = array[0], array[1]
    else: max, min = array[0], array[1]
    for i in range(2, len(array), 2):
        if array[i] &lt; array[i + 1]: small, big = array[i], array[i + 1]
        else: big, small = array[i], array[i + 1]
        if big &gt; max: max = big
        if small &lt; min: min = small
    if len(array) % 2:
        if array[-1] &gt; max: max = array[-1]
        if array[-1] &lt; min: min = array[-1]</code></pre>
<p>It can actually be proven, using graph theory, that the number of comparisons for this algorithm is optimal. However, it's too lengthy and complicated to cover here.</p>
<p>Suppose we want to determine whether and which three elements of an array of integers sum to 0. This is pretty easy to do in <span class="math inline">O(n^3)</span> (specifically, <span class="math inline">n \choose 3</span> runs of the inner loop) simply by checking all possible triples in the array. Basically, we pick two elements <span class="math inline">A[i]</span> and <span class="math inline">A[j]</span>, then try to search for a <span class="math inline">A[k] = -(A[i] + A[j])</span> using linear search. However, it is actually possible to do so in <span class="math inline">O(n^2 \log n)</span> by sorting the array first, and searching for <span class="math inline">k</span> using binary search instead.</p>
<p>In fact, it's possible to do even better by sorting the array, taking each <span class="math inline">A[i]</span>, and searching from both ends of the array inward for <span class="math inline">j</span> and <span class="math inline">k</span> such that <span class="math inline">A[j] + A[k] = -A[i]</span>. After each step in the search, we either move inward from the left if <span class="math inline">A[j] + A[k] &lt; -A[i]</span>, or inward from the right if <span class="math inline">A[j] + A[k] &gt; -A[i]</span></p>
<h1 id="section-1">16/9/15</h1>
<p>The pseudocode for the above algorithm looks like the following:</p>
<pre><code>def better_3sum(array):
    array = sorted(array)
    result = []
    for i in range(n - 2):
        j, k = i + 1, n
        while j &lt; k:
            triple_sum = array[i] + array[j] + array[k]
            if triple_sum &lt; 0: j += 1
            elif triple_sum &gt; 0: k -= 1
            else:
                result.append((i, j, k))
                j += 1
                k -= 1</code></pre>
<p>Basically, this goes through each value of <span class="math inline">i</span>, and does an <span class="math inline">O(n)</span> search for two values that would make it possible to have all three sum to 0. Proving this correct is left as an exercise to the reader - prove that <span class="math inline">k - j</span> is monotonically decreasing, and the pairs cover all possible pairs that can possibly sum up to <span class="math inline">A[i]</span>. It's somewhat reminiscent of the array merge in merge sort.</p>
<p>This algorithm is <span class="math inline">O(n^2)</span>, since the search is <span class="math inline">O(n^2)</span> and the sort is <span class="math inline">O(n \log n)</span>. There are actually even better algorithms, and the best currently known ones are <span class="math inline">O\left(n^2 \left(\frac{\log \log n}{\log n}\right)^2\right)</span>.</p>
<p>A <strong>problem</strong> is a computational task. A <strong>problem instance</strong> is the input for the computational task. The <strong>problem solution</strong> is the output. The <strong>size of a problem instance</strong> is a positive integer that is a measure of the size of the instance, and depends on the problem itself. The size is usually fairly intuitive, such as the size of an array for array sum, but sometimes it gets a bit more tricky.</p>
<p>An <strong>algorithm</strong> is a high level description of a computation. An algorithm <strong>solves</strong> a problem if it finds a valid solution for any instance in finite time. A <strong>program</strong> is an implementation of an algorithm using a computer language.</p>
<h2 id="time-complexity-analysis">Time Complexity Analysis</h2>
<p>The running time of a program <span class="math inline">M</span> for a problem instance <span class="math inline">I</span> is <span class="math inline">T_M(I)</span>. The worst case running time for problem instances of size <span class="math inline">n</span> for the program is <span class="math inline">T_M(n)</span>. The average case running time is <span class="math inline">T_M^{avg}(n)</span>.</p>
<p>The <strong>worst case time complexity</strong> of an algorithm <span class="math inline">A</span> is <span class="math inline">f(n)</span> such that a program <span class="math inline">M</span> exists implementing <span class="math inline">A</span> such that <span class="math inline">T_M(n) \in \Theta(f(n))</span>.</p>
<p>Running time can only be found by running the program on a computer. Complexity is independent, but we lose information such as the constant factors.</p>
<p>To get the time complexity of an algorithm, we can either use <span class="math inline">\Theta(f(n))</span> throughout our analysis, or prove <span class="math inline">O(f(n))</span> and then <span class="math inline">\Omega(f(n))</span>, which is often easier due to being able to make more assumptions for each separate proof.</p>
<p>The complexity of a loop in code is sum of the complexity of its body over each iteration of the loop. The complexity of consecutive operations is the sum of the complexities of the operations they are composed of.</p>
<p>Prove that <span class="math inline">(\ln n)^a \in o(n^b)</span> for any <span class="math inline">a</span> and <span class="math inline">b</span>:</p>
<blockquote>
<p>Let <span class="math inline">L = \lim_{n \to \infty} \frac{(\ln n)^a}{n^b} \lH \lim_{n \to \infty} \frac{a (\ln n)^{a - 1}}{b n^b} \lH \ldots \lH \lim_{n \to \infty} \frac{a!}{b^a n^b} = 0</span>.<br />
By the limit order rule, <span class="math inline">(\ln n)^a \in o(n^b)</span>.</p>
</blockquote>
<h1 id="section-2">21/9/15</h1>
<p>Useful to know:</p>
<ul>
<li><span class="math inline">\sum_{i = 0}^{n - 1} (a + di) = na + \frac{dn(n - 1)} 2</span> - arithmetic sequence</li>
<li><span class="math inline">\sum_{i = 0}^{n - 1} ar^i = \begin{cases} a\frac{r^n - 1}{r - 1} &amp;\text{if } r \ne 1 \\ na &amp;\text{if } r = 1 \end{cases}</span> - geometric sequence</li>
<li><span class="math inline">\sum_{i = 0}^{n - 1} (a + di)r^i = \frac a {1 - r} - \frac{(a + (n - 1)d)r^n}{1 - r} + \frac{dr(1 - r^{n - 1})}{(1 - r)^2}</span> -arithmetic-geometric sequence</li>
<li><span class="math inline">\sum_{i = 1}^n \frac 1 {i^2} = \frac{\pi^2} 6 = \Theta(1)</span></li>
<li><span class="math inline">_n = \sum_{i = 1}^n \frac 1 i \in \Theta(\log n)</span> - harmonic sequence</li>
</ul>
<p>The order of <span class="math inline">n!</span> is <span class="math inline">\Theta(n^2 \sqrt n e^{-n})</span>. This is similar to Stirling's approximation, <span class="math inline">\sqrt{2 \pi n} n^n e^{-n}</span> (this gets more and more accurate for larger values).</p>
<p>For order notation, the base of logarithmic terms don't matter because <span class="math inline">\log_{10} n = \frac 1 {\log_b 10} \log_b n</span>, and <span class="math inline">\frac 1 {\log_b 10}</span> is a constant factor.</p>
<p><strong>Polynomial growth rates</strong> are those in <span class="math inline">O(n^k), k \in \mb{R}</span>. For example, the best known algorithm for graph isomorphism is <span class="math inline">n^{\sqrt n \log_2 n}</span>, so it is not polynomial.</p>
<h1 id="section-3">23/9/15</h1>
<p>Consider the following program:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> useless(n):
    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n):
        j <span class="op">=</span> i
        <span class="cf">while</span> j <span class="op">&gt;=</span> <span class="dv">1</span>:
            j <span class="op">/=</span> <span class="dv">2</span></code></pre></div>
<p>Clearly, each iteration of the inner loop will run <span class="math inline">\Theta(\log n)</span> times, so the overall time complexity is <span class="math inline">\sum_{i = 1}^n \Theta(\log i) = \Theta(\sum_{i = 1}^n \log i) = \Theta(\log(\prod_{i = 1}^n i)) = \Theta(\log(n!)) = \Theta(n \log n)</span>.</p>
<p>A <strong>recurrence relation</strong> specifies <span class="math inline">a_n</span> in the infinite sequence of real numbers <span class="math inline">a_1, a_2, a_3, \ldots</span> in terms of the previous terms <span class="math inline">a_1, \ldots, a_{n - 1}</span> (<span class="math inline">a_1</span> must therefore be a constant - the <strong>initial value</strong>). A <strong>solution</strong> to a recurrence relation is a closed form formula for <span class="math inline">a_n</span> - one that does not depend on the previous values of <span class="math inline">a_1, \ldots, a_{n - 1}</span>. These are generally solved using techniques like guess and check or recursion trees, but there aren't any methods that are guaranteed to solve all recurrences, and in fact, reurrences may not necessarily even have solutions.</p>
<p>Recurrence relations can also be written using function notation, like <span class="math inline">T(1) = 2, T(n) = T(n - 1) + 1</span>.</p>
<p>The <strong>guess and check</strong> method involves computing enough <span class="math inline">a_n</span> instances to guess the form of the solution (without any constants). Then, we solve for the constants using the computed values, and check if our solution is correct - if it is, we must prove it is correct using induction, and if not, we start over with a different guess.</p>
<p>Solve <span class="math inline">a_0 = 4, a_n = a_{n - 1} + 6n - 5</span>:</p>
<blockquote>
<p>Guess and check: the first few elements of the sequence are <span class="math inline">a_0 = 4, a_1 = 5, a_2 = 12, a_3 = 25, a_4 = 44</span>.<br />
This seems to be quadratic growth, so we guess <span class="math inline">an^2 + bn + c, a, b, c \in \mb{R}</span> as the solution.<br />
Solving for <span class="math inline">a, b, c</span>, <span class="math inline">c = 4, a + b + c = 5, 4a + 2b + c = 12</span>, we get <span class="math inline">a = 3, b = -2, c = 4</span>.<br />
So we guess that <span class="math inline">a_n = 3n^2 - 2n + 4</span>. We can verify this for <span class="math inline">0 \le n \le 4</span> as our inductive base case.<br />
Assume for some <span class="math inline">k \in \mb{N}</span> that <span class="math inline">a_k = 3k^2 - 2k + 4</span>.<br />
Clearly, <span class="math inline">a_{k + 1} = 3k^2 - 2k + 4 + 6(k + 1) - 5 = 3(k + 1)^2 - 2(k + 1) + 4</span>.<br />
So by induction, <span class="math inline">a_n = 3n^2 - 2n + 4</span>.</p>
</blockquote>
<p>The <strong>recurrence tree</strong> method is often used for divide-and-conquer algorithms. This technique involves building a call tree for the recurrence. then summing up the results of the function on each level of the tree.</p>
<p>To construct a recurrence tree, we start with the timing function <span class="math inline">T(n)</span>, then add children for each recursive call. The recursive calls are then removed from the parent. This is repeated for the children, all the way until we reach the base case. Note that after each step of growing the tree, the sum of all the nodes are always equivalent to <span class="math inline">T(n)</span>.</p>
<p>Then, we sum the nodes at each level in the tree. The sum of all these sums is equal to <span class="math inline">T(n)</span>.</p>
<p>Use the recurrence tree method to find the running time of merge sort:</p>
<blockquote>
<p>Clearly, <span class="math inline">T(n) = \begin{cases} 2T\left(\frac n 2\right) + cn &amp;\text{if } n &gt; 1 \wedge n \text{ is a power of 2} \\ d &amp;\text{if } n = 1 \end{cases}</span> where <span class="math inline">c, d</span> are constants.<br />
Construct a recursion tree: start with the root node <span class="math inline">N</span> with value <span class="math inline">T(n)</span>, then add 2 children, <span class="math inline">N_1, N_2</span>, both <span class="math inline">T\left(\frac n 2\right)</span>, and replace <span class="math inline">N</span>'s value with <span class="math inline">cn</span>.<br />
What we get is a binary tree with every internal node having 2 children, and value <span class="math inline">c\frac n {2^i}</span> (where <span class="math inline">i</span> is the level in the tree, where 0 is the root level), and each leaf node has value <span class="math inline">d</span>.<br />
Let <span class="math inline">n = 2^j</span>. Clearly, the height of the tree must then be <span class="math inline">j + 1</span>, with <span class="math inline">j</span> interior levels. Clearly, at the bottom level there are <span class="math inline">2^j</span> leaf nodes, since each level <span class="math inline">i</span> has <span class="math inline">2^{j - i}</span> nodes. The sum of the bottom level is therefore <span class="math inline">2^j d = dn</span>.<br />
Clearly, at each internal level <span class="math inline">i</span> each node has value <span class="math inline">c\frac n {2^i}</span>. Since there are <span class="math inline">2^i</span> nodes, the sum of each interior level is <span class="math inline">c2^i\frac n {2^i} = cn</span>.<br />
So <span class="math inline">T(n) = dn + jcn = dn + cn \log_2 n = \Theta(n \log n)</span>.</p>
</blockquote>
<p>The <strong>master theorem</strong> is a generalized formula for solving certain forms of recurrences. One thing it says is that given <span class="math inline">T(n) = aT(\frac n b) + \Theta(n^y)</span> where <span class="math inline">a \ge 1, b &gt; 1</span>, and <span class="math inline">n</span> is a power of <span class="math inline">b</span>, then <span class="math inline">T(n) \in \begin{cases} \Theta(n^x) &amp;\text{if } y &lt; x \\ \Theta(n^x \log n) &amp;\text{if } y = x \\ \Theta(n^y) &amp;\text{if } y &gt; x \end{cases}</span> where <span class="math inline">x = \log_b a</span>.</p>
<p>Also, the exact value is <span class="math inline">T(n) = da^j + cn^y \sum_{i = 0}^{j - 1} \left(\frac a {b^y}\right)^i</span>, or more simply <span class="math inline">T(n) = dn^x + cn^y \sum_{i = 0}^{j - 1} r^i</span> where <span class="math inline">x = \log_b a</span> and <span class="math inline">r = \frac a {b^y} = b^{x - y}</span>.</p>
<p>This can be proved by constructing the recursion tree, figuring out the geometric sequence for the tree sums, and then solving for its value for each of the three cases.</p>
<p>A more general version, which will not be proved here, is that given <span class="math inline">T(n) = aT(\frac n b) + f(n)</span> where <span class="math inline">a \ge 1, b &gt; 1</span>, and <span class="math inline">n</span> is a power of <span class="math inline">b</span>, then <span class="math inline">T(n) \in \begin{cases} \Theta(n^x) &amp;\text{if } f(n) \in O(n^{x - \epsilon}) \text{ for some } \epsilon &gt; 0 \\ \Theta(n^x \log n) &amp;\text{if } f(n) \in \Theta(n^x) \\ \Theta(f(n)) &amp;\text{if } \frac{f(n)}{n^{x + \epsilon}} \text{ is an increasing function of } n \text{ for some } \epsilon &gt; 0 \text{ after } n \text{ is greater than some } n_0 \end{cases}</span> where <span class="math inline">x = \log_b a</span>.</p>
<h1 id="section-4">28/9/15</h1>
<p>Solve <span class="math inline">T(1) = 1, T(n) = 3T(\frac n 4) + n \log n</span>:</p>
<blockquote>
<p>Let <span class="math inline">a = 3, b = 4, f(n) = n \log n</span>, so <span class="math inline">x = \log_4 3</span> and <span class="math inline">f(n) / n^{x + \epsilon}</span> is an increasing function of <span class="math inline">n</span> for <span class="math inline">\epsilon = 1 - \log_4 3</span> since <span class="math inline">f(n) / n^{x + \epsilon} = \log n</span>.<br />
So <span class="math inline">T(n) \in \Theta(n \log n)</span>, by the master theorem.</p>
</blockquote>
<p>The main task when using the master theorem to prove time complexities is choosing the right <span class="math inline">\epsilon</span> to make one of the cases valid. For example, the master theorem cannot be used for something like <span class="math inline">T(1) = 1, T(n) = 2T(\frac n 2) + n \log n</span>, since none of the cases apply. Instead, we can use the recurrence tree method to solve the recurrence directly:</p>
<blockquote>
<p>Each level <span class="math inline">i</span> (from the bottom) of the tree has <span class="math inline">2^i</span> nodes, each with value <span class="math inline">2^{j - i}(j - i)</span> where <span class="math inline">n = 2^j</span>.<br />
So at each level, the sum is <span class="math inline">2^i 2^{j - i} (j - i) = (j - i) 2^j</span>.<br />
So the sum of the tree is <span class="math inline">\sum_{i = 0}^j (j - i) 2^j = j^2 2^j - 2^j \sum_{i = 0}^j i = 2^j\left(j^2 - \frac{j(j + 1)}{2}\right) = \Theta(n \log^2 n)</span>.</p>
</blockquote>
<h2 id="divide-and-conquer">Divide and Conquer</h2>
<p>Divide and conquer is a design strategy for algorithms where we divide a problem into smaller subproblems, solve those individually, then combine them back together to get a good result. Formally:</p>
<ul>
<li>Divide the problem instance <span class="math inline">I</span> into smaller subproblems <span class="math inline">I_1, \ldots, I_n</span>.</li>
<li>Solve <span class="math inline">I_1, \ldots, I_n</span> recursively to get sollutions <span class="math inline">S_1, \ldots, S_n</span>.</li>
<li>Use <span class="math inline">S_1, \ldots, S_n</span> to compute <span class="math inline">S</span> - combine the results. This is usually the hardest step.</li>
</ul>
<p>For example, mergesort splits an array into two subarrays, and then recursively sorts those. Then, the merge is performed to combine the two sorted subarrays.</p>
<p>The time complexity of this is <span class="math inline">T(n) = \begin{cases} T(\ceil{\frac n 2}) + T(\floor{\frac n 2}) + cn &amp;\text{if } n &gt; 1 \\ d &amp;\text{if } n = 1 \end{cases}</span> for some constants <span class="math inline">c, d</span>. This is called the <strong>exact recurrence</strong>. If we remove the ceilings and floors, we get a <strong>sloppy recurrence</strong> <span class="math inline">T(n) = \begin{cases} 2T(\frac n 2) + cn &amp;\text{if } n &gt; 1 \\ d &amp;\text{if } n = 1 \end{cases}</span>, which only works the same when <span class="math inline">n</span> is a power of 2, but is a lot simpler to work with.</p>
<p>The master theorem can prove that mergesort is <span class="math inline">\Theta(n \log n)</span> for the sloppy recurrence, and therefore only for when <span class="math inline">n</span> is a pwoer of 2. However, if we want to get the time complexity for all <span class="math inline">n</span>, we need to use induction on the exact recurrence.</p>
<h1 id="section-5">30/9/15</h1>
<h3 id="non-dominated-points">Non-dominated points</h3>
<p>Suppose we have a set of 2D coordinates/points. One point <strong>dominates</strong> another if it has a greater or equal X and Y coordinate than the other. We want to find those points that are non-dominated - the points that are not dominated by any other point. The non-dominated points form a sort of boundary below which all the dominated points lie.</p>
<p>Clearly, this can easily be solved simply by checking every pair of points to see which dominate which, but this would take <span class="math inline">\Theta(n^2)</span> time.</p>
<p>A better solution is to sort the points by their X coordinate, and then apply divide and conquer.</p>
<p>Dividing larger sets of points <span class="math inline">S</span> is also easy - we can just split the sorted points into two sets <span class="math inline">S_1, S_2</span>, and then recursively solve the problem. It is the combining step that is the difficult step.</p>
<p>Given a point and the highest point on its right, it is easy to determine whether it is dominated. This is our base case</p>
<p>Clearly, no point in <span class="math inline">S_1</span> can dominate a point in <span class="math inline">S_2</span>, since the X coordinates of points in <span class="math inline">S_1</span> is strictly less than points in <span class="math inline">S_2</span>. So we find the point in <span class="math inline">S_2</span> with the highest Y coordinate, and all points in <span class="math inline">S_1</span> with Y coordinate at or below this value are guaranteed to be dominated by at least one point in <span class="math inline">S_2</span>. In this way, we can eliminate all the points in <span class="math inline">S_1</span> that are dominated by points in <span class="math inline">S_2</span> in <span class="math inline">O(n)</span> time - this is the combining step.</p>
<p>In code, this looks like:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> non_dominated(points):
    points <span class="op">=</span> <span class="bu">sorted</span>(points, key<span class="op">=</span><span class="kw">lambda</span> p: p.x)
    <span class="cf">if</span> <span class="bu">len</span>(points) <span class="op">==</span> <span class="dv">1</span>: <span class="cf">return</span> {points[<span class="dv">0</span>]}
    pivot <span class="op">=</span> floor(<span class="bu">len</span>(points) <span class="op">/</span> <span class="dv">2</span>)
    left <span class="op">=</span> non_dominated(points[:pivot])
    right <span class="op">=</span> non_dominated(points[pivot <span class="op">+</span> <span class="dv">1</span>:])
    i <span class="op">=</span> <span class="dv">0</span>
    <span class="cf">while</span> i <span class="op">&lt;</span> <span class="bu">len</span>(left) <span class="kw">and</span> left[i].y <span class="op">&gt;</span> right[<span class="dv">1</span>].y:
        i <span class="op">+=</span> <span class="dv">1</span>
    <span class="cf">return</span> left[:i] <span class="op">+</span> right</code></pre></div>
<p>Clearly, the time complexity of this algorithm is <span class="math inline">T(n) = T\left(\floor{\frac n 2}\right) + T\left(\ceil{\frac n 2}\right) = O(n \log n)</span> (the sloppy version of this can be proven using the master theorem).</p>
<p>Without divide and conquer, we could also just sort the list, and then iterate though it from right to left, keeping track of the largest Y axis coordinate seen so far and eliminating points at or below this Y coordinate:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> non_dominated(points):
    largest <span class="op">=</span> <span class="bu">float</span>(<span class="st">&quot;-inf&quot;</span>)
    <span class="cf">for</span> point <span class="kw">in</span> <span class="bu">reversed</span>(<span class="bu">sorted</span>(points, key<span class="op">=</span><span class="kw">lambda</span> p: p.x)):
        <span class="cf">if</span> point.y <span class="op">&lt;=</span> largest:
            <span class="cf">yield</span> point
        <span class="cf">else</span>:
            largest <span class="op">=</span> point.y</code></pre></div>
<h3 id="closest-pair">Closest pair</h3>
<p>Suppose we have a set <span class="math inline">Q</span> of <span class="math inline">n</span> distinct 2D points in Euclidean space. We want to find the two points that are closest together - the points <span class="math inline">u, v</span> for which <span class="math inline">\sqrt{(u_x - v_x)^2 + (u_y - v_y)^2}</span> is minimised.</p>
<p>Clearly, minimising <span class="math inline">\sqrt{(u_x - v_x)^2 + (u_y - v_y)^2}</span> is the same as minimising <span class="math inline">(u_x - v_x)^2 + (u_y - v_y)^2</span>. We will again sort the points in <span class="math inline">Q</span> by X coordinate, taking <span class="math inline">\Theta(n \log n)</span> time.</p>
<p>The dividing step is again trivial to get two partitions <span class="math inline">Q_1, Q_2</span>, but the combining step takes some work.</p>
<p>If we are given the distance <span class="math inline">\delta</span> between the closest pair of points seen so far in the left and right partitions, and a point <span class="math inline">p</span>, we know that if this point is in the closest pair of points, the other point must have X coordinate within <span class="math inline">\delta</span> of this point's X coordinate. Therefore, the other point must be in the <strong>critical strip</strong> - a section of the plane from <span class="math inline">p_x - \delta</span> to <span class="math inline">p_x + \delta</span> and extending infinitely both up and down.</p>
<p>We now want to find the closest pair between the left and right partitions, if it is less than the .</p>
<p>There is also a lemma that says that if a critical strip <span class="math inline">R</span> of <span class="math inline">Q</span> is sorted by Y coordinate, and <span class="math inline">R[j]</span> and <span class="math inline">R[k]</span> have distance less than <span class="math inline">\delta</span> where <span class="math inline">j &lt; k</span>, then <span class="math inline">k \le j + 7</span>. Here, <span class="math inline">\delta</span> is the smaller of the minimum distance between points in <span class="math inline">Q_1</span>, and the minimum distance between points in <span class="math inline">Q_2</span>. This can be proven by dividing the strip around the point as squares of side length <span class="math inline">\frac \delta 2</span>, and then showing that no two points can be in the same square. In other words, within a critical strip and given <span class="math inline">\delta</span>, the point closest to a given point can be determined in 7 comparisons or less.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> closest_pair(points, start, end):
    <span class="cf">if</span> start <span class="op">==</span> end: <span class="cf">return</span> <span class="bu">float</span>(<span class="st">&quot;inf&quot;</span>)
    pivot <span class="op">=</span> floor((left <span class="op">+</span> right) <span class="op">/</span> <span class="dv">2</span>)
    closest_left <span class="op">=</span> closest_pair(points, left, pivot)
    closest_right <span class="op">=</span> closest_pair(points, pivot <span class="op">+</span> <span class="dv">1</span>, right)
    closest <span class="op">=</span> <span class="bu">min</span>(closest_left, closest_right)
    candidates <span class="op">=</span> select_candidates(left, right, closest, points[pivot].x) <span class="co"># find points in the critical strip</span>
    candidates <span class="op">=</span> <span class="bu">sorted</span>(candidates, key<span class="op">=</span><span class="kw">lambda</span> p: p.y)
    closest <span class="op">=</span> check_candidates(candidates, closest)
    <span class="cf">return</span> closest</code></pre></div>
<p>The time complexity of this is <span class="math inline">O(n \log n \log n)</span>. We can improve this by eliminating the Y coordinate sort of the candidates. One of them is to sort all the points <span class="math inline">Q</span> by Y coordinate to begin with, and then use that list when checking candidates, and then adding the necessary checks that make it work. Alternatively, we could replace the Y coordinate sort with a merge, and maintaining the candidates in sorted order.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> closest_pair(points, start, end):
    <span class="cf">if</span> start <span class="op">==</span> end: <span class="cf">return</span> <span class="bu">float</span>(<span class="st">&quot;inf&quot;</span>)
    pivot <span class="op">=</span> floor((left <span class="op">+</span> right) <span class="op">/</span> <span class="dv">2</span>)
    closest_left <span class="op">=</span> closest_pair(points, left, pivot)
    closest_right <span class="op">=</span> closest_pair(points, pivot <span class="op">+</span> <span class="dv">1</span>, right)
    closest <span class="op">=</span> <span class="bu">min</span>(closest_left, closest_right)
    merge_in_place(points, start, pivot, end)
    candidates <span class="op">=</span> select_candidates(left, right, closest, points[pivot].x) <span class="co"># find points in the critical strip</span>
    closest <span class="op">=</span> check_candidates(candidates, closest)
    <span class="cf">return</span> closest</code></pre></div>
<h1 id="section-6">5/10/15</h1>
<p>The <strong>size</strong> of an integer is the number of bits that we use to represent it, rather than the value of the integer itself - this is basically just <span class="math inline">\ceil{\log_2 n}</span>. The <strong>bit complexity</strong> of algorithms operating on integers is therefore the complexity as a function of the sizes of those integers.</p>
<h2 id="multiplication">Multiplication</h2>
<h3 id="integer-multiplication">Integer multiplication</h3>
<p>Suppose we have 2 positive binary integers, <span class="math inline">X = [X_{k - 1}, \ldots, X_0]</span> and <span class="math inline">Y = [Y_{k - 1}, \ldots, Y_0]</span>. We want to compute the <span class="math inline">2k</span>-bit product <span class="math inline">Z = [Z_{2k - 1}, \ldots, Z_0]</span>. Naively, we can use the simple binary multiplication algorithm, to shift <span class="math inline">Y</span> by each position <span class="math inline">0 \le i &lt; k</span> and add those shifted values for which <span class="math inline">X_i = 1</span>, in <span class="math inline">O(k^2)</span>. Note that the fastest way to add two integers is <span class="math inline">\Theta(k)</span>.</p>
<p>We can do better than this by using divide and conquer, by using <span class="math inline">XY = (2^{\frac k 2} X_L + X_R)(2^{\frac k 2} Y_L + Y_R) = 2^k X_L Y_L + 2^{\frac k 2}(X_L Y_R + X_R Y_L) + X_R Y_R</span>, then solving for those four multiplications between <span class="math inline">X_L, X_R, Y_L, Y_R</span> (multiplying the powers of 2 are simply shifts, so they're basically free):</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> multiply(x, y):
    <span class="cf">if</span> <span class="bu">len</span>(x) <span class="op">==</span> <span class="dv">1</span>: <span class="cf">return</span> x[<span class="dv">0</span>] <span class="op">*</span> y[<span class="dv">0</span>]
    x_left, x_right <span class="op">=</span> x[:<span class="bu">len</span>(x) <span class="op">//</span> <span class="dv">2</span>], x[<span class="bu">len</span>(x) <span class="op">//</span> <span class="dv">2</span>:]
    y_left, y_right <span class="op">=</span> y[:<span class="bu">len</span>(y) <span class="op">//</span> <span class="dv">2</span>], y[<span class="bu">len</span>(y) <span class="op">//</span> <span class="dv">2</span>:]
    z_top <span class="op">=</span> multiply(x_left, y_left)
    z_middle <span class="op">=</span> add(multiply(x_left, x_right), multiply(x_right, y_left))
    z_bottom <span class="op">=</span> multiply(x_right, y_right)
    z <span class="op">=</span> add(
        shift_left(z_top, <span class="bu">len</span>(x)),
        shift_left(z_middle, <span class="bu">len</span>(x) <span class="op">//</span> <span class="dv">2</span>),
        z_bottom
    )
    <span class="cf">return</span> z</code></pre></div>
<p>Using the master theorem, we can show that this is takes <span class="math inline">\Theta(n^2)</span> time - no improvement over the naive algorithm. However, this is a good base for an improved version.</p>
<p>Consider <span class="math inline">(X_L + X_R)(Y_L + Y_R) = X_L Y_L + X_L Y_R + X_R Y_L + X_R Y_R</span>. So <span class="math inline">X_L Y_R + X_R Y_L = (X_L + X_R)(Y_L + Y_R) - X_L Y_L - X_R Y_R</span>. Since we need to calculate <span class="math inline">X_L Y_L</span> and <span class="math inline">X_R Y_R</span> anyways, we can compute <span class="math inline">X_L Y_R + X_R Y_L</span> with just one multiplication by computing <span class="math inline">(X_L + X_R)(Y_L + Y_R)</span>.</p>
<p>So using this:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> multiply(x, y):
    <span class="cf">if</span> <span class="bu">len</span>(x) <span class="op">==</span> <span class="dv">1</span>: <span class="cf">return</span> x[<span class="dv">0</span>] <span class="op">*</span> y[<span class="dv">0</span>]
    x_left, x_right <span class="op">=</span> x[:<span class="bu">len</span>(x) <span class="op">//</span> <span class="dv">2</span>], x[<span class="bu">len</span>(x) <span class="op">//</span> <span class="dv">2</span>:]
    y_left, y_right <span class="op">=</span> y[:<span class="bu">len</span>(y) <span class="op">//</span> <span class="dv">2</span>], y[<span class="bu">len</span>(y) <span class="op">//</span> <span class="dv">2</span>:]
    z_top <span class="op">=</span> multiply(x_left, y_left)
    z_middle <span class="op">=</span> multiply(add(x_left, x_right), add(y_left, y_right))
    z_bottom <span class="op">=</span> multiply(x_right, y_right)
    z <span class="op">=</span> add(
        shift_left(z_top, <span class="bu">len</span>(x)),
        shift_left(z_middle, <span class="bu">len</span>(x) <span class="op">//</span> <span class="dv">2</span>) <span class="op">-</span> z_top <span class="op">-</span> z_bottom,
        z_bottom
    )
    <span class="cf">return</span> z</code></pre></div>
<p>Now, we only solve 3 subproblems instead of 4. This gives us a time complexity of around <span class="math inline">\Theta(k^{\log_2 3})</span>, or about <span class="math inline">\Theta(k^{1.59})</span>.</p>
<p>Even better algorithms exist. One is Toom-Cook, which splits <span class="math inline">X</span> and <span class="math inline">Y</span> into 3 pieces each, and solves 5 subproblems, all in <span class="math inline">O(k^{\log_3 5})</span>. The Stronhage-Strassen method takes <span class="math inline">O(k \log \log \log k)</span> time, using Fourier transforms. The Furer method is the most advanced as of the present, using <span class="math inline">O(k \log k 2^{3 \log^* k})</span> time where <span class="math inline">\log^* k</span> is the inverse Ackermann function.</p>
<h3 id="matrix-multiplication">Matrix multiplication</h3>
<p>We will now assume that integer multiplication can be done in constant time, to avoid having to consider bit complexity.</p>
<p>Suppose we have two <span class="math inline">n</span> by <span class="math inline">n</span> matrices <span class="math inline">A, B</span>. Clearly, the naive algorithm for multiplying them is <span class="math inline">\Theta(n^3)</span>, since we just take the dot product of the corresponding row of <span class="math inline">A</span> and the corresponding column of <span class="math inline">B</span> - the dot product takes <span class="math inline">\Theta(n)</span>, and there are <span class="math inline">\Theta(n^2)</span> cells in the result.</p>
<p>We can use the same technique as for integer multiplcation - we break each <span class="math inline">n</span> by <span class="math inline">n</span> matrix into 4 <span class="math inline">\frac n 2</span> by <span class="math inline">\frac n 2</span> matrices, multiply those, then combine those together, then again try to remove subproblem calculations where possible.</p>
<p>If we split <span class="math inline">A, B</span> into 4 blocks <span class="math inline">A = \begin{bmatrix} a &amp; b \\ c &amp; d \end{bmatrix}, B = \begin{bmatrix} e &amp; f \\ g &amp; h \end{bmatrix}</span>, then <span class="math inline">AB = \begin{bmatrix} ae + bg &amp; af + bh \\ ce + dg &amp; cf + dh \end{bmatrix}</span>. If we use divide and conquer to solve these 8 multiplications in the block version, it still takes <span class="math inline">\Theta(n^3)</span>. However, the problem is now in a form that is useful for optimizing, by reducing the number of sub-problems.</p>
<p>Strassen's method notes that if <span class="math inline">P_1 = a(f - h), P_2 = (a + b)h, P_3 = (c + d)e, P_4 = d(g - e), P_5 = (a + d)(e + h), P_6 = (b - d)(g + h), P_7 = (a - c)(e - f)</span>, then <span class="math inline">ae + bg = P_5 + P_4 - P_2 + P_6, af + bh = P_1 + P_2, ce + dg = P_3 + P_4, cf + dh = P_5 + P_1 - P_3 - P_7</span>.</p>
<p>So <span class="math inline">AB = \begin{bmatrix} P_5 + P_4 - P_2 + P_6 &amp; P_1 + P_2 \\ P_3 + P_4 &amp; P_5 + P_1 - P_3 - P_7 \end{bmatrix}</span>, and since there are only 7 multiplications needed to calculate <span class="math inline">P_1, \ldots, P_7</span>, we only have 7 sub-problems to solve. As a result, the algorithm is <span class="math inline">\Theta(n^{\log_2 7})</span> or around <span class="math inline">\Theta(n^{2.81})</span>, since the recurrence is <span class="math inline">T(n) = 7T\left(\frac n 2\right) + \Theta(n^2)</span> (solve this using the master theorem).</p>
<p>Research into this problem has resulted into better algorithms. The current state of the art is the Coppersmith-Winograd algorithm, taking around <span class="math inline">O(n^{2.373})</span> time.</p>
<h3 id="selection">Selection</h3>
<p>Suppose we have an array <span class="math inline">A</span> of size <span class="math inline">n</span>, and we want to find the <span class="math inline">k</span>th smallest integer, where the smallest element is found when <span class="math inline">k = 1</span>. The median algorithm is a special case of this where <span class="math inline">k = \ceil{\frac n 2}</span>.</p>
<p>Naively, we could just sort the array and pick the <span class="math inline">k</span>th element, all in <span class="math inline">\Theta(n \log n)</span> time. Another naive approach is to use a modified selection sort (repeatedly finding the smallest element in the remaining part of the array, and putting it at the end of the sorted array), where we stop after <span class="math inline">k</span> moves, all in <span class="math inline">\Theta(kn)</span> time - this is linear if <span class="math inline">k</span> is constant, but quadratic if it is not. Combining the two, we can use a modified heapsort, which can solve the problem in <span class="math inline">\Theta(n + k \log n)</span> time.</p>
<h1 id="section-7">7/10/15</h1>
<p>There's a make-up lecture on November 21st or so.</p>
<p>Using divide and conquer, we might do something similar to quicksort, called quickselect - we pick a pivot, partition the elements, and then recursively quickselect within the partition that contains the desired element:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> quickselect(array, k):
    pivot <span class="op">=</span> array[<span class="dv">0</span>]
    partition1, partition2 <span class="op">=</span> [x <span class="cf">for</span> x <span class="kw">in</span> array <span class="cf">if</span> x <span class="op">&lt;</span> pivot], [x <span class="cf">for</span> x <span class="kw">in</span> array <span class="cf">if</span> x <span class="op">&gt;</span> pivot]
    <span class="cf">if</span> k <span class="op">-</span> <span class="dv">1</span> <span class="op">==</span> <span class="bu">len</span>(partition1):
        <span class="cf">return</span> pivot
    <span class="cf">elif</span> k <span class="op">-</span> <span class="dv">1</span> <span class="op">&lt;</span> <span class="bu">len</span>(partition1):
        <span class="cf">return</span> quickselect(partition1, k)
    <span class="cf">else</span>:
        <span class="cf">return</span> quickselect(partition2, k <span class="op">-</span> <span class="bu">len</span>(partition1) <span class="op">-</span> <span class="dv">1</span>)</code></pre></div>
<p>Note that unlike quicksort, there's only one recursive call in each invocation. On average, the pivot will be good about 50% of the time, where a good pivot is one that is within the 25th and 75th percentile inclusive. So on average, we would expect that every two calls, we would get a good pivot. So on average, every other call would have partitions that are of size at most 75% of the array's size. Therefore, on average the size of the subproblem would be reduced by 25% on every other call, which makes this function <span class="math inline">\Theta(n)</span> average case.</p>
<p>Note that in the worst case, the pivot would be at one of the extremes of the percentiles, and so the subproblem would only be reduced by a constant amount, and there are <span class="math inline">n</span> calls. Therefore, the function is <span class="math inline">\Theta(n^2)</span> worst case.</p>
<p>However, if we could always choose a good pivot, then we can ensure that the function is <span class="math inline">\Theta(n)</span> worst case as well. Doing so is a bit more clever, but it is possible, using the <strong>median of medians</strong> algorithm. First, we assume that there are at least 15 elements, so <span class="math inline">n = 10r + 5 + \theta</span>, where <span class="math inline">r \ge 1, 0 \le \theta \le 9</span>.</p>
<p>We now split <span class="math inline">A</span> into subarrays of exactly 5 elements: <span class="math inline">B_1, \ldots, B_{2r + 1}</span> (we ignore the last <span class="math inline">\theta</span> elements entirely here), and then find the median of each <span class="math inline">B_i</span>, as <span class="math inline">m_i</span>. Then, we use a selection algorithm to find the median of this array of medians.</p>
<p>We claim that <span class="math inline">y</span> is always a good pivot - that is is guaranteed to be between the 25th and 75th percentile inclusive. We will prove this later, but first, we will integrate it into quickselect:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> median_of_medians_quickselect(array, k):
    <span class="co"># generate a good pivot</span>
    <span class="cf">if</span> <span class="bu">len</span>(array) <span class="op">&lt;</span> <span class="dv">15</span>: <span class="cf">return</span> <span class="bu">sorted</span>(array)[k]
    write $n <span class="op">=</span> 10r <span class="op">+</span> <span class="dv">5</span> <span class="op">+</span> <span class="op">\</span>theta$
    construct $B_1, <span class="op">\</span>ldots, B_{2r <span class="op">+</span> <span class="dv">1</span>}$
    find medians $M <span class="op">=</span> [m_1, <span class="op">\</span>ldots, m_{2r <span class="op">+</span> <span class="dv">1</span>}]$
    pivot <span class="op">=</span> median_of_medians_quickselect(M, r <span class="op">+</span> <span class="dv">1</span>)
    
    <span class="co"># do the rest of the steps from quickselect</span>
    partition1, partition2 <span class="op">=</span> [x <span class="cf">for</span> x <span class="kw">in</span> array <span class="cf">if</span> x <span class="op">&lt;</span> pivot], [x <span class="cf">for</span> x <span class="kw">in</span> array <span class="cf">if</span> x <span class="op">&gt;</span> pivot]
    <span class="cf">if</span> k <span class="op">-</span> <span class="dv">1</span> <span class="op">==</span> <span class="bu">len</span>(partition1):
        <span class="cf">return</span> pivot
    <span class="cf">elif</span> k <span class="op">-</span> <span class="dv">1</span> <span class="op">&lt;</span> <span class="bu">len</span>(partition1):
        <span class="cf">return</span> quickselect(partition1, k)
    <span class="cf">else</span>:
        <span class="cf">return</span> quickselect(partition2, k <span class="op">-</span> <span class="bu">len</span>(partition1) <span class="op">-</span> <span class="dv">1</span>)</code></pre></div>
<p>We have two recursive calls now, since the median of medians algorithm also required us to do a selection to find <code>pivot</code> (<span class="math inline">y</span>).</p>
<p>There are <span class="math inline">r + 1</span> medians at or below <span class="math inline">y</span> in <span class="math inline">m_1, \ldots, m_{2r + 1}</span> - let these be represented <span class="math inline">m_{i_1}, \ldots, m_{i_{r + 1}}</span>. Clearly, in each <span class="math inline">B_{i_1}, \ldots, B_{i_{r + 1}}</span>, there are at least 3 out of the 5 elements that are less than or equal to <span class="math inline">y</span> (since they are less than or equal to the median). Therefore, there must be at least <span class="math inline">3(r + 1)</span> elements in total that are less than or equal to <span class="math inline">y</span>. Since there are <span class="math inline">10r + 5</span> elements (since we ignored <span class="math inline">\theta</span>), the ratio of elements below <span class="math inline">y</span> to the total number of elements is <span class="math inline">\frac{3(r + 1)}{10r + 5}</span>, or basically <span class="math inline">\frac 3 {10}</span>. Therefore, 30% or more of the elements must be less than or equal to <span class="math inline">y</span>.</p>
<p>Using a similar argument, but with the elements at or above <span class="math inline">y</span> rather than at or below, we can prove that 30% or more of the elements must be greater or equal to <span class="math inline">y</span>. Therefore, <span class="math inline">y</span> is within the 30th to 70th percentile inclusive.</p>
<p>Therefore, the sloppy recurrence is <span class="math inline">T(n) \le T(\frac n 5) + T(\frac{7n}{10}) + \Theta(n)</span>, which we can solve using the recurrence tree method - each node has two children, which are the node's value times <span class="math inline">\frac 1 5</span> and <span class="math inline">\frac 7 {10}</span>, respectively, and the root node's value is <span class="math inline">n</span>. When we draw the tree, we see that each level <span class="math inline">i</span> from the root (level 0) has sum <span class="math inline">\left(\frac 9 {10}\right)^in</span>, and the sum of all levels to infinity converges to <span class="math inline">10n</span>, so <span class="math inline">T(n) \in \Theta(n)</span>.</p>
<p>The exact recurrence for this is <span class="math inline">T(n) \le T(\floor{\frac n 5}) + T(\floor{\frac{7n + 12}{10}}) + \Theta(n)</span>. This accounts for the rounding down in the partitioning, and accounts for the last <span class="math inline">\theta</span> values we were ignoring earlier.</p>
<h2 id="greedy-algorithms">Greedy Algorithms</h2>
<p>Greedy algorithms are often used for <strong>optimization problems</strong>, which are problems where the goal is to find a solution that both satisfies the <strong>problem constraints</strong>, and maximizes/minimizes an <strong>objective/profit/cost function</strong>.</p>
<p><strong>Feasible solutions</strong> are those that satisfy the problem constraints, and given a problem instance <span class="math inline">I</span>, are represented as <span class="math inline">\mathrm{feasible}(I)</span>. The objective function is in <span class="math inline">\mathrm{feasible}(I) \to \mb{R}</span>.</p>
<p>For greedy algorithms, it is generally the proof of correctness that is the hard part. Greedy algorithms build up partial solutions, making locally optimal decisions at each step, with the goal of getting a globally optimal result.</p>
<p>A feasible solution should be writeable as an <span class="math inline">n</span>-tuple <span class="math inline">\tup{x_1, \ldots, x_n}</span>. A <strong>partial solution</strong> is an <span class="math inline">i</span>-tuple <span class="math inline">\tup{x_1, \ldots, x_i}</span> where <span class="math inline">i &lt; n</span> and none of the problem constraints are violated (note that this is not necessarily a feasible solution). A partial solution isn't always extendible to a feasible solution - some partial solutions are dead ends, and greedy algorithms can't get an optimal solution here.</p>
<p>A <strong>choice set</strong> for a partial solution <span class="math inline">\tup{x_1, \ldots, x_i}</span> is the set <span class="math inline">\mathcal{X}</span> of all elements we can append to <span class="math inline">\tup{x_1, \ldots, x_i}</span> to make another, longer partial or feasible solution <span class="math inline">\tup{x_1, \ldots, x_i, y}</span> - the set of all possible <span class="math inline">y</span>.</p>
<h1 id="section-8">14/10/15</h1>
<p>For any <span class="math inline">y</span> in the choice set <span class="math inline">\mathcal{X}</span>, we have a <strong>local evaluation criterion</strong> <span class="math inline">g(y)</span>, that measures the profit/cost of including <span class="math inline">y</span> in the current partial solution. <strong>Extension</strong> of a partial solution is the process of adding the <span class="math inline">y</span> with the highest <span class="math inline">g(y)</span> value to the partial solution. A greedy algorithm, in general, is repeated extension until we have a feasible solution <span class="math inline">X</span>.</p>
<p>Greedy algorithms do no backtracking or lookahead - they only consider the elements of the choice set at each step. That means they are often faster than algorithms that do backtracking, but the solution is possibly not as good. When implementing greedy algorithms, it is often efficient to do a preprocessing step to compute the local evaluation criterion, like sorting the elements of an array.</p>
<p>For some greedy algorithms, it is possible to always obtain an optimal solution. However, these proofs are often rather difficult.</p>
<p>Suppose we have a set <span class="math inline">A</span> of pairs representing intervals. What is the largest subset <span class="math inline">B</span> of these pairs such that none of the intervals intersect?</p>
<p>One greedy strategy might be to repeatedly choose the interval with the earliest start time that doesn't intersect with any intervals in the current partial solution. Another might be to choose the interval with the smallest duration that doesn't intersect. A third is to choose the interval with the earliest end time that doesn't intersect.</p>
<p>Clearly, the first one is not valid because we can make a counterexample: an interval that starts early but is really long is chosen over multiple non-intersecting short intervals that start a bit later, which means this strategy doesn't always give the optimal solution.</p>
<p>It is also possible to find a counterexample for the second strategy - a short interval that intersects with two non-intersecting longer intervals is chosen over those two, which means this strategy also doesn't always give the optimal solution.</p>
<p>The last strategy is correct. We can implement it as follows:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> select_interval(intervals):
    intervals <span class="op">=</span> <span class="bu">sorted</span>(intervals, key<span class="op">=</span><span class="kw">lambda</span> interval: interval[<span class="dv">1</span>])
    solution <span class="op">=</span> [intervals[<span class="dv">0</span>]]
    latest_finish <span class="op">=</span> intervals[<span class="dv">0</span>][<span class="dv">1</span>]
    <span class="cf">for</span> interval <span class="kw">in</span> intervals[<span class="dv">1</span>:]:
        <span class="cf">if</span> interval[<span class="dv">0</span>] <span class="op">&gt;=</span> latest_finish:
            solution.append(interval)
            latest_finish <span class="op">=</span> interval[<span class="dv">1</span>]
    <span class="cf">return</span> solution</code></pre></div>
<p>Most correctness proofs for greedy algorithms use strong induction. The proof for the greedy interval selection algorithm is relatively straightforward:</p>
<blockquote>
<p>Let <span class="math inline">A</span> be an array of intervals, sorted by their end values.<br />
Let <span class="math inline">\mathcal{B} = \tup{A_{i_1}, \ldots, A_{i_k}}</span> where <span class="math inline">i_1 &lt; \ldots &lt; i_k</span> be the greedy solution.<br />
Let <span class="math inline">\mathcal{O} = \tup{A_{j_1}, \ldots, A_{j_k}}</span> where <span class="math inline">j_1 &lt; \ldots &lt; j_l</span> be the optimal solution.<br />
Clearly, <span class="math inline">k \le l</span> since <span class="math inline">O</span> is the optimal solution, and it is impossible for the greedy solution to be better than optimal.<br />
We want to first prove that <span class="math inline">i_m \le j_m</span> for all <span class="math inline">1 \le m \le k</span>, which means that the ends of the intervals in the greedy solution are always before the ends of the corresponding intervals in the optimal solution.<br />
For <span class="math inline">m = 1</span>, <span class="math inline">i_1 \le j_1</span> since <span class="math inline">i_1 = 1</span> - the greedy algorithm always initially chooses the interval that ends earliest.<br />
Assume <span class="math inline">i_{m - 1} \le j_{m - 1}</span>. So in the greedy solution, the <span class="math inline">m - 1</span>th interval ends no later than the <span class="math inline">m - 1</span>th interval in the optimal solution.<br />
Suppose <span class="math inline">i_m &gt; j_m</span>. Clearly, the optimal solution's <span class="math inline">m</span>th interval ends earlier than the greedy algorithm's <span class="math inline">m</span>th interval, since <span class="math inline">A</span> is sorted by end values.<br />
However, this is not possible, since that means the greedy algorithm didn't choose the earliest end time. So <span class="math inline">i_m \le j_m</span>.<br />
Suppose <span class="math inline">k &lt; l</span>. Clearly, the optimal solution must have at least <span class="math inline">k + 1</span> intervals.<br />
Clearly, <span class="math inline">i_k \le j_k</span>. So the optimal solution's <span class="math inline">k + 1</span>th interval must be in the choice set for the greedy algorithm at the <span class="math inline">k</span>th step.<br />
However, this isn't possible since the greedy algorithm didn't choose that interval, so <span class="math inline">k \ge l</span>.<br />
Since <span class="math inline">k \le l</span> and <span class="math inline">k \ge l</span>, <span class="math inline">k = l</span> and the greedy solution is the same length as the optimal solution, and so the greedy algorithm must be optimal.</p>
</blockquote>
<p>The time complexity of this algorithm is easy to prove - it's <span class="math inline">\Theta(n \log n)</span> for the sort and <span class="math inline">\Theta(n)</span> for the traversal, so <span class="math inline">\Theta(n \log n)</span> overall.</p>
<p>Suppose we want to partition <span class="math inline">A</span> into disjoint sets of non-intersecting intervals, minimizing the number of these sets. This is called the interval coloring problem - we want to assign a color to each interval such that all intervals with a given color do not intersect each other.</p>
<p>An optimal greedy strategy for this is to sort the intervals by start values ascending, and for each interval in order, attempt to insert each interval into existing sets (assign it an existing color), and create a new set if not possible (assign it a new color):</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> partition_intervals(intervals):
    intervals <span class="op">=</span> <span class="bu">sorted</span>(intervals, key<span class="op">=</span><span class="kw">lambda</span> interval: interval[<span class="dv">0</span>])
    partitions <span class="op">=</span> [{intervals[<span class="dv">0</span>]}]
    partition_latest_finishes <span class="op">=</span> [intervals[<span class="dv">0</span>][<span class="dv">1</span>]]
    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="bu">len</span>(intervals)):
        interval <span class="op">=</span> intervals[i]
        <span class="cf">for</span> partition_index, latest_finish <span class="kw">in</span> <span class="bu">enumerate</span>(partition_latest_finishes): <span class="co"># try to assign interval to existing partitions</span>
            <span class="cf">if</span> latest_finish <span class="op">&lt;=</span> interval[<span class="dv">0</span>]:
                partitions[partition_index].add(interval)
                partition_latest_finishes[partition_index] <span class="op">=</span> interval[<span class="dv">1</span>]
                <span class="cf">break</span>
        <span class="cf">else</span>: <span class="co"># no existing colors can accept the interval</span>
            partition_index <span class="op">=</span> <span class="bu">len</span>(partition_latest_finishes)
            partitions[partition_index] <span class="op">=</span> {interval}
            partition_latest_finishes.append(interval[<span class="dv">1</span>])
    <span class="cf">return</span> partitions</code></pre></div>
<p>The correctness proof for this algorithm is a bit unusual:</p>
<blockquote>
<p>Suppose the greedy algorithm uses <span class="math inline">D</span> colors. Since we only create new colors if existing colors can't accept the interval, for every color <span class="math inline">c &lt; D</span>, there exists an interval <span class="math inline">A_c</span> that overlaps <span class="math inline">A_i</span>.<br />
That means that there are at least <span class="math inline">D</span> intervals that all overlap each other, since they all overlap <span class="math inline">A_i</span>.<br />
Therefore there must be at least <span class="math inline">D</span> colors, which means that having <span class="math inline">D</span> colors is optimal.</p>
</blockquote>
<p>Clearly, this algorithm is <span class="math inline">O(n^2)</span>, or more specifically <span class="math inline">O(nD)</span> since <span class="math inline">D</span> is <span class="math inline">O(n)</span>. We can actually do better by using a different data structure for the inner loop - the inner loop simply is trying to find the lowest end value in a partition, so if we use a heap for the partitions instead, we can have the algorithm run in <span class="math inline">O(n \log D)</span>, or more simply, <span class="math inline">O(n \log n)</span>:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">import</span> heapq
<span class="kw">def</span> partition_intervals(intervals):
    intervals <span class="op">=</span> <span class="bu">sorted</span>(intervals, key<span class="op">=</span><span class="kw">lambda</span> interval: interval[<span class="dv">0</span>])
    partitions <span class="op">=</span> [{intervals[<span class="dv">0</span>]}]
    partition_latest_finishes_heap <span class="op">=</span> [(intervals[<span class="dv">0</span>][<span class="dv">1</span>], <span class="dv">0</span>)]
    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="bu">len</span>(intervals)):
        interval <span class="op">=</span> intervals[i]
        latest_finish, partition_index <span class="op">=</span> heapq.heappop(partition_latest_finishes_heap)
        <span class="cf">if</span> latest_finish <span class="op">&lt;=</span> interval[<span class="dv">0</span>]: <span class="co"># try to assign interval to the existing partition that ends earliest</span>
            partitions[partition_index].add(interval)
            heapq.heappush(partition_latest_finishes_heap, (interval[<span class="dv">1</span>], partition_index))
        <span class="cf">else</span>: <span class="co"># no existing colors can accept the interval</span>
            partition_index <span class="op">=</span> <span class="bu">len</span>(partition_latest_finishes_heap)
            partitions[partition_index] <span class="op">=</span> {interval}
            heapq.heappush(partition_latest_finishes_heap, (interval[<span class="dv">1</span>], partition_index))
    <span class="cf">return</span> partitions</code></pre></div>
<h1 id="section-9">19/10/15</h1>
<p>Consider the <strong>knapsack problem</strong>: given profits <span class="math inline">P = [P_1, \ldots, P_n]</span> and weights <span class="math inline">W = [W_1, \ldots, W_n]</span>, and a capacity <span class="math inline">M</span>, all positive integers, find the <span class="math inline">x = \tup{x_1, \ldots, x_n}</span> that maximises <span class="math inline">\sum P_i x_i</span> given that <span class="math inline">\sum W_i x_i \le M</span>.</p>
<p>There are several variations of this problem: for the 0-1 knapsack problem, <span class="math inline">x_i</span> is either 0 or 1 (we either take the item or don't); in the rational knapsack problem, <span class="math inline">0 \le x_i \le 1</span> (we can take a fraction of the item from none to all).</p>
<p>As it turns out, the rational knapsack problem can be solved greedily (but not other knapsack problems; 0-1 knapsack problems are NP-complete, for example) - we simply take as much as possible of each item, considering them in decreasing order of profit divided by weight:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> greedy_rational_knapsack(profits, weights, capacity):
    items <span class="op">=</span> <span class="bu">sorted</span>(<span class="bu">zip</span>(profits, weights), <span class="kw">lambda</span> x: x[<span class="dv">0</span>] <span class="op">/</span> x[<span class="dv">1</span>])
    current_weight <span class="op">=</span> <span class="dv">0</span>
    <span class="cf">for</span> item <span class="kw">in</span> items: <span class="co"># consider items in decreasing order of profit to weight ratio</span>
        profit, weight <span class="op">=</span> item
        <span class="cf">if</span> current_weight <span class="op">+</span> weight <span class="op">&lt;=</span> capacity: <span class="co"># we have enough space to take all of the item, so take it</span>
            result.append(<span class="dv">1</span>)
            current_weight <span class="op">+=</span> weight
        <span class="cf">else</span>: <span class="co"># we can&#39;t take all of the item, but we&#39;ll take enough to fill the rest of the knapsack</span>
            result.append((capacity <span class="op">-</span> current_weight) <span class="op">/</span> weight)
            <span class="cf">break</span>
    <span class="cf">return</span> result</code></pre></div>
<p>Proof of correctness:</p>
<blockquote>
<p>Suppose the greedy algorithm is not optimal. Then for certain inputs the output must differ from optimal algorithms.<br />
Let the result of the greedy algorithm be <span class="math inline">G = [G_1, \ldots, G_n]</span> and a result <span class="math inline">B = [B_1, \ldots, B_n]</span> of all optimal algorithms for this problem such that <span class="math inline">B</span> and <span class="math inline">G</span> agree for as many items as possible, disagreeing at index <span class="math inline">d</span>, so <span class="math inline">B = [G_1, \ldots, G_{d - 1}, B_d, \ldots, B_n]</span>.<br />
The greedy algorithm always chooses the item with the highest profit per weight, so the actual profit from <span class="math inline">G_1, \ldots, G_d</span> is at least as much as the actual profit of <span class="math inline">B_1, \ldots, B_d</span>, because we will pick the most.<br />
Construct <span class="math inline">B&#39; = [B_1, \ldots, B_{d - 1}, G_d, B_{d + 1}, \ldots, B_n]</span>. Clearly, <span class="math inline">B&#39;</span> is at least as profitable as <span class="math inline">B</span>, but it agrees with <span class="math inline">G</span> for 1 more element than <span class="math inline">B</span>, a contradiction.<br />
Therefore, the greedy algorithm is optimal.</p>
</blockquote>
<p>Consider the <strong>coin changing problem</strong>: given a list of coin denominations <span class="math inline">d_1, \ldots, d_n</span> and a positive integer <span class="math inline">T</span> (target sum), find <span class="math inline">A = [A_1, \ldots, A_n]</span> such that <span class="math inline">T = \sum a_i d_i</span> and <span class="math inline">N = \sum A_i</span> is minimised. In other words, we are trying to find the smallest set of coins that add up to a certain value.</p>
<p>We can do this simply by considering the coins from largest denomination to smallest, filling up the sum as close to the target sum as possible. Although this works for a coin system like <span class="math inline">1, 5, 10, 25, 100, 200</span>, this isn't always optimal: the coin system <span class="math inline">12, 5, 1</span> doesn't give the optimal solution for something like a target sum of 15. The study of where these algorithms are optimal is an area of active research.</p>
<p>Consider the <strong>stable marriage problem</strong>: suppose we have sets of <span class="math inline">n</span> men and <span class="math inline">n</span> women, <span class="math inline">M = [M_1, \ldots, M_n], W = [W_1, \ldots, W_n]</span>. Define <span class="math inline">\mathrm{pref}(M_i, k)</span> is the <span class="math inline">k</span>th favourite <span class="math inline">w \in W</span> for <span class="math inline">M_i</span>, and <span class="math inline">\mathrm{pref}(W_i, k)</span> is the <span class="math inline">k</span>th favourite <span class="math inline">m \in M</span> for <span class="math inline">W_i</span>. Find the matching <span class="math inline">\set{(M_{i_1}, W_{j_1}), \ldots, (M_{i_n}, W_{j_n})}</span> such that there does not exist any <span class="math inline">(M_u, W_v)</span> that are not in the set, but prefer each other to their matches in the set - a <strong>stable matching</strong>.</p>
<p>Formally, we want to find a matching such that</p>
<p>The Gale-Shapley algorithm is an algorithm for this problem, for which Gale and Shapely won a Nobel prize for economics. In this algorithm, men propose to women from their most preferable to least preferable, and women accept proposals if not engaged or if a proposal is better than their currently accepted proposal (the new proposal replaces the old one), and women reject proposals otherwise.</p>
<p>Basically, the men propose starting from their most preferable to least preferable, and the women are proposed to starting from the least preferable to most preferable - the men always move down in their preferences, and the women always move up in their preferences:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> woman_prefers(woman, preference_function, man1, man2):
    i <span class="op">=</span> <span class="dv">1</span>
    <span class="cf">while</span> <span class="va">True</span>:
        man <span class="op">=</span> preference_function(woman, i)
        <span class="cf">if</span> man <span class="op">==</span> man1: <span class="cf">return</span> <span class="va">True</span>
        <span class="cf">if</span> man <span class="op">==</span> man2: <span class="cf">return</span> <span class="va">False</span>
        i <span class="op">+=</span> <span class="dv">1</span>

<span class="kw">def</span> gale_shapley(men, women, preference_function):
    engagements <span class="op">=</span> {}
    men_ranks <span class="op">=</span> {man: <span class="dv">0</span> <span class="cf">for</span> man <span class="kw">in</span> men}
    unengaged_men <span class="op">=</span> <span class="bu">set</span>(men)
    <span class="cf">while</span> unengaged_men:
        man <span class="op">=</span> <span class="bu">next</span>(unengaged_men.keys())
        men_ranks[man] <span class="op">+=</span> <span class="dv">1</span> <span class="co"># next rank in preferences</span>
        woman <span class="op">=</span> preference_function(man, men_ranks[man])
        <span class="cf">if</span> woman <span class="kw">not</span> <span class="kw">in</span> engagements <span class="kw">or</span> woman_prefers(woman, preference_function, man, engagements[woman]):
            <span class="cf">if</span> woman <span class="kw">in</span> engagements: unengaged_men.add(engagements[man])
            unengaged_men.remove(man)
            engagements[woman] <span class="op">=</span> man
    <span class="cf">return</span> <span class="bu">set</span>(engagements.items())</code></pre></div>
<p>This algorithm is good for the men, but not the women - suiters can ask any of the reviewers, but reviewers can only pick from the suiters that proposed to them. For example, given 3 men and 3 women with preferences <span class="math inline">M_1: W_2, W_1, W_3; M_2: W_3, W_2, W_1; M_3: W_1, W_3, W_2; W_1: M_2, M_1, M_3; W_2: M_3, M_2, M_1; W_3: M_1, M_3, M_2</span>, all men get their first choices, while all women get their last choice. The men get the best possible match that will accept them, while the women get the worst possible match they will accept.</p>
<p>This algorithm always terminates because women cannot reject offers if unengaged, there is a man for every woman, and at some point a man will ask that woman. In the worst case, the men will all have their last choise, and have to propose to all <span class="math inline">n</span> of their preferences. Since there are <span class="math inline">n</span> men, the algorithm takes <span class="math inline">O(n^2)</span> proposals, specifically <span class="math inline">n^2 - n + 1</span> or less.</p>
<p>Proof of correctness:</p>
<blockquote>
<p>We want to prove that there is no man that prefers a woman that also prefers that man. Suppose Alice prefers Bob over her current partner, and Bob also prefers Alice over his current partner.<br />
If Bob prefers Alice, then he must have proposed to her before his current partner.<br />
Alice must have rejected the proposal, since Bob is engaged to a less preferable choice.<br />
However, Alice is with a less preferable partner, which means Alice should have accepted Bob and rejected her current partner when he proposed.<br />
This is a contradiction, so Alice and Bob cannot exist, and the matching is stable.</p>
</blockquote>
<h1 id="section-10">21/10/15</h1>
<p>To implement the Gale-Shapely algorithm more efficiently, we can represent the set of unengaged men as a linked list, represent the men's preferences using arrays/linked lists, and represent engagements using an array. We could also precompute a lookup table for <span class="math inline">M_j = \mathrm{pref}(W_i, k)</span> to look up <span class="math inline">j</span> given <span class="math inline">W_i</span> and <span class="math inline">M_j</span>.</p>
<h2 id="dynamic-programming">Dynamic Programming</h2>
<p>Consider a naive function to compute the <span class="math inline">n</span>th Fibonacci number, based on its recurrence definition:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> fibinacci(n):
    <span class="cf">if</span> n <span class="op">&lt;=</span> <span class="dv">1</span>: <span class="cf">return</span> n
    <span class="cf">return</span> fibinacci(n <span class="op">-</span> <span class="dv">1</span>) <span class="op">+</span> fibinacci(n <span class="op">-</span> <span class="dv">2</span>)</code></pre></div>
<p>However, this is extremely inefficient. Using the recurrence tree method, we find that it takes <span class="math inline">2f_{n + 1}</span> total calls of the function where <span class="math inline">f_n</span> is the <span class="math inline">n</span>th Fibonacci number. Using the closed form for the Fibonacci number, <span class="math inline">f_n = \frac{\phi^n - (-\phi)^{-n}}{\sqrt{5}}</span> where <span class="math inline">\phi</span> is the golden ratio, or around 1.62. Therefore, this function takes <span class="math inline">\Omega(\phi^n)</span> time.</p>
<p>This function basically is inefficient because it computes the same thing over and over again - for example, for <code>fibonacci(100)</code> we will call <code>fibonacci(10)</code> millions of times, when it gives the same result each time.</p>
<p>The idea behind dynamic programming is to avoid recomputing the same thing over and over again, by keeping those results around and simply looking them up when necessary. As a result dynamic programming trades off memory for time.</p>
<p>For example, if we were to write the above function iteratively, we might start from the base case and build upward toward the result:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> fibinacci(n):
    saved <span class="op">=</span> [<span class="dv">0</span>, <span class="dv">1</span>]
    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>, n <span class="op">+</span> <span class="dv">1</span>):
        saved[i] <span class="op">=</span> saved[i <span class="op">-</span> <span class="dv">1</span>] <span class="op">+</span> saved[i <span class="op">-</span> <span class="dv">2</span>]
    <span class="cf">return</span> saved[n]</code></pre></div>
<p>However, note that at any given time, we only really use the last two elements of <code>saved</code>. Therefore, we can just store the last two elements:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> fibinacci(n):
    <span class="cf">if</span> n <span class="op">&lt;=</span> <span class="dv">1</span>: <span class="cf">return</span> n
    last1, last2 <span class="op">=</span> <span class="dv">0</span>, <span class="dv">1</span>
    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>, n <span class="op">+</span> <span class="dv">1</span>):
        current <span class="op">=</span> last1 <span class="op">+</span> last2
        last1, last2 <span class="op">=</span> current, last1
    <span class="cf">return</span> current</code></pre></div>
<p>This is much better - we now have <span class="math inline">\Theta(n)</span> iterations and constant memory. Since the numbers grow exponentially, however, and addition takes <span class="math inline">\Theta(\log n)</span> (using multiprecision arithmetic), addition will actually take <span class="math inline">\Theta(n)</span>, so the algorithm overall takes <span class="math inline">\Theta(n^2)</span>.</p>
<p>Dyanmic programming is used to solve optimization problems, like greedy algorithms are often used to do.</p>
<p>To solve a problem using dynamic programming, we examine the structure of an optimal solution - the <strong>optimal structure</strong>. Given a problem instance <span class="math inline">I</span>:</p>
<ol type="1">
<li>Define all the relevant subproblems <span class="math inline">S(I)</span> that allows us to compute <span class="math inline">I</span>.
<ul>
<li>This is just the direct subproblems, not the subproblems of the subproblems. For example, for <code>fibonacci(n)</code> we would have <span class="math inline">S(I)</span> be <code>fibonacci(n - 1)</code> and <code>fibonacci(n - 2)</code>.</li>
</ul></li>
<li>Write a recurrence relation for <span class="math inline">I</span> in terms of <span class="math inline">S(I)</span> and the base cases.
<ul>
<li>The references to <span class="math inline">S(I)</span> allow us to look up the answers later in the table.</li>
</ul></li>
<li>Compute the optimal solution to <span class="math inline">I</span> using the recurrence relation from the bottom-up (starting from the base cases of the recurrence relation) - filling in a table of values for the solutions to each subproblem.
<ul>
<li>Whenever we have a subproblem we've previously encountered, we can simply look up the answer in the table to avoid recomputing it.</li>
<li>When all subproblems have been solved, we can often compute the solution to <span class="math inline">I</span> very simply using values from the table.</li>
</ul></li>
</ol>
<p>This is somewhat similar to divide and conquer, where we have subproblems and solve them recursively, up until we actually need to combine the solutions - instead of combining recursively, we solve them from the bottom up.</p>
<p>While the 0-1 knapsack problem we saw previously is NP-complete, we can solve it relatively well using dynamic programming.</p>
<p>Suppose <span class="math inline">[x_1, \ldots, x_n]</span> is an optimal solution for a given problem instance where <span class="math inline">P = [P_1, \ldots, P_n]</span> and weights <span class="math inline">W = [W_1, \ldots, W_n]</span>, and capacity <span class="math inline">M</span>. Clearly, either <span class="math inline">x_n = 0</span> or <span class="math inline">x_n = 1</span>.</p>
<p>If <span class="math inline">x_n = 0</span>, then <span class="math inline">[x_1, \ldots, x_{n - 1}]</span> is an optimal solution to a smaller problem instance where <span class="math inline">P = [P_1, \ldots, P_{n - 1}]</span> and weights <span class="math inline">W = [W_1, \ldots, W_{n - 1}]</span>, and capacity <span class="math inline">M</span>, and the profit of <span class="math inline">[x_1, \ldots, x_n]</span> is the same as the profit of <span class="math inline">[x_1, \ldots, x_{n - 1}]</span>.</p>
<p>If <span class="math inline">x_n = 1</span>, then <span class="math inline">[x_1, \ldots, x_{n - 1}]</span> is an optimal solution to a smaller problem instance where <span class="math inline">P = [P_1, \ldots, P_{n - 1}]</span> and weights <span class="math inline">W = [W_1, \ldots, W_{n - 1}]</span>, and capacity <span class="math inline">M - W_n</span>, and the profit of <span class="math inline">[x_1, \ldots, x_n]</span> is the same as the profit of <span class="math inline">[x_1, \ldots, x_{n - 1}]</span> plus <span class="math inline">P_n</span>.</p>
<p>Clearly, the profit of <span class="math inline">[x_1, \ldots, x_n]</span> is the larger of the two cases, where either <span class="math inline">x_n = 0</span> or <span class="math inline">x_n = 1</span>.</p>
<p>These are our relevant subproblems - the knapsack problem instances with the same weights and profits, but only including the first <span class="math inline">0 \le i \le n</span> items and for different capacities <span class="math inline">0 \le m \le M</span>, so <span class="math inline">P = [P_1, \ldots, P_i]</span> and weights <span class="math inline">W = [W_1, \ldots, W_i]</span>.</p>
<p>Now we can define a 2D array (a table) <span class="math inline">P[i, m]</span> that defines the optimal profit of such a subproblem, considering the first <span class="math inline">i</span> items and for a knapsack with capacity <span class="math inline">m</span>.</p>
<p>The basic idea will be to compute <span class="math inline">P[i, m]</span> for all <span class="math inline">i</span> and <span class="math inline">m</span>, until we get to <span class="math inline">P(n, M)</span>.</p>
<h1 id="section-11">26/10/15</h1>
<p>Now we'll write the recurrence relation: <span class="math inline">P[i, m] = \max\left(P[i - i, m], P[i - 1, m - W_i]\right)</span>. Here, <span class="math inline">P[i - i, m]</span> represents the case where <span class="math inline">x_i = 0</span>, and <span class="math inline">P[i - 1, m - W_i]</span> represents the case where <span class="math inline">x_i = 1</span>.</p>
<p>We also need some base cases: <span class="math inline">P[i, m] = \begin{cases} 0 &amp;\text{if } i = 1 \wedge m &lt; W_i \\ P_1 &amp;\text{if } i = 1 \wedge m \ge W_i \\ P[i - 1, m] &amp;\text{if } i &gt; 1 \wedge m &lt; W_i \\ \max\left(P[i - i, m - W_i] + P_i, P[i - 1, m]\right) &amp;\text{otherwise} \end{cases}</span>.</p>
<p>Now we fill in an <span class="math inline">n</span> by <span class="math inline">M + 1</span> table of all <span class="math inline">P[i, m]</span> values, For each row <span class="math inline">i</span>, we fill in all the <span class="math inline">m</span> values from smallest to largest. The final answer is therefore at the last cell of the bottommost row.</p>
<p>In code, it looks like this:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">from</span> collections <span class="im">import</span> defaultdict
<span class="kw">def</span> knapsack(profits, weights, max_weight):
    max_profits <span class="op">=</span> defaultdict(<span class="bu">dict</span>)
    <span class="cf">for</span> weight <span class="kw">in</span> <span class="bu">range</span>(max_weight):
        <span class="cf">if</span> weight <span class="op">&gt;=</span> weights[<span class="dv">0</span>]:
            max_profits[<span class="dv">0</span>][m] <span class="op">=</span> profits[<span class="dv">0</span>]
        <span class="cf">else</span>:
            max_profits[<span class="dv">0</span>][m] <span class="op">=</span> <span class="dv">0</span>
    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="bu">len</span>(profits)):
        <span class="cf">for</span> weight <span class="kw">in</span> <span class="bu">range</span>(max_weight):
            <span class="cf">if</span> weight <span class="op">&lt;</span> weights[i]:
                max_profits[i][weight] <span class="op">=</span> P[i <span class="op">-</span> <span class="dv">1</span>][weight]
            <span class="cf">else</span>:
                max_profits[i][weight] <span class="op">=</span> <span class="bu">max</span>(max_profits[i <span class="op">-</span> <span class="dv">1</span>][weight <span class="op">-</span> weights[i]] <span class="op">+</span> profits[i], max_profits[i <span class="op">-</span> <span class="dv">1</span>][weight])
    <span class="cf">return</span> max_profits[<span class="bu">len</span>(profits) <span class="op">-</span> <span class="dv">1</span>][max_weight <span class="op">-</span> <span class="dv">1</span>]</code></pre></div>
<p>One improvement we could make is that in the last row we only need to compute the single entry <span class="math inline">P[n, M]</span>, since it only relies on <span class="math inline">P[n - 1, M - w_i]</span> and <span class="math inline">P[n - 1, M]</span>.</p>
<p>The above algorithm finds the maximum profit, but not the actual items chosen to get that profit. We can get the items by repeatedly finding which <span class="math inline">\max\left(P[i - i, m - W_i] + P_i, P[i - 1, m]\right)</span> we chose at each step, and if <span class="math inline">P[i, m] = P[i - i, m - W_i] + P_i</span>, then we include the item in the solution. then, we repeatedly do the same check for <span class="math inline">P[i - i, m - W_i]</span> if <span class="math inline">P[i, m] = P[i - i, m - W_i] + P_i</span> or do the same check for <span class="math inline">P[i - 1, m]</span> if <span class="math inline">P[i, m] = P[i - 1, m]</span>, and so on:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">from</span> collections <span class="im">import</span> defaultdict
<span class="kw">def</span> knapsack(profits, weights, max_weight):
    max_profits <span class="op">=</span> defaultdict(<span class="bu">dict</span>)
    <span class="cf">for</span> weight <span class="kw">in</span> <span class="bu">range</span>(max_weight):
        <span class="cf">if</span> weight <span class="op">&gt;=</span> weights[<span class="dv">0</span>]:
            max_profits[<span class="dv">0</span>][m] <span class="op">=</span> profits[<span class="dv">0</span>]
        <span class="cf">else</span>:
            max_profits[<span class="dv">0</span>][m] <span class="op">=</span> <span class="dv">0</span>
    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="bu">len</span>(profits)):
        <span class="cf">for</span> weight <span class="kw">in</span> <span class="bu">range</span>(max_weight):
            <span class="cf">if</span> weight <span class="op">&lt;</span> weights[i]:
                max_profits[i][weight] <span class="op">=</span> P[i <span class="op">-</span> <span class="dv">1</span>][weight]
            <span class="cf">else</span>:
                max_profits[i][weight] <span class="op">=</span> <span class="bu">max</span>(max_profits[i <span class="op">-</span> <span class="dv">1</span>][weight <span class="op">-</span> weights[i]] <span class="op">+</span> profits[i], max_profits[i <span class="op">-</span> <span class="dv">1</span>][weight])
    
    <span class="co"># figure out which items are in this max profit selection</span>
    weight <span class="op">=</span> max_weight
    items <span class="op">=</span> [<span class="va">None</span>] <span class="op">*</span> <span class="bu">len</span>(profits)
    profit <span class="op">=</span> max_profits[<span class="bu">len</span>(profits) <span class="op">-</span> <span class="dv">1</span>, capacity <span class="op">-</span> <span class="dv">1</span>]
    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>[<span class="bu">len</span>(profits) <span class="op">-</span> <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">-1</span>]: <span class="co"># go through indices from the last row&#39;s index to the second row&#39;s index</span>
        <span class="cf">if</span> profit <span class="op">==</span> max_profits[i <span class="op">-</span> <span class="dv">1</span>, weight]:
            items[i] <span class="op">=</span> <span class="va">False</span>
        <span class="cf">else</span>:
            items[i] <span class="op">=</span> <span class="va">True</span>
        profit <span class="op">-=</span> profits[i]
        weight <span class="op">-=</span> weights[i]
    items[<span class="dv">0</span>] <span class="op">=</span> profit <span class="op">==</span> <span class="dv">0</span>

    <span class="cf">return</span> items</code></pre></div>
<p>Clearly, this is an <span class="math inline">\Theta(nM)</span> algorithm, since the table has <span class="math inline">nM</span> cells and each cell value takes constant time to compute, and finding the actual items can be done in <span class="math inline">\Theta(n)</span> since we trace one row per iteration.</p>
<p>Although this seems to be a polynomial time algorithm, this is actually not the case. The size of a given problem instance is the <strong>sum of the sizes of its inputs</strong>.</p>
<p>At first, we might assume that this is <span class="math inline">n + M</span>. However, the size of the <span class="math inline">M</span> input is <strong>actually the number of bits it has</strong>, which is <span class="math inline">k = \ceil{\log_2 M}</span> - the size of the input is <span class="math inline">n + k</span>. Therefore, the algorithm is <span class="math inline">O(n2^k)</span>. This is consistent with the fact that the knapsack problem is NP-complete.</p>
<p>There is also a recursive backtracking algorithm that tries all the combinations of items and finds the largest profit, which runs in <span class="math inline">\Theta(2^n)</span>. This is useful in different in different situations - even though both are exponential, the dynamic programming one is better for small <span class="math inline">M</span> and large <span class="math inline">n</span>, and the backtracking one is better for small <span class="math inline">n</span> and large <span class="math inline">M</span>.</p>
<p>We can solve the coin changing problem optimally as well, in almost the same way:</p>
<blockquote>
<p>Note that for <span class="math inline">d_n</span>, <span class="math inline">0 \le a_n \le \frac T {d_n}</span>.<br />
Our dynamic programming table will be <span class="math inline">n</span> by <span class="math inline">\ceil{\frac{T}{d_1}}</span> - each row is a number of coin denominations used, and each column is a different target sum.<br />
Let <span class="math inline">N[i, t]</span> be a list of coins used in the optimal solution to the coin changing problem instance with only the coin denominations <span class="math inline">d_1, \ldots, d_i</span> and target sum <span class="math inline">t</span>.<br />
Then we have the recurrence relation <span class="math inline">N[i, t] = \begin{cases} j \text{ where } 0 \le j \le \floor{\frac t d_i} \text{ is the index for which } N[i - 1, t - jd_i] + j \text{ is minimized} &amp;\text{if } i &gt; 1 \\ t &amp;\text{if } i = 1 \end{cases}</span>.<br />
The final answer is in <span class="math inline">N[n, T]</span>. We could also save memory by, instead of having <span class="math inline">N[i, t]</span> for solutions, having a table <span class="math inline">A[i, t]</span> store the number of coins used in the optimal solutions, and then at the end tracing backwards like we did for the knapsack problem.</p>
</blockquote>
<h1 id="section-12">28/10/15</h1>
<h3 id="longest-common-subsequence">Longest Common Subsequence</h3>
<p>Suppose we have two sequences <span class="math inline">X = [X_1 \ldots X_m]</span> and <span class="math inline">Y = [Y_1 \ldots Y_n]</span> where all elements are in some finite alphabet <span class="math inline">A</span>. What is the longest subsequence that is in both <span class="math inline">X</span> and <span class="math inline">Y</span>?</p>
<p>A subsequence of a sequence <span class="math inline">Z</span> is a sequence that contains any number of the elements of <span class="math inline">Z</span> in order, but not necessarily consecutively. For example, <span class="math inline">1, 4, 8, 9</span> is a subsequence of <span class="math inline">1, 2, 3, 4, 5, 6, 7, 8, 9</span>.</p>
<p>We will solve this using dynamic programming. Let <span class="math inline">\mathrm{LCS}(X, Y)</span> represent the length of the longest common subsequence.</p>
<p>If <span class="math inline">X_m = Y_n</span>, then <span class="math inline">\mathrm{LCS}(X_1 \ldots X_m, Y_1 \ldots Y_n) = \mathrm{LCS}(X_1 \ldots X_{m - 1}, Y_1 \ldots Y_{n - 1}) + 1</span>.</p>
<p>If <span class="math inline">X_m \ne Y_n</span>, then <span class="math inline">\mathrm{LCS}(X_1 \ldots X_m, Y_1 \ldots Y_n) = \max\left(\mathrm{LCS}(X_1 \ldots X_{m - 1}, Y_1 \ldots Y_n), \mathrm{LCS}(X_1 \ldots X_m, Y_1 \ldots Y_{n - 1})\right)</span>. This is because there are three cases: if the LCS ends on <span class="math inline">Y_n</span>, then it doesn't end on <span class="math inline">X_m</span> and we have <span class="math inline">\mathrm{LCS}(X_1 \ldots X_m, Y_1 \ldots Y_n) = \mathrm{LCS}(X_1 \ldots X_{m - 1}, Y_1 \ldots Y_n)</span>; if the LCS ends on <span class="math inline">X_m</span>, then it doesn't end on <span class="math inline">Y_n</span> and we have <span class="math inline">\mathrm{LCS}(X_1 \ldots X_m, Y_1 \ldots Y_n) = \mathrm{LCS}(X_1 \ldots X_m, Y_1 \ldots Y_{n - 1})</span>. The third case is if the LCS doesn't end on <span class="math inline">X_m</span> or <span class="math inline">Y_n</span>, but we don't need to consider it in the formula because <span class="math inline">\mathrm{LCS}(X_1 \ldots X_{m - 1}, Y_1 \ldots Y_{n - 1}) \le \mathrm{LCS}(X_1 \ldots X_{m - 1}, Y_1 \ldots Y_n)</span> - it is already covered by the other cases.</p>
<p>The basic idea behind the recurrence is that we keep chopping off elements from the ends of <span class="math inline">X</span> and <span class="math inline">Y</span> - we are computing the LCS for all prefixes of the string. We now define our dynamic programming table, <span class="math inline">c[i, j] = \mathrm{LCS}(X_1 \ldots X_i, Y_1 \ldots Y_j)</span> (the rows are values of <span class="math inline">i</span> and the columns are values of <span class="math inline">j</span>).</p>
<p>Clearly, we now have the recurrence <span class="math inline">c[i, j] = \begin{cases} 0 &amp;\text{if } i = 0 \vee j = 0 \\ c[i - 1, j - 1] + 1 &amp;\text{if } X_i = Y_j \wedge i &gt; 0 \wedge j &gt; 0 \\ \max(c[i - 1, j], c[i, j - 1]) &amp;\text{if } X_i \ne Y_j \wedge i &gt; 0 \wedge j &gt; 0 \end{cases}</span>. We want to compute <span class="math inline">c[m, n]</span>.</p>
<p>Clearly, to compute any <span class="math inline">c[i, j]</span>, we need to ensure that <span class="math inline">c[i - 1, j - 1], c[i - 1, j], c[i, j - 1]</span> are already computed. We can do this by filling in the table from left to right, top to bottom. In code, it looks like the following:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">from</span> collections <span class="im">import</span> defaultdict
<span class="kw">def</span> LCS(X, Y):
    c <span class="op">=</span> defaultdict(<span class="bu">dict</span>)
    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(X) <span class="op">+</span> <span class="dv">1</span>): c[i][<span class="dv">0</span>] <span class="op">=</span> <span class="dv">0</span>
    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(Y) <span class="op">+</span> <span class="dv">1</span>): c[<span class="dv">0</span>][j] <span class="op">=</span> <span class="dv">0</span>
    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="bu">len</span>(X) <span class="op">+</span> <span class="dv">1</span>):
        <span class="cf">for</span> j <span class="kw">in</span> rnage(<span class="dv">1</span>, <span class="bu">len</span>(Y) <span class="op">+</span> <span class="dv">1</span>):
            <span class="cf">if</span> X[i] <span class="op">=</span> Y[j]:
                c[i, j] <span class="op">=</span> c[i <span class="op">-</span> <span class="dv">1</span>][j <span class="op">-</span> <span class="dv">1</span>]
            <span class="cf">else</span>:
                c[i, j] <span class="op">=</span> <span class="bu">max</span>(c[i][j <span class="op">-</span> <span class="dv">1</span>], c[i <span class="op">-</span> <span class="dv">1</span>][j <span class="op">-</span> <span class="dv">1</span>])
    <span class="cf">return</span> c[<span class="bu">len</span>(X), <span class="bu">len</span>(Y)]</code></pre></div>
<p>We can also get the actual sequence by doing a traceback like for the knapsack problem. There are three cases: if we moved up and to the left in the table, then we include <span class="math inline">X_i</span> or <span class="math inline">Y_j</span> in the LCS and set <span class="math inline">i = i - 1, j = j - 1</span>; if we moved left in th etable, we set <span class="math inline">j = j - 1</span>; if we moved right in the table, we set <span class="math inline">i = i - 1</span>. By repeatedly following this until we get a base case, we get the LCS:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">from</span> collections <span class="im">import</span> defaultdict
<span class="kw">def</span> LCS(X, Y):
    c <span class="op">=</span> defaultdict(<span class="bu">dict</span>)
    direction <span class="op">=</span> defaultdict(<span class="bu">dict</span>)
    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(X) <span class="op">+</span> <span class="dv">1</span>): c[i][<span class="dv">0</span>] <span class="op">=</span> <span class="dv">0</span>
    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(Y) <span class="op">+</span> <span class="dv">1</span>): c[<span class="dv">0</span>][j] <span class="op">=</span> <span class="dv">0</span>
    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="bu">len</span>(X) <span class="op">+</span> <span class="dv">1</span>):
        <span class="cf">for</span> j <span class="kw">in</span> rnage(<span class="dv">1</span>, <span class="bu">len</span>(Y) <span class="op">+</span> <span class="dv">1</span>):
            <span class="cf">if</span> X[i] <span class="op">=</span> Y[j]:
                c[i, j] <span class="op">=</span> c[i <span class="op">-</span> <span class="dv">1</span>][j <span class="op">-</span> <span class="dv">1</span>]
                direction[i][j] <span class="op">=</span> <span class="st">&quot;up_left&quot;</span>
            <span class="cf">else</span>:
                c[i, j] <span class="op">=</span> <span class="bu">max</span>(c[i][j <span class="op">-</span> <span class="dv">1</span>], c[i <span class="op">-</span> <span class="dv">1</span>][j <span class="op">-</span> <span class="dv">1</span>])
                direction[i][j] <span class="op">=</span> <span class="st">&quot;left&quot;</span> <span class="cf">if</span> c[i, j] <span class="op">==</span> c[i][j <span class="op">-</span> <span class="dv">1</span>] <span class="cf">else</span> <span class="st">&quot;up&quot;</span>
    
    <span class="co"># retrieve the LCS</span>
    i, j <span class="op">=</span> <span class="bu">len</span>(X), <span class="bu">len</span>(Y)
    sequence <span class="op">=</span> []
    <span class="cf">while</span> i <span class="op">&gt;</span> <span class="dv">0</span> <span class="kw">and</span> j <span class="op">&gt;</span> <span class="dv">0</span>:
        <span class="cf">if</span> direction[i][j] <span class="op">==</span> <span class="st">&quot;up_left&quot;</span>:
            sequence <span class="op">=</span> X[i] <span class="op">+</span> sequence <span class="co"># we could also use `Y[j]` rather than `X[i]` since they are identical here</span>
            i <span class="op">-=</span> <span class="dv">1</span><span class="op">;</span> j <span class="op">-=</span> <span class="dv">1</span>
        <span class="cf">elif</span> direction[i, j] <span class="op">==</span> <span class="st">&quot;up&quot;</span>:
            i <span class="op">-=</span> <span class="dv">1</span>
        <span class="cf">else</span>:
            j <span class="op">-=</span> <span class="dv">1</span>
    <span class="cf">return</span> sequence</code></pre></div>
<p>Clearly, this algorithm takes <span class="math inline">\Theta(mn)</span> time and memory, since computing each table entry takes constant time and there are <span class="math inline">(m + 1)(n + 1)</span> table entries.</p>
<h3 id="minimum-length-triangulation">Minimum Length Triangulation</h3>
<p>Suppose we have a convex polygon denoted by point <span class="math inline">q_1, \ldots, q_n</span>. How can we triangulate it such that the sum of the perimeters of the <span class="math inline">n - 2</span> triangles are minimized?</p>
<p>In other words, what is the trangulation such that the sum of the lengths of all the chords (lines within the polygon that divide it into triangles) is minimised? These two formulations are the same because the sum of lengths of all the chords times 2 plus the perimeter of the polygon is the sum of the perimeters of all the triangles.</p>
<p>As it turns out, a polygon with <span class="math inline">n</span> vertices has <span class="math inline">C_n</span> possible triangulations, where <span class="math inline">C_n</span> is the <span class="math inline">n</span>th Catalan number (<span class="math inline">C_n = \frac 1 {n + 1} {{2n} \choose n} = \Theta\left(\frac{4^n}{n^{\frac 3 2}}\right)</span>). This is exponential, so we want to avoid computing all possible triangulations.</p>
<p>Clearly, <span class="math inline">q_n q_1</span> is in a triangle <span class="math inline">q_n q_k q_1</span>, where <span class="math inline">2 \le k \le n - 1</span>. Clearly, for each <span class="math inline">k</span>, we have the triangle <span class="math inline">q_n q_k q_1</span>, which divides the polygon into a polygon with vertices <span class="math inline">q_1, \ldots, q_k</span>, and a polygon with vertices <span class="math inline">q_k, \ldots, q_n</span>.</p>
<p>Clearly, the smallest triangulation of the polygon that includes the triangle <span class="math inline">q_n q_k q_n</span> is this triangle along with the triangles in the smallest triangulation of <span class="math inline">q_1, \ldots, q_k</span> and the smallest triangulation of <span class="math inline">q_k, \ldots, q_n</span>. If we check all possible <span class="math inline">k</span>, then the smallest of all of these is the smallest triangulation of the polygon <span class="math inline">q_1, \ldots, q_n</span>. These smaller polygons are our subproblems. Since all of them are polygons of the form <span class="math inline">q_1, \ldots, q_i, \ldots, q_j, \ldots, q_n, 1 \le i &lt; j \le n</span>, we have <span class="math inline">\Theta(n^2)</span> subproblems in total.</p>
<p>Let <span class="math inline">S[i, j]</span> denote the smallest triangulation of the polygon <span class="math inline">q_i, \ldots, q_j</span>. Let <span class="math inline">P(q_i, q_k, q_j)</span> be the perimeter of the triangle formed by <span class="math inline">q_i, q_k, q_j</span>. Then <span class="math inline">S[i, j] = \min\set{P(q_i, q_k, q_j) + S[i, k] + S[k, j] \middle| k \in \mb{Z}, i &lt; k &lt; j}</span>.</p>
<h1 id="section-13">2/11/15</h1>
<h2 id="graph-algorithms">Graph Algorithms</h2>
<p>A graph is a pair <span class="math inline">\tup{V, E}</span>, where <span class="math inline">V</span> is the set of vertices <span class="math inline">\set{v_1, \ldots, v_n}</span> and <span class="math inline">E</span> is the set of edges <span class="math inline">\set{\set{u_1, v_1}, \ldots, \set{u_m, v_m}}</span>, where <span class="math inline">u, v</span> are edges. A digraph (directed graph) is also a pair <span class="math inline">\tup{V, E}</span>, but <span class="math inline">E</span> is a set of edges <span class="math inline">\set{\tup{u_1, v_1}, \ldots, \tup{u_m, v_m}}</span> - they have arcs (directed edges) that represent direction. In directed edges, <span class="math inline">u</span> is the <strong>tail</strong> and <span class="math inline">v</span> is the <strong>head</strong>. We can also write <span class="math inline">\tup{u, v}</span> and <span class="math inline">\set{u, v}</span> as <span class="math inline">uv</span>, which is easier to write and covers both directed and undirected edges.</p>
<p>In this course, we will not allow duplicate edges, so <span class="math inline">m \le n^2</span>. We will also not allow loops, so vertices will never have edges connecting them to themselves.</p>
<p>On the computer, we often represent graphs with adjecency matrices and adjacency lists.</p>
<p>An <strong>adjacency matrix</strong> is an <span class="math inline">n</span> by <span class="math inline">n</span> matrix where each row represents a vertex and each column represents a vertex. The elements of this matrix are 1 if there is an edge from the vertex for its row to the vertex for its column (<span class="math inline">A_{u, v} = \begin{bmatrix} a_{1, 1} &amp; \ldots &amp; a_{1, u} \\ \vdots &amp; \vdots &amp; \vdots \\ a_{v, 1} &amp; \ldots &amp; a_{v, u} \end{bmatrix}</span>). Formally, the adjacency matrix is <span class="math inline">A = \begin{bmatrix} a_{1, 1} &amp; \ldots &amp; a_{1, n} \\ \vdots &amp; \vdots &amp; a_\vdots \\ a_{n, 1} &amp; \ldots &amp; a_{n, n} \end{bmatrix}</span>.</p>
<p>Since loops (edges that have the same vertex on both ends) aren't allowed, the diagonals <span class="math inline">a_{1, 1}, \ldots, a_{n, n}</span> are all 0. Note that for an undirected graph, the adjacency matrix has <span class="math inline">2\abs{E(G)}</span> 1 entries, while for a directed graph, the adjacency matrix has <span class="math inline">\abs{E(G)}</span> 1 entries.</p>
<p>An <strong>adjacency list</strong> is an array of <span class="math inline">n</span> linked lists, represented as <span class="math inline">\adj(_1), \ldots, \adj(v_n)</span>, where <span class="math inline">\adj(v_i)</span> is the linked list of the vertices that <span class="math inline">v_i</span> is directly connected to. This is basically the same as the adjacency mtrix, but as an array of linked lists rather than a matrix. This is more memory efficient for sparse graphs, but make some operations more difficult since we can't test for edges in <span class="math inline">O(1)</span> time.</p>
<p>We usually use breath-first search for undirected graphs, and depth-first search for directed graphs.</p>
<p>A <strong>clique</strong> is a subset of vertices that is fully connected. A <strong>vertex cover</strong> is a subset of vertices such that all edges have at least one end in that set.</p>
<p>To do a <strong>breadth-first search</strong> (BFS), we pick a vertex <span class="math inline">s</span>, then spread out from <span class="math inline">s</span> by keeping track of the vertices we've explored so far, and exploring the unexplored neighbors of the explored vertices. Basically, the search spreads out from <span class="math inline">s</span> in layers. Starting from <span class="math inline">s</span>, we explore its neighbors, then the neighbors of those, then the neighbors of those, and so on. The full form of breadth-first search appears as follows:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">from</span> collections <span class="im">import</span> deque
<span class="kw">def</span> breadth_first_search(vertices, adjacency_list, start_vertex):
    colors, predecessors <span class="op">=</span> {}, {}
    <span class="cf">for</span> vertex <span class="kw">in</span> vertices:
        colors[vertex] <span class="op">=</span> <span class="st">&quot;white&quot;</span>
        predecessors[v] <span class="op">=</span> <span class="bu">set</span>()
    colors[start_vertex] <span class="op">=</span> <span class="st">&quot;gray&quot;</span>
    q <span class="op">=</span> deque([start_vertex])
    <span class="cf">while</span> q:
        vertex <span class="op">=</span> q.popleft()
        <span class="cf">for</span> neighbor <span class="kw">in</span> adjacency_list[vertex]:
            <span class="cf">if</span> colors[neighbor] <span class="op">==</span> <span class="st">&quot;white&quot;</span>:
                colors[neighbors] <span class="op">=</span> <span class="st">&quot;gray&quot;</span>
                predecessors[neighbor] <span class="op">=</span> vertex
                q.append(neighbor)
            colors[vertex] <span class="op">=</span> <span class="st">&quot;black&quot;</span></code></pre></div>
<p>The colors represent the visit state of vertices - white means a vertex is undiscovered, gray means it's discovered but its neighbors are still being processed, and black means it's discovered and fully processed. The <code>predecessors</code> dict maps all vertices (except the start vertex) to the the vertices they were visited from (their <strong>predecessors</strong>), and is formally represented <span class="math inline">\pi(v_i)</span>.</p>
<p>In breadth-first search, an edge <span class="math inline">\set{\pi(v), v}</span> is a <strong>tree edge</strong>. All other edges are <strong>cross edges</strong>. The set of all tree edges forms a <strong>BFS forest</strong> (there is a BFS tree for each graph component), and if the graph is connected, the BFS tree is a spanning tree.</p>
<p>A simpler form of the algorithm might appear as follows:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">from</span> collections <span class="im">import</span> deque
<span class="kw">def</span> breadth_first_search(vertices, adjacency_list):
    start_vertex <span class="op">=</span> vertices[<span class="dv">0</span>]
    visited <span class="op">=</span> <span class="bu">set</span>()
    q <span class="op">=</span> deque([start_vertex])
    <span class="cf">while</span> q:
        vertex <span class="op">=</span> q.popleft()
        visited.add(vertex)
        <span class="cf">for</span> neighbor <span class="kw">in</span> adjacency_list[vertex]:
            <span class="cf">if</span> neighbor <span class="kw">not</span> <span class="kw">in</span> visited:
                q.append(neighbor)</code></pre></div>
<p>Note that breadth first search, since it visits every vertex and could potentially visit every edge, takes <span class="math inline">\Theta(\abs{V} + \abs{E})</span> worst case time.</p>
<p>Shortest path from a starting vertex:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">from</span> collections <span class="im">import</span> deque
<span class="kw">def</span> shortest_path(vertices, adjacency_list, start_vertex):
    start_vertex <span class="op">=</span> vertices[<span class="dv">0</span>]
    visited <span class="op">=</span> <span class="bu">set</span>()
    q <span class="op">=</span> deque([start_vertex])
    distance_to_start <span class="op">=</span> {start_vertex: <span class="dv">0</span>}
    <span class="cf">while</span> q:
        vertex <span class="op">=</span> q.popleft()
        visited.add(vertex)
        <span class="cf">for</span> neighbor <span class="kw">in</span> adjacency_list[vertex]:
            <span class="cf">if</span> neighbor <span class="kw">not</span> <span class="kw">in</span> visited:
                q.append(neighbor)
                distance_to_start[neighbor] <span class="op">=</span> distance_to_start[vertex] <span class="op">+</span> <span class="dv">1</span></code></pre></div>
<p>Proof of correctness:</p>
<blockquote>
<p>Let <span class="math inline">\mathrm{dist}(v)</span> represent <code>distance_to_start[v]</code> where <span class="math inline">v</span> is a vertex, and <span class="math inline">s</span> represent <code>start_vertex</code>.<br />
We want to prove that for any vertex <span class="math inline">v</span>, <span class="math inline">\mathrm{dist}(v)</span> is the length of the shortest path from <span class="math inline">s</span> to <span class="math inline">v</span>.<br />
Define layers as <span class="math inline">L_i = \set{v \middle| \mathrm{dist}(v) = i}</span>.<br />
Clearly, <span class="math inline">v \in L_i</span> if and only if <span class="math inline">uv</span> is an edge such that <span class="math inline">u \in L_{i - 1}</span> and there is no <span class="math inline">u&#39; \in L_j, j &lt; i - 1</span> - it is adjacent to a vertex in a previous layer and is not adjacent to any vertices in lower layers.<br />
Clearly, if <span class="math inline">u</span> is discovered before <span class="math inline">v</span>, then <span class="math inline">\mathrm{dist}(u) \le \mathrm{dist}(v)</span>.<br />
Clearly, if <span class="math inline">uv</span> is an edge, then <span class="math inline">\abs{\mathrm{dist}(u) - \mathrm{dist}(v)} \le 1</span>.<br />
Clearly, if <span class="math inline">uv</span> is a tree edge, then <span class="math inline">\mathrm{dist}(v) = \mathrm{dist}(u) + 1</span>.<br />
Let <span class="math inline">\delta(v)</span> be the length of the shortest path from <span class="math inline">s</span> to <span class="math inline">v</span>.<br />
Clearly, <span class="math inline">\delta(v) \le \mathrm{dist}(v)</span> because <span class="math inline">\mathrm{dist}(v)</span> is the length of the sequence <span class="math inline">v, \pi(v), \pi(\pi(v)), \ldots, s</span>.<br />
We can easily use induction over the shortest path <span class="math inline">s, v_{i_1}, \ldots, v_{i_k}, v</span> to prove that <span class="math inline">\delta(v) \le \mathrm{dist}(v)</span>.<br />
Therefore, <span class="math inline">\delta(v) = \mathrm{dist}(v)</span>, and <span class="math inline">\mathrm{dist}(v)</span> is the length of the shortest path.</p>
</blockquote>
<p>For most graph algorithms, the steps will be quite intuitive, but the proofs of correctness will be rather difficult.</p>
<h1 id="section-14">4/11/15</h1>
<p>Determine whether a graph is bipartite:</p>
<blockquote>
<p>Clearly, a graph is bipartite if and only if it contains an odd cycle (see MATH239 notes for proof of this.<br />
We can check for odd cycles using breadth first search.<br />
At each step, if we encounter an edge <span class="math inline">\set{u, v}</span> where <span class="math inline">\mathrm{dist}(u) = \mathrm{dist}(v)</span>, then we have found an odd cycle containing <span class="math inline">u</span> and <span class="math inline">v</span> and the first common ancestor of <span class="math inline">u</span> and <span class="math inline">v</span> in the BFS tree.<br />
This is because the distance from <span class="math inline">u</span> and <span class="math inline">v</span> to the common ancester must be equal, so if the distance is <span class="math inline">k</span> then the cycle must have <span class="math inline">2k + 1</span> edges (<span class="math inline">k</span> for <span class="math inline">u</span> to the ancestor, <span class="math inline">k</span> for <span class="math inline">v</span> to the ancestor, and 1 for <span class="math inline">u</span> to <span class="math inline">v</span>), which forms an odd cycle.<br />
Otherwise, if we go through the entire graph without finding an odd cycle, there are no odd cycles. If this is the case, then we can partition the vertices graph into two partitions <span class="math inline">\set{u \in V(G) \middle| \mathrm{dist}(u) \text{ is even}}</span> and <span class="math inline">\set{u \in V(G) \middle| \mathrm{dist}(u) \text{ is odd}}</span>.</p>
</blockquote>
<p><strong>Depth-first search</strong> (DFS) is done in a way very similar to breadth-first search, but with a different visiting mechanism and using a stack rather than a queue.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> depth_first_search(adjacency_list):
    colors, predecessors <span class="op">=</span> {}, {}
    discovery_time, finish_time <span class="op">=</span> {}, {}
    <span class="cf">for</span> vertex <span class="kw">in</span> adjacency_list:
        colors[vertex] <span class="op">=</span> <span class="st">&quot;white&quot;</span>
        predecessors[vertex] <span class="op">=</span> <span class="va">None</span>
    current_time <span class="op">=</span> [<span class="dv">0</span>]
    <span class="cf">for</span> vertex <span class="kw">in</span> adjacency_list:
        <span class="cf">if</span> colors[vertex] <span class="op">==</span> <span class="st">&quot;white&quot;</span>:
            visit(adjacency_list, vertex, colors, predecessors, current_time, discovery_time, finish_time)

<span class="kw">def</span> visit(adjacency_list, vertex, colors, predecessors, current_time, discovery_time, finish_time):
    colors[vertex] <span class="op">=</span> <span class="st">&quot;gray&quot;</span>
    current_time[<span class="dv">0</span>] <span class="op">+=</span> <span class="dv">1</span>
    discovery_time[vertex] <span class="op">=</span> time
    <span class="cf">for</span> neighbor <span class="kw">in</span> adjacency_list[vertex]:
        <span class="cf">if</span> colors[neighbor] <span class="op">=</span> <span class="st">&quot;white&quot;</span>:
            predecessors[neighbor] <span class="op">=</span> vertex
            visit(adjacency_list, neighbor, colors, predecessors, current_time, discovery_time, finish_time)
    colors[vertex] <span class="op">=</span> <span class="st">&quot;black&quot;</span>
    current_time[<span class="dv">0</span>] <span class="op">+=</span> <span class="dv">1</span>
    finish_time[vertex] <span class="op">=</span> time</code></pre></div>
<p>Note that we keep track of the discovery and finish times for each vertex. This information can be useful in some situations. A simplified version of DFS can be written as follows:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> depth_first_search(vertices, adjacency_list):
    visited <span class="op">=</span> <span class="bu">set</span>()
    <span class="cf">for</span> vertex <span class="kw">in</span> vertices:
        <span class="cf">if</span> vertex <span class="kw">not</span> <span class="kw">in</span> visited:
            depth_first_search_visit(vertices, adjacency_list, vertex, visited)

<span class="kw">def</span> depth_first_search_visit(vertices, adjacency_list, vertex, visited):
    visited.add(vertex)
    <span class="cf">for</span> neighbor <span class="kw">in</span> adjacency_list[vertex]:
        <span class="cf">if</span> neighbor <span class="kw">not</span> <span class="kw">in</span> visited:
            depth_first_search_visit(vertices, adjacency_list, neighbor, visited)</code></pre></div>
<p>In depth-first search, an edge <span class="math inline">\tup{\pi(v), v}</span> is a <strong>tree edge</strong>. The set of all tree edges forms a <strong>DFS forest</strong> (there is a DFS tree for each graph component). If the graph is connected, the DFS forest is a spanning tree. For a tree edge, we discover and finish with <span class="math inline">v</span> during <span class="math inline">u</span>'s processing - <span class="math inline">v</span>'s discovery and finish time interval is nested within <span class="math inline">u</span>'s discovery and finish time interval. Additionally, <span class="math inline">v</span> will be white when the edge is dicovered.</p>
<p>An edge <span class="math inline">\tup{u, v}</span> is a <strong>forward edge</strong> if and only if it is not a tree edge, and <span class="math inline">v</span> is a descendant of <span class="math inline">u</span> in the depth-first forest - an edge that connects a vertex with a descendant that is not a direct child. Like for tree edges, <span class="math inline">v</span>'s time interval is nested in <span class="math inline">v</span>'s, but <span class="math inline">v</span> will be black when we the edge is discovered.</p>
<p>A <strong>back edge</strong> is the same thing as a forward edge but in the other direction. However, <span class="math inline">\tup{u, v}</span> can't possibly be a tree edge since <span class="math inline">u</span> is a descendant of <span class="math inline">v</span> in the DFS tree, so we can simply say a back edge is one that connects a vertex to an ancestor. For a back edge, we discover and finish with <span class="math inline">u</span> during <span class="math inline">v</span>'s processing - <span class="math inline">u</span>'s discovery and finish time interval is nested within <span class="math inline">v</span>'s. Also, <span class="math inline">v</span> will always be gray.</p>
<p>All other edges are <strong>cross edges</strong> - generally this happens when edges connect siblings in the tree. For example, in the graph <span class="math inline">\adj(u) = \set{v, w}, \adj(v) = \set{w}, \adj(w) = \set{v}</span>, either <span class="math inline">vw</span> is a cross edge, or <span class="math inline">wv</span> is, depending on the order we visit the vertices in. For a cross edge, the discovery and finish time intervals for <span class="math inline">u</span> and <span class="math inline">v</span> will be disjoint - they will never overlap. Also, <span class="math inline">v</span> will always be black.</p>
<p>The <strong>parenthesis theorem</strong> says that the discovery and finish time intervals are either disjoint or nested.</p>
<p>BFS and DFS might not visit all the vertices of a graph if it is not connected - if it has multiple components. To have it visit all nodes, we can keep track of unvisited vertices, and while there are still unvisited nodes, we can repeatedly run the search algorithm, stopping when all vertices have been visited.</p>
<h1 id="section-15">9/11/15</h1>
<p>A <strong>directed acyclic graph</strong> (DAG) is a directed graph with no cycles. A <strong>topological ordering/topological sort</strong> of a directed graph <span class="math inline">G</span> is an ordering <span class="math inline">&lt;</span> of all vertices in <span class="math inline">G</span> such that <span class="math inline">\set{u, v} \in E(G) \implies u &lt; v</span>. A directed graph has a topological ordering if and only if it is a DAG - if it has no cycles.</p>
<p>A directed graph <span class="math inline">G</span> is a DAG if and only if it has no back edges when we do a depth-first search on it. This is because if there is a back edge, that back edge and the tree edges connecting its ends form a directed cycle, and if it has a cycle, then one of its edges must be a back edge.</p>
<p>As it turns out, the negative finishing time forms a topological ordering - for all <span class="math inline">\set{u, v} \in E(G)</span>, <code>finish_time[u] &gt; finish_time[v]</code>, so <code>-finish_time[u] &lt; -finish_time[v]</code>. In other words, <strong>ordering vertices by finishing time descending results in a topological ordering</strong>.</p>
<p>The <strong>indegree</strong> of a vertex in a directed graph is the number of edges going into it. The <strong>outdegree</strong> of a vertex in a directed graph is the number of edges leaving it.</p>
<p>Prove that a DAG must have a vertex of indegree 0:</p>
<blockquote>
<p>We will prove the contrapositive - that a graph with no vertices of indegree 0 must have a cycle.<br />
If there are any edges starting and ending at the same vertex, then there is a cycle. Assume that there aren't any.<br />
Clearly, there is an arc from <span class="math inline">v_1</span> to <span class="math inline">v_2</span>, and <span class="math inline">v_2</span>t o <span class="math inline">v_3</span>, and so on - every vertex must have at least one edge going into it from another vertex.<br />
At some point, we must encounter a <span class="math inline">v_j = v_i</span> where <span class="math inline">j &gt; i</span> - we run out of vertices to come from.<br />
Therefore, there must also be a cycle with <span class="math inline">v_i</span> and <span class="math inline">v_j</span>. So if there are no cycles, there must be a vertex of indegree 0.</p>
</blockquote>
<p>Prove that a directed graph <span class="math inline">G</span> has a topological ordering if and only if it is a DAG:</p>
<blockquote>
<p>Clearly, if <span class="math inline">G</span> has a directed cycle <span class="math inline">v_1, \ldots, v_k, v_1, \ldots</span>, then that cycle cannot have a topological ordering since <span class="math inline">v_k &gt; 1</span> and <span class="math inline">v_1 &gt; v_k</span>.<br />
So if <span class="math inline">G</span> has a topological ordering, then it must be a DAG.<br />
Suppose <span class="math inline">G</span> is a DAG. Then <span class="math inline">G</span> has a vertex <span class="math inline">v_1</span> of indegree 0, and it can only have outward edges.<br />
Construct a topological ordering where <span class="math inline">v_1</span> is the lowest value, and then delete <span class="math inline">v_1</span> and all incident edges to make a new graph <span class="math inline">G&#39;</span>.<br />
Clearly, <span class="math inline">G&#39;</span> is still a DAG, since it is just <span class="math inline">G</span> with a vertex deleted. So it also has a vertex <span class="math inline">v_2</span> of indegree 0.<br />
Let this be the second lowest value in the topological ordering, then delete <span class="math inline">v_2</span> and all incident edges and repeat the steps for the new graph until we have processed all vertices.<br />
Clearly, once we have processed all vertices we have a topological ordering. Therefore, if <span class="math inline">G</span> is a DAG then we have a topological ordering.<br />
Therefore, <span class="math inline">G</span> is a DAG if and only if it has a topological ordering.<br />
Note that this proof gives an algorithm for constructing a topological ordering, but this is rather inefficient. We can use depth-first search to do this more efficiently.</p>
</blockquote>
<p>Making a topological ordering is possible in <span class="math inline">\Theta(\abs{V} + \abs{E})</span>:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> topological_ordering(vertices, adjacency_list):
    visited <span class="op">=</span> {}
    topological_ordering <span class="op">=</span> []
    <span class="cf">for</span> vertex <span class="kw">in</span> vertices: visited[vertex] <span class="op">=</span> <span class="va">False</span>
    <span class="cf">for</span> vertex <span class="kw">in</span> vertices:
        <span class="cf">if</span> <span class="kw">not</span> visited[vertex]:
            topological_ordering_visit(vertices, adjacency_list, vertex, colors, predecessors, current_time, discovery_time, finish_time)
    <span class="cf">return</span> <span class="bu">reversed</span>(topological_ordering)

<span class="kw">def</span> topological_ordering_visit(vertices, adjacency_list, vertex, colors, visited, topological_ordering):
    visited[vertex] <span class="op">=</span> <span class="va">True</span>
    <span class="cf">for</span> neighbor <span class="kw">in</span> adjacency_list[vertex]:
        <span class="cf">if</span> <span class="kw">not</span> visited[neighbor]:
            topological_ordering_visit(vertices, adjacency_list, neighbor, visited, topological_ordering)
    topological_ordering.append(vertex)</code></pre></div>
<h1 id="section-16">11/11/15</h1>
<p>For two vertices <span class="math inline">x, y</span> in a directed graph <span class="math inline">G</span>, let <span class="math inline">x \sim y</span> if and only if <span class="math inline">x = y</span> or <span class="math inline">x \ne y</span> and there exist directed paths from <span class="math inline">x</span> to <span class="math inline">y</span> and from <span class="math inline">y</span> to <span class="math inline">x</span>. Note that <span class="math inline">\sim</span> as defined here is an equivalence relation - it is a relation that is reflexive (<span class="math inline">a \sim a</span>), symmetric (<span class="math inline">a \sim b \implies b \sim a</span>), and transitive (<span class="math inline">a \sim b \wedge b \sim c \implies a \sim c</span>)- An equivalence relation partitions <span class="math inline">V(G)</span> into sets of vertices such that <span class="math inline">u \sim v</span> between any two <span class="math inline">u, v</span> in the same partition.</p>
<p>A directed graph is <strong>strongly connected</strong> if and only if for any two vertices <span class="math inline">u, v</span> there is a directed path from <span class="math inline">u</span> to <span class="math inline">v</span> and <span class="math inline">v</span> to <span class="math inline">u</span> - <span class="math inline">u \sim v</span>. A <strong>strongly connected component</strong> is a maximal strongly connected subgraph.</p>
<p>The <strong>component graph</strong> of a directed graph is a directed graph that has vertices corresponding to each component in the graph, and vertices corresponding to whether any two components have an edge between them. Define the discovery time and finishing time of a component to be the earliest discovery and latest finish time of any vertex in that component.</p>
<p>The component graph must be a DAG. This is because if there is a cycle between strongly connected components, then there must exist a path from each component to each other component, which means each of those components must not be maximal. Instead, the whole thing is all one component.</p>
<p>For two components <span class="math inline">C_1, C_2</span> in a component graph, if there is an edge from <span class="math inline">C_1</span> to <span class="math inline">C_2</span>, then <span class="math inline">C_1</span> has a later finishing time than <span class="math inline">C_2</span> (note that there can't be any cycles, so if this is the case there can't be an edges from <span class="math inline">C_2</span> to <span class="math inline">C_1</span> as well). This is because a depth-first search must go through all of <span class="math inline">C_2</span> before going to <span class="math inline">C_1</span> if it encounters <span class="math inline">C_2</span> first, and must go through all of <span class="math inline">C_2</span> before being able to finish going through <span class="math inline">C_1</span> if it encounters <span class="math inline">C_1</span> first.</p>
<p>A graph is strongly connected if and only if for any fixed vertex <span class="math inline">s</span>, there is a directed path from <span class="math inline">s</span> to any vertex <span class="math inline">t</span> and a directed path from <span class="math inline">t</span> to <span class="math inline">s</span>. Knowing this, it is simple to create an algorithm to test if a graph <span class="math inline">G</span> is strongly connected:</p>
<ol type="1">
<li>We need to test if we can reach all vertices from <span class="math inline">s</span>.
<ol type="1">
<li>Pick any vertex <span class="math inline">s</span> and call <code>visit(s)</code> (from DFS) once on <span class="math inline">G</span>.</li>
<li>If any of the vertices are not black, then there are vertices that we can't reach from <span class="math inline">s</span> at all, so <span class="math inline">G</span> is not a strongly connected component.</li>
</ol></li>
<li>We need to test if we can reach <span class="math inline">s</span> from all vertices.
<ol type="1">
<li>Construct a new graph <span class="math inline">H</span> that is the same as <span class="math inline">G</span> but where all the edges are reversed.</li>
<li>Pick any vertex <span class="math inline">s</span> and call <code>visit(s)</code> (from DFS) once on <span class="math inline">H</span>.</li>
<li>If any of the vertices are not black, then there are vertices that can't reach <span class="math inline">s</span> at all, so <span class="math inline">G</span> is not a strongly connected component.</li>
</ol></li>
<li>If both tests pass, then <span class="math inline">G</span> is a strongly connected component.</li>
</ol>
<p>To find the strongly connected components in <span class="math inline">G</span>:</p>
<ol type="1">
<li>Do a depth-first search on <span class="math inline">G</span>, storing the finishing times.</li>
<li>Construct a new graph <span class="math inline">H</span> with all the edges reversed.</li>
<li>Do a depth-first search on <span class="math inline">H</span>, each time calling <code>visit</code> starting with the unvisited vertex with the latest finishing time in the DFS from step 1.</li>
<li>The strongly connected components of <span class="math inline">G</span> are the trees in the DFS forest from the DFS in step 3.</li>
</ol>
<p>Proof of correctness:</p>
<blockquote>
<p>Note that <span class="math inline">G</span> and <span class="math inline">H</span> have the same strongly connected components.<br />
Let <span class="math inline">u</span> be the first vertex visited in step 3 - the one with the highest finishing time from the DFS in step 1.<br />
Let <span class="math inline">C</span> be the strongly connected component that contains <span class="math inline">u</span>. Let <span class="math inline">C&#39;</span> be any other strongly connected component.<br />
Clearly, the finishing time of <span class="math inline">C</span> is later than that of <span class="math inline">C&#39;</span>.<br />
So there cannot be an arc from <span class="math inline">C&#39;</span> to <span class="math inline">C</span> in <span class="math inline">G</span> (it is <span class="math inline">G</span> rather than <span class="math inline">h</span> because the finishing times were for a DFS on <span class="math inline">G</span>).<br />
So we can only reach the vertices in <span class="math inline">C</span> from <span class="math inline">u</span>, not from <span class="math inline">C&#39;</span>.</p>
</blockquote>
<p>Clearly, this has time complexity <span class="math inline">\Theta(\abs{V} + \abs{E})</span>, since both tests from step 1 and 2 take <span class="math inline">\Theta(\abs{V} + \abs{E})</span>.</p>
<h1 id="section-17">16/11/15</h1>
<p>The spanning tree of a connected undirected graph <span class="math inline">G</span> is a subgraph <span class="math inline">T</span> that is a tree containing every vertex in <span class="math inline">G</span>. <span class="math inline">T</span> is a spanning tree if and only if <span class="math inline">T</span> is an acyclic subgraph that has <span class="math inline">\abs{V(G)} - 1</span> edges.</p>
<p>Given a weight function <span class="math inline">w: V(G) \to \mb{R}</span> on a graph <span class="math inline">G</span>, we want to find a spanning tree of <span class="math inline">G</span> such that <span class="math inline">\sum_{e \in E(T)} w(e)</span> is minimized - the <strong>minimum spanning tree problem</strong>. In other words, we want a connected subgraph of <span class="math inline">G</span> such that the sum of the weights is minimised.</p>
<h3 id="kruskals-algorithm">Kruskal's Algorithm</h3>
<p>Kruskal's algorithm is one way to solve this. Basically, we sort the edges by weight ascending and considering each edge in order, we add the edge to the result if it doesn't form a cycle in the result.</p>
<p>At any point, we have a forest of disjoint trees (no trees share any vertices). We start off with <span class="math inline">n</span> trees, and whenever we add an edge to the forest, we merge two trees, reducing the number of trees in the forest by 1.</p>
<p>We can efficiently test if adding an edge would result in a cycle by keeping track of the parents of each vertex in its tree as <span class="math inline">L[v]</span>. The root of the tree is therefore the vertex <span class="math inline">v</span> such that <span class="math inline">L[v] = v</span>. By repeatedly getting the parent/leader of the vertex, checking <span class="math inline">v, L[v], L[L[v]], \ldots</span>, we eventually get the root of the tree. When we add an edge to the resulting tree, we simply add an entry <span class="math inline">L[v] = u</span> or <span class="math inline">L[u] = v</span>.</p>
<p>We now define <span class="math inline">\operatorname{find}(v)</span> to be the root of the tree containing the vertex <span class="math inline">v</span>, and implement it by repeatedly checking <span class="math inline">L</span>. Testing if an edge causes a cycle is the same as checking if the edge <span class="math inline">\set{u, v}</span> has <span class="math inline">u, v</span> in two different trees - if <span class="math inline">\operatorname{find}(u) = \operatorname{find}(v)</span>. So we add an edge <span class="math inline">\set{u, v}</span> if and only if <span class="math inline">\operatorname{find}(u) \ne \operatorname{find}(v)</span>.</p>
<p>As it turns out, if we ensure that the depth of the tree containing <span class="math inline">u</span> is less than the depth of the tree containing <span class="math inline">v</span> when adding the entry <span class="math inline">L[v] = u</span>, we can ensure that <span class="math inline">\operatorname{find}</span> is always <span class="math inline">\Theta(\log(\abs{E}))</span>.</p>
<p><span class="math inline">L</span> is a type of data structure called a <strong>union-find/merge-find</strong> data structure, because it has a find operation and a union/merge (setting the leader/parent of a vertex) operation.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> spanning_tree(vertices, edges, weights):
    leaders <span class="op">=</span> <span class="bu">list</span>(<span class="bu">range</span>(<span class="bu">len</span>(vertices)))
    tree_edges <span class="op">=</span> <span class="bu">set</span>()
    <span class="cf">for</span> edge <span class="kw">in</span> <span class="bu">sorted</span>(edges, key<span class="op">=</span><span class="kw">lambda</span> edge: weights[edge]):
        vertex1_root, vertex1_depth <span class="op">=</span> find(edge[<span class="dv">0</span>], leaders)
        vertex2_root, vertex2_depth <span class="op">=</span> find(edge[<span class="dv">1</span>], leaders)
        <span class="cf">if</span> vertex1_root <span class="op">!=</span> vertex2_root:
            tree_edges.add(edge)
            <span class="cf">if</span> vertex1_depth <span class="op">&lt;</span> vertex2_depth:
                leaders[vertex2_root] <span class="op">=</span> vertex1_root
            <span class="cf">else</span>:
                leaders[vertex1_root] <span class="op">=</span> vertex2_root
    <span class="cf">return</span> (vertices, tree_edges)

<span class="kw">def</span> find(vertex, leaders):
    depth <span class="op">=</span> <span class="dv">0</span>
    <span class="cf">while</span> leaders[vertex] <span class="op">!=</span> vertex:
        vertex <span class="op">=</span> leaders[vertex]
        depth <span class="op">+=</span> <span class="dv">1</span>
    <span class="cf">return</span> vertex, depth</code></pre></div>
<p>Since sorting the edges by weight takes <span class="math inline">\Theta(\abs{E} \log \abs{E})</span> and we run the <span class="math inline">\Theta(\log \abs{E})</span> union and find operations.</p>
<h3 id="prims-algorithm">Prim's Algorithm</h3>
<p>Prim's algorithm is another way to find the minimum spanning tree of a graph.</p>
<p>Basically, we define a tree <span class="math inline">A</span>, and starting from an arbitrary vertex <span class="math inline">u_0</span>, we repeatedly add the lowest-weight edge that connects a vertex in <span class="math inline">A</span> to a vertex not in <span class="math inline">A</span> to <span class="math inline">A</span>.</p>
<p>Let <span class="math inline">V_A</span> be the set of vertices that are incident to edges in <span class="math inline">A</span>. Let <span class="math inline">N[v]</span> be a vertex <span class="math inline">u \in V_A</span> where <span class="math inline">\set{u, v}</span> is a minimum weight edge. Let <span class="math inline">W[v] = w(N[v], v)</span> - the best possible choice we can make at any vertex.</p>
<p>This is a pretty standard greedy algorithm.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> spanning_tree(vertices, edges, weights):
    tree_edges <span class="op">=</span> <span class="bu">set</span>()
    start_vertex <span class="op">=</span> vertices[<span class="dv">0</span>]
    tree_vertices <span class="op">=</span> {start_vertex}
    <span class="cf">for</span> vertex <span class="kw">in</span> vertices <span class="op">-</span> {start_vertex}:
        W[vertex] <span class="op">=</span> weights[{start_vertex, vertex}]
        N[vertex] <span class="op">=</span> start_vertex

    <span class="cf">while</span> <span class="bu">len</span>(tree_edges) <span class="op">&lt;</span> <span class="bu">len</span>(vertices) <span class="op">-</span> <span class="dv">1</span>:
        min_weight_vertex <span class="op">=</span> <span class="bu">min</span>(vertices <span class="op">-</span> tree_vertices, key<span class="op">=</span><span class="kw">lambda</span> vertex: W[vertex])
        tree_vertices.add(min_weight_vertex)
        edge_vertex <span class="op">=</span> N[min_weight_vertex]
        tree_edges.add({min_weight_vertex, edge_vertex})

        <span class="cf">for</span> vertex <span class="kw">in</span> vertices <span class="op">-</span> tree_vertices: <span class="co"># look at the vertices in the rest of the graph</span>
            <span class="cf">if</span> weights[min_weight_vertex, vertex] <span class="op">&lt;</span> W[vertex]: <span class="co"># new edge has a lower weight than the current one</span>
                W[vertex] <span class="op">=</span> weights[min_weight_vertex, vertex]
                N[vertex] <span class="op">=</span> min_weight_vertex
    <span class="cf">return</span> (vertices, tree_edges)</code></pre></div>
<p>Since we need to check edges <span class="math inline">\Theta(\abs{V})</span> times, and each time takes <span class="math inline">O(\abs{V})</span> time, this implementation of Prim's algorithm takes <span class="math inline">O(\abs{V}^2)</span> time. Alternatively, we could use the adjacency list representation and build a heap of all the vertices in <span class="math inline">W</span> to compute <code>min_weight_vertex</code> in <span class="math inline">O(\log(\abs{V}))</span> time, giving us <span class="math inline">O(\abs{E} \log \abs{V})</span>, which is better for sparse graphs (graphs with relatively few edges compared to vertices).</p>
<h1 id="section-18">18/11/15</h1>
<p>To prove the correctness of Kruskal's and Prim's algorithm, we will construct a general algorithm of which Kriskal's and Prim's algorithm are special cases of. Proving this algorithm correct will prove both Kruskal's and Prim's algorthms correct.</p>
<p>A <strong>cut</strong> is a paritition of <span class="math inline">V(G)</span> into sets <span class="math inline">S, V \setminus S</span> such that both are non-empty. A <strong>crossing edge</strong> with respect to a cut <span class="math inline">S, V \setminus S</span> is an edge that has one end in <span class="math inline">S</span> and another in <span class="math inline">V \setminus S</span>. A cut <span class="math inline">S, V \setminus S</span> <strong>respects</strong> a set of edges <span class="math inline">A \subseteq E(G)</span> if and only if none of the edges in <span class="math inline">A</span> are crossing edges - all edges in <span class="math inline">A</span> have vertices that are either both in <span class="math inline">S</span> or both in <span class="math inline">V \setminus S</span>.</p>
<p>The general algorithm is as follows:</p>
<pre><code>def greedy_minimum_spanning_tree(vertices, edges, weights):
    spanning_tree = set()
    while len(A) &lt; len(vertices) - 1:
        let `S` and `vertices - S` be a cut that respects `spanning_tree`
        let `e` be the minimum weight crossing edge
        spanning_tree.add(edge)
    return spanning_tree</code></pre>
<p>Kruskal's algorithm simply has the cut be between two trees being grown in the graph. Prim's algoorithm has the cut be between the tree being grown and the rest of the graph.</p>
<p>Proof of correctness:</p>
<blockquote>
<p>Let <span class="math inline">j = \abs{V} - 1</span>. Let <span class="math inline">A = \set{e_1, \ldots, e_j}</span> be the set of edges in the spanning tree where the edges are chosen in that order.<br />
We will prove that <span class="math inline">A</span> is a minimum spanning tree, via induction. For simplicity, we will assume that all of the edge weights are distinct.<br />
Clearly, if <span class="math inline">j = 0</span> then there is only 1 vertex and the spanning tree has no edges, so <span class="math inline">A</span> is a minimum spanning tree.<br />
Assume for some <span class="math inline">j</span> that <span class="math inline">\set{e_1, \ldots, e_{j - 1}}</span> is contained in a minimum spanning tree.<br />
So there is a cut <span class="math inline">S, V(G) \setminus S</span> such that <span class="math inline">\set{e_1, \ldots, e_{j - 1}}</span> respects the cut - <span class="math inline">S</span> and <span class="math inline">V \setminus S</span> form two trees that span all vertices.<br />
In the algorithm, <span class="math inline">e_j</span> is chosen to be the minimum weight crossing edge with respect to this cut.<br />
If <span class="math inline">e_j</span> is in a minimum spanning tree, then <span class="math inline">A</span> is a minimum spanning tree. Assume <span class="math inline">e_j</span> is not in the minimal spanning tree.<br />
Clearly, There is some other crossing edge joining the two trees <span class="math inline">e_j&#39;</span> that has lower weight than <span class="math inline">e_j</span>. Then the minimum spanning tree has a lower weight than <span class="math inline">A</span>, so <span class="math inline">w(e_j&#39;) &lt; w(e_j)</span>, a contradiction since <span class="math inline">e_j</span> is chosen to be the one with the lowest weight. So <span class="math inline">A</span> is a minimum spanning tree.<br />
So by induction, the spanning tree is minimal.</p>
</blockquote>
<h2 id="shortest-paths">Shortest Paths</h2>
<p>Given a directed graph <span class="math inline">G</span>, a weight function <span class="math inline">w: E(G) \to \mb{R}^+</span> (non-negative), and a source/start vertex <span class="math inline">u_0</span>, find the shortest directed path <span class="math inline">P</span> from <span class="math inline">u_0</span> to any vertex <span class="math inline">v</span> such that the total weights are minimised - the smallest possible value of <span class="math inline">\sum_{e \in P} w(e)</span>. This is called the <strong>single source shortest path problem</strong>.</p>
<p><strong>Dijkstra's algorithm</strong> (pronounced &quot;diig-stra&quot;) is one algorithm for this problem, structurally very similar to Prim's algorithm. ;wip: explain this better</p>
<p>Let <span class="math inline">S</span> be the subset of vertices for which the shortest path from <span class="math inline">u_0</span> to all vertices in <span class="math inline">S</span> are known. Let <span class="math inline">D[v]</span> where <span class="math inline">v \notin S</span> be the weight of the shortest path from <span class="math inline">u_0</span> to <span class="math inline">v</span> where all the interior vertices of the path (vertices excluding <span class="math inline">u_0</span> and <span class="math inline">v</span>) are in <span class="math inline">S</span>.</p>
<p>Let <span class="math inline">\pi[v]</span> be the predecessor of <span class="math inline">v</span> on a path (vertices in this algorithm will only be part of one path at a time).</p>
<p>Initially <span class="math inline">S = \set{u_0}</span> and <span class="math inline">D[v] = w(u_0, v)</span> for all <span class="math inline">v</span> (edges that don't exist have weight <span class="math inline">\infty</span>). At each step, we choose a <span class="math inline">v \in V(G) \setminus S</span> such that <span class="math inline">D[v]</span> is minimised, add <span class="math inline">v</span> to <span class="math inline">S</span>, then update <span class="math inline">D</span> and <span class="math inline">\pi</span> to match:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> compute_paths(vertices, edges, weights, start_vertex):
    visited <span class="op">=</span> {start_vertex}
    min_path_weights <span class="op">=</span> {start_vertex: <span class="dv">0</span>}
    predecessors <span class="op">=</span> {}
    <span class="cf">for</span> vertex <span class="kw">in</span> vertices <span class="op">-</span> {start_vertex}:
        min_path_weights[vertex] <span class="op">=</span> weights[{start_vertex, vertex}]
        predecessors[vertex] <span class="op">=</span> start_vertex

    <span class="cf">while</span> <span class="bu">len</span>(visited) <span class="op">&lt;</span> <span class="bu">len</span>(vertices):
        min_weight_vertex <span class="op">=</span> <span class="bu">min</span>(vertices <span class="op">-</span> visited, key<span class="op">=</span><span class="kw">lambda</span> vertex: min_path_weights[vertex]) <span class="co"># vertex in the rest of the graph with lowest current path weight</span>
        visited.add(min_weight_vertex)
        <span class="cf">for</span> vertex <span class="kw">in</span> vertices <span class="op">-</span> visited: <span class="co"># look at the vertices in the rest of the graph</span>
            <span class="cf">if</span> min_path_weights[min_weight_vertex] <span class="op">+</span> weights[{min_weight_vertex, vertex}] <span class="op">&lt;</span> min_weight_vertex[vertex]: <span class="co"># new vertex has lower path weight than the current one</span>
                min_path_weights[vertex] <span class="op">=</span> min_path_weights[min_weight_vertex] <span class="op">+</span> weights[{min_weight_vertex, vertex}]
                predecessors[vertex] <span class="op">=</span> min_weight_vertex
    
    <span class="cf">return</span> predecessors

<span class="kw">def</span> shortest_path(start_vertex, end_vertex, predecessors):
    path <span class="op">=</span> [start_vertex]
    vertex <span class="op">=</span> end_vertex
    <span class="cf">while</span> vertex <span class="op">!=</span> start_vertex:
        vertex <span class="op">=</span> predecessors[vertex]
        path.append(vertex)
    <span class="cf">return</span> <span class="bu">reversed</span>(path)</code></pre></div>
<p>Since this is structurally so similar to Prim's algorithm, it has the same time complexity, <span class="math inline">O(\abs{V(G)}^2)</span> for the above implementation or <span class="math inline">O(\abs{E(G)} \log \abs{V(G)})</span> for a heap-based version.</p>
<p>This works because <span class="math inline">D</span> has the weight of the shortest path from <span class="math inline">u_0</span> to any vertex <span class="math inline">v_k \in S</span>, and then we compute, step by step, the best paths from <span class="math inline">v_k</span> to <span class="math inline">v</span>. Basically, <span class="math inline">D</span> keeps track of all the possible candidate paths until we get to <span class="math inline">v</span>.</p>
<p>The Bellman-Ford algorithm can also solve the single-source path problem, but <strong>also supports negative weights</strong>, as long as there are no negative weight cycles (cycles whose edge weight sum is negative). This runs in <span class="math inline">O(\abs{E(G)} \abs{V(G)})</span>.</p>
<p>We might think that adding a constant offset to all the negative weights (to make them positive) would allow us to find the shortest paths using the algorithms we've seen so far. However, there exist graphs with negative weights such that if we add a constant offset, the shortest path changes. Consider a graph with 2 paths <span class="math inline">P_1, P_2</span> from <span class="math inline">u</span> to <span class="math inline">v</span>, where <span class="math inline">P_1</span> has 5 edges each with weight -1, and <span class="math inline">P_2</span> has 2 edges each with weight -2, so <span class="math inline">P_1</span> has a lower weight. If we add 2 to the weights of each edge to make all the weights non-negative, <span class="math inline">P_2</span> has a lower weight.</p>
<p>If there are <strong>no cycles in the graph</strong> (a DAG), there is an algorithm that can find the shortest path even if there are negative weight edges, and running more efficiently than the Bellman-Ford algorithm (see slide 160 for code).</p>
<p>The <strong>all-pairs shortest path problem</strong> is to find the shortest path between all pairs of vertices in the graph, allowing negative weight edges but not negative weight cycles. We can solve this by running the Bellman-Ford algorithm on each vertex, but the Floyd-Warshall algorithm can do it in <span class="math inline">O(n^3)</span>, using a dynamic-programming-like technique (see slide 164 for code). Like Bellman-ford, Floyd-Warshall also supports negative edge weights as long as there are no negative weight cycles.</p>
<p><span class="math inline">D_m[i, j]</span> denotes the weight of the shortest path from <span class="math inline">i</span> to <span class="math inline">j</span> such that the interior vertices of the path (vertices excluding <span class="math inline">i</span> and <span class="math inline">j</span>) are in <span class="math inline">\set{1, \ldots, m}</span>. At each step, we check our candidate path <span class="math inline">D_m[i, m] + D[m, j]</span> against the old best path <span class="math inline">D_m[i, j]</span>:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> shortest_path(vertices, edges, weights):
    min_weight_path <span class="op">=</span> [[[[<span class="bu">float</span>(<span class="st">&quot;inf&quot;</span>)] <span class="op">*</span> <span class="bu">len</span>(vertices)] <span class="cf">for</span> _ <span class="kw">in</span> vertices] <span class="cf">for</span> _ <span class="kw">in</span> vertices] <span class="co"># 3D array of size $n$ by $n$ by $n$:</span>
    <span class="cf">for</span> m <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(vertices)):
        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(vertices)):
            <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(vertices)):
                D[m][i][j] <span class="op">=</span> <span class="bu">min</span>(D[m <span class="op">-</span> <span class="dv">1</span>][i][j], D[m <span class="op">-</span> <span class="dv">1</span>][i][m] <span class="op">+</span> D[m <span class="op">-</span> <span class="dv">1</span>][m][j])</code></pre></div>
<h1 id="section-19">23/11/15</h1>
<h2 id="intractability-and-undecidability">Intractability and Undecidability</h2>
<p>The complexity classes we will look at are <span class="math inline">P</span> (polynomial), <span class="math inline">NP</span> (non-deterministic polynomial), and <span class="math inline">NP</span>-complete. These are sets of problems. <span class="math inline">NP</span> is a superset of <span class="math inline">P</span>, and <span class="math inline">NP</span>-complete is a subset of <span class="math inline">NP</span>.</p>
<p>A <strong>decision problem</strong> is a problem whose solutions are either &quot;yes&quot; or &quot;no&quot;. A yes-instance/no-instance is a problem instance of a decision problem for which the answer is yes/no. An algorithm solving a decision problem is <strong>polynomial time</strong> if and only if it has a worst case time complexity of <span class="math inline">O(n^k)</span> for some positive integer <span class="math inline">k</span>.</p>
<p>The complexity class <span class="math inline">P</span> is the <strong>set of all decision problems that have polynomial time algorithms solving them</strong>. Given a decision problem <span class="math inline">\Pi</span>, <span class="math inline">\Pi \in P</span> if and only if there is a polynomial time algorithm solving <span class="math inline">\Pi</span>. For example, sorting is not in <span class="math inline">P</span> because it is not a decision problem. For example, checking if a graph contains a cycle is in <span class="math inline">P</span> (<span class="math inline">O(n^2)</span>), while checking if a graph contains a Hamiltonian cycle has no known polynomial-time algorithms that solve it (and there probably isn't one).</p>
<p>The rational knapsack problem (where we answer yes if and only if the optimal profit is <span class="math inline">T</span> or more) is in <span class="math inline">P</span>, since we have a polynomial time greedy algorithm that can solve it, but the 0-1 knapsack problem (where we answer yes if and noly if the optimal profit is <span class="math inline">T</span> or more) is NP-complete.</p>
<p>A <strong>certificate</strong> for a yes-instance <span class="math inline">I</span> is extra information that allows us to verify whether <span class="math inline">I</span> is in fact a yes-instance - enough information to generate a formal proof that the answer is correct. A <strong>certificate verification algorithm</strong> is an algorithm that correctly determines whether a problem instance <span class="math inline">I</span> is actually a yes-instance given a certificate <span class="math inline">C</span>, written as <span class="math inline">\operatorname{Ver}(I, C)</span>.</p>
<p>A valid certificate is one that does in fact verify that <span class="math inline">I</span> is a yes-instance. Certificates can also be invalid, which means their information doesn't prove that <span class="math inline">I</span> is a yes-instance. Note that if <span class="math inline">I</span> is a no-instance, then no valid certificate for <span class="math inline">I</span> can exist. <span class="math inline">\operatorname{Ver}(I, C)</span> outputs yes if <span class="math inline">I</span> is a yes-instance and <span class="math inline">C</span> is a valid certificate for <span class="math inline">I</span>, and no if <span class="math inline">C</span> is not a valid certificate for <span class="math inline">I</span>.</p>
<p>A certificate verification algorithm <span class="math inline">\operatorname{Ver}(I, C)</span> <strong>solves</strong> a decision problem if and only if for every yes-instance <span class="math inline">I</span>, there exists a certificate <span class="math inline">C</span> such that <span class="math inline">\operatorname{Ver}(I, C)</span> outputs yes, and for every no-instance <span class="math inline">I</span>, all certificates <span class="math inline">X</span> make <span class="math inline">\operatorname{Ver}(I, C)</span> output no. In other words, given a certificate for a problem instance, the certificate verification algorithm outputs the correct answer to that problem instance, assuming yes-instances come with a valid certificate.</p>
<p>The complexity class <span class="math inline">NP</span> is the set of all decision problems that can be solved by a polynomial-time certificate verification algorithm - the <strong>decision problems for which it is possible to verify that the result is correct in polynomial time</strong>. Note that this includes all problems in <span class="math inline">P</span> because the certificate verification algorithm for polynomial time algorithms can just be that algorithm itself and the certificates can just be the problem instances.</p>
<p>For example, the problem &quot;does a graph contain a Hamiltonian cycle?&quot; is in <span class="math inline">NP</span>, because the certificate can just be the Hamiltonian cycle itself (a list of vertices in the cycle), and we can verify whether that particular cycle is a Hamiltonian cycle in <span class="math inline">O(n)</span> time (check that the cycle includes all vertices once, and neighboring vertices are connected by an edge). So <span class="math inline">P \subseteq NP</span>.</p>
<p>Whether <span class="math inline">P = NP</span> or not is currently a big open question in computer science. It is commonly believed that <span class="math inline">P \ne NP</span> - that there are problems that can be verified in polynomial time, but not computed in polynomial time.</p>
<p>All problem instances for problems in <span class="math inline">NP</span> can be solved in exponential time. This is because we could potentially generate all possible certificates (this is of exponential size because the certificate's size must be <span class="math inline">O(n)</span>, or else the certificate verification algorithm would not be polynomial time) and verify them. If any certificate verifies to a yes, then the problem instance is a yes-instance. Otherwise, it is a no-instance.</p>
<p>Let <span class="math inline">\mathcal{I}(\Pi)</span> be the set of all instances of a decision problem <span class="math inline">\Pi</span>. Let <span class="math inline">\mathcal{I}_{\text{yes}}(\Pi), \mathcal{I}_{\text{no}}(\Pi)</span> be the set of all yes-instances and no-instances, respectively, of a decision problem <span class="math inline">\Pi</span>.</p>
<p>A <strong>polynomial-time reduction/polynomial transformation</strong> from decision problem <span class="math inline">\Pi_1</span> to decision problem <span class="math inline">\Pi_2</span> (written as <span class="math inline">\Pi_1 \le_P \Pi_2</span>) exists if and only if there exists <span class="math inline">f: \mathcal{I}(\Pi_1) \to \mathcal{I}(\Pi_2)</span> such that <span class="math inline">f</span> can be computed in polynomial time, <span class="math inline">I \in \mathcal{I}_{\text{yes}}(\Pi_1) \implies f(I) \in \mathcal{I}_{\text{yes}}(\Pi_2)</span>, and <span class="math inline">I \in \mathcal{I}_{\text{no}}(\Pi_1) \implies f(I) \in \mathcal{I}_{\text{no}}(\Pi_2)</span>.</p>
<p>In other words, if <span class="math inline">\Pi_1 \le_P \Pi_2</span>, then <span class="math inline">\Pi_1</span> is no more difficult to solve than <span class="math inline">\Pi_2</span> - problem instances can be transformed into a different problem with the same answer in polynomial time, even without knowing what the answer is.</p>
<p>So if <span class="math inline">\Pi_1 \le_P \Pi_2</span>, and <span class="math inline">\Pi_2</span> is in <span class="math inline">P</span>, then <span class="math inline">\Pi_1</span> must also be in <span class="math inline">P</span>.</p>
<p>Some NP-complete problems are checking clique sizes (determine whether a graph contains a clique of size <span class="math inline">k</span> or greater), and checking vertex cover sizes (determine whether a graph contains a vertex cover of size <span class="math inline">k</span> or smaller). Note that there is a polynomial transformation from instances of the clique problem to instances of the vertex cover problem. We can define <span class="math inline">f(\tup{G, k}) = \tup{H, \abs{V(G)} - k}</span> where <span class="math inline">H</span> is the complement of <span class="math inline">G</span> (a graph with edges where <span class="math inline">G</span> doesn't have edges, and no edges where <span class="math inline">G</span> does). Since <span class="math inline">f</span> can be computed in polynomial time, and transformed problem instances give the same result, the polynomial transformation exists.</p>
<h1 id="section-20">25/11/15</h1>
<p>Proof that the clique problem has a polynomial transformation to the vertex cover problem:</p>
<blockquote>
<p>Let <span class="math inline">I</span> be a yes-instance of the clique problem for size <span class="math inline">k</span>.<br />
So there exists a <span class="math inline">W \subseteq V(G)</span> such that <span class="math inline">\abs{W} = k</span> and <span class="math inline">W</span> is a clique in <span class="math inline">G</span> - <span class="math inline">W</span> is a subset of size <span class="math inline">k</span> of the largest clique. We want to prove that <span class="math inline">f(I)</span> is also a yes-instance of the vertex cover problem.<br />
Clearly, <span class="math inline">V(H) \setminus W</span> is a vertex cover in <span class="math inline">H</span>, since every edge in <span class="math inline">V(H) \setminus W</span> has one end in <span class="math inline">V(H) \setminus W</span> and another in <span class="math inline">V(H)</span> (this is a theorem in graph theory).<br />
So if <span class="math inline">f(I)</span> is a yes-instance, <span class="math inline">I</span> must also be a yes-instance.<br />
Let <span class="math inline">f(I)</span> be a yes-instance of the vertex cover problem for size <span class="math inline">n - k</span>.<br />
By a similar argument as above, show that there must be a clique of size <span class="math inline">n - k</span> in <span class="math inline">G</span>.<br />
So if <span class="math inline">I</span> is a yes-instance, <span class="math inline">f(I)</span> must also be a yes-instance.<br />
Therefore, <span class="math inline">f(I)</span> is a yes-instance if and only if <span class="math inline">I</span> is a yes-instance.</p>
</blockquote>
<p>Note that if a problem instance <span class="math inline">I</span> has size <span class="math inline">n</span>, then the size of any polynomial transformation of <span class="math inline">I</span> must have size <span class="math inline">O(n^k)</span> for positive integer <span class="math inline">k</span>. This is because the computation creating a problem instance of size larger than <span class="math inline">O(n^k)</span> would take longer than <span class="math inline">O(n^k)</span> time, so the transformation would not be able to run in polynomial time. In other words, <span class="math inline">\abs{f(I)} \in O(\abs{I}^k)</span> for some positive integer <span class="math inline">k</span>.</p>
<p>If <span class="math inline">\Pi_1 \le_P \pi_2</span> and <span class="math inline">\Pi_2 \in P</span>, then <span class="math inline">\Pi_1 \in P</span> - if a problem has a polynomial transformation to another, and the other can be solved in polynomial time, then so can the first problem. This is because we can solve an instance of <span class="math inline">\Pi_1</span> by transforming it into an instance of <span class="math inline">\Pi_2</span> in polynomial time, solve the <span class="math inline">\Pi_2</span> instance in polynomial time, and give that as the answer.</p>
<p>If <span class="math inline">\Pi_1 \le_P \Pi_2</span> and <span class="math inline">\Pi_2 \le_P \Pi_3</span>, then <span class="math inline">\Pi_1 \le_P \Pi_3</span> - polynomial transformations can be composed. This is because any instance of the first problem can be polynomially transformed into an instance of the third by transforming it into an instance of the second, and then transforming that instance into an instance of the third problem.</p>
<p>The complexity class <span class="math inline">NP</span>-complete (also known as <span class="math inline">NPC</span>) are the set of all decision problems such that <span class="math inline">\Pi \in NP</span> and for all <span class="math inline">\Pi&#39; \in NP</span>, <span class="math inline">\Pi&#39; \le_P \Pi</span> - the <strong>decision problems in NP such that there are polynomial transformations from all other problems in NP to them</strong>.</p>
<p>That means that if <span class="math inline">P \cap NPC \ne \emptyset</span>, then <span class="math inline">P = NP</span> - if any NP-complete problem can be solved in polynomial time, then that algorithm can be used to solve any NP-complete problem in polynomial time.</p>
<p>Given a Boolean formula <span class="math inline">F</span> over Boolean variables <span class="math inline">x_1, \ldots, x_n</span> in conjunctive normal form (the formula is a product of sums), is there a truth assignment of the <span class="math inline">n</span> variables such that the formula evaluates to true? In other words, do there exist values of <span class="math inline">x_1, \ldots, x_n</span> such that <span class="math inline">F</span> is true? This is the <strong>CNF-satisfiability problem</strong> (also known as SAT).</p>
<p>A <strong>literal</strong> is either a variable or a negation of a variable, <span class="math inline">a</span> or <span class="math inline">\neg a</span>. A <strong>clause</strong> is a logical OR over literals, usually represented as a set of literals. A <strong>formula</strong> is a set of clauses.</p>
<p>According to the Cook-Levin theorem, the CNF-satisfiability problem is NP-complete. This is the first ever problem to be proved NP-complete.</p>
<p>There is a theorem that says that if <span class="math inline">\Pi_1</span> is NP-complete, and <span class="math inline">\Pi_1 \le_P \Pi_2</span>, then <span class="math inline">\Pi_2</span> is also NP-complete, since <span class="math inline">\Pi_2 \in NP</span> and any <span class="math inline">\Pi&#39;</span> must satisfy <span class="math inline">\Pi&#39; \le_P \Pi_1 \le_P \Pi_2</span>. So once we know one NP-complete problem, we can simply show that an NP-complete problem can be transformed in polynomial time to the problem we want to prove is NP-complete, which is a much simpler problem than proving a problem NP-complete from scratch.</p>
<p>For example, we can prove the 3SAT/3-CNF-satisfiability problem (same as SAT, but <span class="math inline">F</span> is a product of sums where every sum has exactly 3 literals/variables, all different) is NP-complete in this way. To convert any SAT problem instance into 3SAT:</p>
<ol type="1">
<li>Initialize <span class="math inline">F&#39;</span> as an empty set.</li>
<li>For each clause <span class="math inline">C = \set{x_{i_1}, \ldots, x_{i_k}}</span> in <span class="math inline">F</span> (each sum in the product of sums):
<ol type="1">
<li>If <span class="math inline">\abs{C} = 1</span>, add the clauses <span class="math inline">x_{i_1} \vee a_1 \vee a_2, x_{i_1} \vee a_1 \vee \neg a_2, x_{i_1} \vee \neg a_1 \vee a_2, x_{i_1} \vee \neg a_1 \vee \neg a_2</span> to <span class="math inline">F&#39;</span>, where <span class="math inline">a_1, a_2</span> are new, unused variables (these should not used in any other clauses).
<ul>
<li>This is clearly true since the clauses evaluate to <span class="math inline">x_{i_1}</span> regardless of the values of <span class="math inline">a_1, a_2</span>.</li>
</ul></li>
<li>If <span class="math inline">\abs{C} = 2</span>, add the clauses <span class="math inline">x_{i_1} \vee x_{i_2} \vee c, x_{i_1} \vee x_{i_2} \vee \neg a</span> to <span class="math inline">F&#39;</span>, where <span class="math inline">a</span> is a new, unused variable (this should not be used in any other clauses).
<ul>
<li>In the same way as above, regardless of the value of the variable, the clauses are always satisfiable if and only if <span class="math inline">C</span> is.</li>
</ul></li>
<li>If <span class="math inline">\abs{C} = 3</span>, add this clause to <span class="math inline">F&#39;</span>.
<ul>
<li>This clause is already a valid 3SAT clause.</li>
</ul></li>
<li>If <span class="math inline">\abs{C} &gt; 3</span>, add <span class="math inline">\abs{C} - 2</span> clauses <span class="math inline">a_1 \vee a_2 \vee x{i_1}, a_3 \vee \neg x_{i_1} \vee x_{i_2}, a_4 \vee \neg x_{i_2} \vee x_{i_3}, a_5 \vee \neg x_{i_3} \vee x_{i_4}, \ldots, a_{k - 2} \vee \neg x_{i_{k - 4}} \vee x_{i_{k - 3}}, a_{k - 1} \vee a_k \vee \neg x_{i_{k - 3}}</span> to <span class="math inline">F&#39;</span> where <span class="math inline">a_1, \ldots, a_k</span> are new, unused variables (these should not be used in any other clauses).
<ul>
<li>We can prove this is correct by proving that <span class="math inline">C</span> being satisfiable implies the added clauses are satisfiable (via direct proof), and if the added clauses are satifiable, <span class="math inline">C</span> is also satisfiable (via contradiction).</li>
</ul></li>
</ol></li>
<li>After this is done, <span class="math inline">F&#39;</span> is an instance of 3SAT that is a yes-instance if and only if <span class="math inline">F</span> is a yes-instance of <span class="math inline">F</span>.</li>
</ol>
<p>Since the transformation runs in polynomial time, 3SAT is in NP and <span class="math inline">\text{SAT} \le_P \text{3SAT}</span>, so 3SAT is NP-complete as well.</p>
<p>We can also prove that 2SAT/2-CNF-satisfiability problem (same as SAT, but <span class="math inline">F</span> is a problem of sums where every sum has exactly 2 variables, all different) is in P.</p>
<h1 id="section-21">30/11/15</h1>
<p>On computers, we can represent SAT problem instances with an <span class="math inline">m</span> by <span class="math inline">n</span> matrix <span class="math inline">M</span> (where <span class="math inline">n</span> is the number of variables and <span class="math inline">m</span> is the number of clauses), where rows represent clauses <span class="math inline">C_1, \ldots, C_m</span>, and columns represent variables <span class="math inline">x_1, \ldots, x_n</span>. Each cell <span class="math inline">M_{i, j}</span>, then represents whether <span class="math inline">x_j</span> is in the clause <span class="math inline">C_i</span> as a literal (represented by 1), in the clause <span class="math inline">C_i</span> negated as a literal (represented by -1), or not in the clause at all (represented by 0).</p>
<p>Clearly, the size of the problem instance <span class="math inline">M</span> is <span class="math inline">\Theta(mn)</span>, and <span class="math inline">m</span> can grow exponentially with respect to <span class="math inline">n</span>. The algorithm that converts instances of SAT to 3SAT runs in polynomial time with respect to the size of the instance, <span class="math inline">\Theta(mn)</span>, and so is a polynomial transformation.</p>
<p>We've been talking about <strong>decision problems</strong> all this time, but now we want to also look at problems in general.</p>
<p>Given a problem <span class="math inline">\Pi</span> (not necessarily a decision problem), an <strong>oracle</strong> for <span class="math inline">\Pi</span> is an (possibly hypothetical) algorithm <span class="math inline">A</span> that solves it.</p>
<p>Given another problem <span class="math inline">\Pi&#39;</span>, an algorithm <span class="math inline">A&#39;</span> is a <strong>Turing reduction</strong> from <span class="math inline">\Pi&#39;</span> to <span class="math inline">\Pi</span> if and only if it solves <span class="math inline">\Pi&#39;</span> using <span class="math inline">A</span> as a subroutine (<span class="math inline">A&#39;</span> depends on <span class="math inline">A</span> to work, so <span class="math inline">A</span> is the solver). A Turing reduction is sort of like a generalization of the concept of polynomial transformations to all problems. We can say that a Turing reduction exists by writing <span class="math inline">\Pi&#39; \le^T \Pi</span>.</p>
<p>A Turing reduction <span class="math inline">A&#39;</span> using <span class="math inline">A</span> as the solver is a <strong>polynomial-time Turing reduction</strong> if and only if <span class="math inline">A&#39;</span> has polynomial running time with respect to the running time of <span class="math inline">A</span>. If this is the case, we can write <span class="math inline">\Pi&#39; \le_P^T \Pi</span>.</p>
<p>Alternatively, we can use a more intuitive definition: if we have a magical box called an <strong>oracle</strong> that can solve any instance of <span class="math inline">\Pi&#39;</span> in polynomial time, and we have an algorithm that transforms instances of <span class="math inline">\Pi</span> to instances <span class="math inline">\Pi&#39;</span> and vice versa in polynomial time, we could use the oracle to solve <span class="math inline">\Pi</span> in polynomial time.</p>
<p>Basically, if there is a polynomial-time Turing reduction <span class="math inline">\Pi&#39; \le_P^T \Pi</span> and <span class="math inline">\Pi</span> can be solved in polynomial time, then <span class="math inline">\Pi&#39;</span> can also be solved in polynomial time.</p>
<p>Turing reductions are interesting because it seems like we can reduce harder versions of problems into simpler versions.</p>
<p>A travelling salesperson problem (TSP) involves finding paths in a graph with weighted edges such that the total weight is minimized.</p>
<p>Given a graph <span class="math inline">G</span> and edge weights <span class="math inline">w: E(G) \to \mb{Z}^+</span>, what is a Hamiltonian cycle <span class="math inline">H</span> in <span class="math inline">G</span> that minimises <span class="math inline">\sum_{e \in H} w(e)</span>? This is the <strong>TSP-optimisation</strong> problem.</p>
<p>Given a graph <span class="math inline">G</span> and edge weights <span class="math inline">w: E(G) \to \mb{Z}^+</span>, what is the smallest <span class="math inline">\sum_{e \in H} w(e)</span> where <span class="math inline">H</span> is a Hamiltonian cycle in <span class="math inline">G</span>? This is the <strong>TSP-optimal-value</strong> problem.</p>
<p>Given a graph <span class="math inline">G</span> and edge weights <span class="math inline">w: E(G) \to \mb{Z}^+</span> and a positive integer <span class="math inline">T</span>, is there a Hamiltonian cycle <span class="math inline">H</span> in <span class="math inline">G</span> such that <span class="math inline">\sum_{e \in H} w(e) \le T</span>? This is the <strong>TSP-decision</strong> problem.</p>
<p>Clearly, <span class="math inline">\text{TSP-decision} \le_P^T \text{TSP-optimal-value} \le_P^T \text{TSP-optimisation}</span>:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> tsp_optimisation(vertices, edges, weights):
    <span class="cf">pass</span> <span class="co"># suppose we are given this algorithm, and it returns a list of edges (or `None` if there are no Hamiltonian cycles)</span>

<span class="kw">def</span> tsp_optimal_value(vertices, edges, weights):
    hamiltonian_cycle <span class="op">=</span> tsp_optimisation(vertices, edges, weights)
    <span class="cf">if</span> hamiltonian_cycle <span class="kw">is</span> <span class="va">None</span>: <span class="cf">return</span> <span class="va">None</span>
    <span class="cf">return</span> <span class="bu">sum</span>(weights[edge] <span class="cf">for</span> edge <span class="kw">in</span> hamiltonian_cycle)

<span class="kw">def</span> tsp_decision(vertices, edges, weights, threshold):
    <span class="cf">return</span> tsp_optimal_value(vertices, edges, weights) <span class="op">&lt;=</span> threshold</code></pre></div>
<p>However, it is also true that <span class="math inline">\text{TSP-optimisation} \le_P^T \text{TSP-optimal-value} \le_P^T \text{TSP-decision}</span>! In other words, if we have a polynomial time algorithm for TSP-decision, we can solve TSP-optimal-value in polynomial time, and if we have a polynomial time algorithm for TSP-optimal-value, we can solve TSP-optimisation in polynomial time:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> tsp_decision(vertices, edges, weights, threshold):
    <span class="cf">pass</span> <span class="co"># suppose we are given this algorithm, and it returns `True` or `False`</span>

<span class="kw">def</span> tsp_optimal_value(vertices, edges, weights):
    <span class="co"># basically, we do a binary search for the exact total weight of the Hamiltonian cycle</span>
    low, high <span class="op">=</span> <span class="dv">0</span>, <span class="bu">sum</span>(weights[edge] <span class="cf">for</span> edge <span class="kw">in</span> edges)
    <span class="cf">if</span> tsp_decision(vertices, edges, weights, high): <span class="cf">return</span> <span class="bu">float</span>(<span class="st">&quot;inf&quot;</span>)
    <span class="cf">while</span> low <span class="op">&lt;</span> high:
        middle <span class="op">=</span> floor((low <span class="op">+</span> high) <span class="op">/</span> <span class="dv">2</span>)
        <span class="cf">if</span> tsp_decision(vertices, edges, weights, middle):
            high <span class="op">=</span> middle
        <span class="cf">else</span>:
            low <span class="op">=</span> middle <span class="op">+</span> <span class="dv">1</span>
    <span class="cf">return</span> high

<span class="kw">def</span> tsp_optimisation(vertices, edges, weights):
    <span class="co"># repeatedly remove edges whose removal doesn&#39;t increase the optimal total weight (removing edges can never lower the optimal total weight)</span>
    lowest_total_weight <span class="op">=</span> tsp_optimal_value(vertices, edges, weights)
    <span class="cf">if</span> lowest_total_weight <span class="op">==</span> <span class="bu">float</span>(<span class="st">&quot;inf&quot;</span>): <span class="cf">return</span> <span class="va">None</span>
    lowest_weight <span class="op">=</span> lowest_total_weight
    hamiltonian_cycle <span class="op">=</span> <span class="bu">set</span>()
    new_weights <span class="op">=</span> <span class="bu">dict</span>(weights)
    <span class="cf">for</span> edge <span class="kw">in</span> edges:
        new_weights[edge] <span class="op">=</span> <span class="bu">float</span>(<span class="st">&quot;inf&quot;</span>) <span class="co"># remove edge by making it have infinite weight</span>
        <span class="cf">if</span> <span class="kw">not</span> tsp_decision(vertices, edges, new_weights, lowest_total_weight):
            new_weights[edge] <span class="op">=</span> weights[edge] <span class="co"># restore the edge, since it&#39;s part of the optimal edge</span>
            hamiltonian_cycle.add(edge)</code></pre></div>
<p>In summary, <span class="math inline">\text{subset-sum} \le_P^T \text{vertex-cover} \le_P^T \text{clique} \le_P^T \text{3SAT} \le_P^T \text{SAT}</span> - these are well-known NP-complete problems.</p>
<h1 id="section-22">2/12/15</h1>
<p>We can transform 3SAT to the clique problem as follows:</p>
<blockquote>
<p>Let <span class="math inline">I</span> be a 3SAT problem instance over variables <span class="math inline">x_1, \ldots, x_n</span> and clauses <span class="math inline">C_1, \ldots, C_m</span>. Let <span class="math inline">C_{i, j}</span> represent the <span class="math inline">j</span>th literal of the <span class="math inline">i</span>th clause.<br />
Let <span class="math inline">f(I) = \tup{G, m}</span> where vertices of <span class="math inline">G</span> are the literals in <span class="math inline">C_1, \ldots, C_m</span> (there are exactly <span class="math inline">3m</span> vertices, duplicates are allowed) and the edges of <span class="math inline">G</span> join any two literals that are not negations of each other and not in the same clause - where <span class="math inline">C_{i, j}, C_{i&#39;, j&#39;}</span> such that <span class="math inline">i \ne i&#39;</span> and <span class="math inline">C_{i, j} \ne \neg C_{i&#39;, j&#39;}</span>. Basically, we transform <span class="math inline">I</span> into checking if there is a clique (fully connected subgraph) in <span class="math inline">G</span> of size <span class="math inline">m</span> or greater. Clearly, <span class="math inline">f(I)</span> can be computed in polynomial time. Assume <span class="math inline">I</span> is a yes-instance. Then there is some assignment such that all clauses are satisfied. Then every clause must have at least one true literal. Since these literals can't contradict each other if the clauses are satisfied, there are edges between all of them, so these literals form a clique of size <span class="math inline">m</span> or more.<br />
Assume <span class="math inline">f(I)</span> is a yes-instance, so there is a clique with <span class="math inline">m</span> vertices. Clearly, if there are <span class="math inline">m</span> vertices then there is one vertex from each clause, since literals within the same clause aren't joined by edges. Since these literals all don't contradict each other as they're connected by edges, it is possible to satisfy all of these literals - all of the clauses are satisfiable and <span class="math inline">I</span> is a yes-instance.<br />
Therefore, <span class="math inline">f(I)</span> is a yes-instance if and only if <span class="math inline">I</span> is a yes-instance.</p>
</blockquote>
<p>Given a list of positive integers <span class="math inline">s_1, \ldots, s_n</span> and a target sum <span class="math inline">T</span>, is there a subset of <span class="math inline">s_1, \ldots, s_n</span> such that the sum of the elements is <span class="math inline">T</span>? This is the <strong>subset-sum</strong> problem. The subset-sum problem is sort of like the knapsack problem.</p>
<p>We can also transform the clique problem to the subset-sum problem:</p>
<blockquote>
<p>Let <span class="math inline">I</span> be a Clique problem instance over graph <span class="math inline">G</span> and clique size threshold <span class="math inline">k</span>.<br />
Let <span class="math inline">V(G) = \set{v_1, \ldots, v_n}</span> and <span class="math inline">E(G) = \set{e_1, \ldots, e_n}</span>. Let <span class="math inline">c_{i, j} = \begin{cases} 1 &amp;\text{if } v_i \in e_j \\ 0 &amp;\text{if } v_i \notin e_j \end{cases}</span>.<br />
Let <span class="math inline">a_i = 10^{\abs{E(G)}} + \sum_{j = 0}^{\abs{E(G)} - 1} c_{i, j} 10^j</span> and <span class="math inline">b_j = 10^j</span>. Let <span class="math inline">T = k 10^{\abs{E(G)}} + \sum_{j = 0}^{\abs{E(G)} - 1} 2 \times 10^j</span>.<br />
We now have a subset-sum problem instance with positive integers <span class="math inline">a_1, \ldots, a_{\abs{V(G)}}, b_1, \ldots, b_{\abs{E}}</span> and target sum <span class="math inline">T</span>.<br />
Each sum value <span class="math inline">a_i</span> has lower digits corresponding to every edge, that are 1 if and only if the edge is incident to vertex <span class="math inline">i</span>. The digits above these act as a counter - adding <span class="math inline">k</span> of <span class="math inline">a_i</span> together makes the top tigits represent <span class="math inline">k</span>. Each sum value <span class="math inline">b_j</span> has digits that are 0 except for the one corresponding to edge <span class="math inline">j</span>. The target sum requires these to be added together to make every digit corresponding to edges equal to 2, and also add exactly <span class="math inline">k</span> of <span class="math inline">a_i</span> to make sure the clique is of size <span class="math inline">k</span>.<br />
Therefore, this problem instance gives the same answer as <span class="math inline">I</span>.</p>
</blockquote>
<h2 id="undecidability">Undecidability</h2>
<p>A decision problem <span class="math inline">\Pi</span> is <strong>undecidable</strong> if and only if there do not exist algorithms that solve it. For an undecidable problem and any algorithm, there is at least one problem instance such that the algorithm won't find the correct answer in finite time.</p>
<p>The <strong>Halting problem</strong> is the most famous example of an undecidable problem - given a computer program <span class="math inline">A</span> and the input for the program <span class="math inline">A</span>, does the program halt in finite time?</p>
<p>This is easy to prove undecidable. Suppose an algorithm solves the Halting problem. Then we can implement it in a program as a function <code>halts(program, input)</code>, returning <code>True</code> if <code>program</code> halts when given input <code>input</code>, and <code>False</code> otherwise.</p>
<p>Consider the following program:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> contrarian(program):
    <span class="cf">if</span> halts(program, program):
        <span class="cf">while</span> <span class="va">True</span>: <span class="cf">pass</span> <span class="co"># never halt</span>
    <span class="cf">else</span>:
        <span class="cf">return</span> <span class="co"># immediately halt</span>

contrarian(contrarian)</code></pre></div>
<p>If <code>contrarian</code> halts, then <code>halts(program, program)</code> returns true, but then <code>contrarian</code> doesn't halt if this is the case, which is impossible. If <code>contrarian</code> doesn't halt, then <code>halts(program, program)</code> returns false, but then <code>contrarian</code> halts immediately if that is the case, which is impossible. Therefore, <code>contrarian</code> neither halts or doesn't halt, a contradiction. So no algorithm can exist that solves the halting problem.</p>
<p>We can actually use Turing reductions to prove that some other problems are undecidable - if we can Turing reduce the halting problem to another problem <span class="math inline">\Pi</span>, then it must also be undecidable, since if it wasn't we could solve the halting problem using it.</p>
<div class="status-banner" style="display: none; position: fixed; bottom: 0; left: 0; right: 0; text-align: center;">
    <div style="display: inline-block; padding: 0.8em 2em 0.5em 2em; background: black; color: white; font-size: 2em;">
        Rendering <svg xmlns="http://www.w3.org/2000/svg" height="1.4em" viewbox="0 0 1200 500" style="vertical-align: text-bottom"><title>LaTeX logo</title><g transform="matrix(45 0 0 45 40 40)" fill="white"><path d="M5.5 4.4C5.5 4.4 5.2 4.4 5.2 4.4 5.1 5.4 5 6.7 3.2 6.7 3.2 6.7 2.4 6.7 2.4 6.7 1.9 6.7 1.9 6.6 1.9 6.3 1.9 6.3 1.9 1 1.9 1 1.9 0.6 1.9 0.5 2.9 0.5 2.9 0.5 3.2 0.5 3.2 0.5 3.2 0.5 3.2 0.2 3.2 0.2 2.8 0.2 1.9 0.2 1.5 0.2 1.1 0.2 0.3 0.2 0 0.2 0 0.2 0 0.5 0 0.5 0 0.5 0.2 0.5 0.2 0.5 1 0.5 1 0.6 1 0.9 1 0.9 1 6.2 1 6.2 1 6.6 1 6.7 0.2 6.7 0.2 6.7 0 6.7 0 6.7 0 6.7 0 7 0 7 0 7 5.2 7 5.2 7 5.2 7 5.5 4.4 5.5 4.4z"/><path d="M5.3 0.2C5.3 0 5.2 0 5.1 0 5 0 4.9 0 4.9 0.2 4.9 0.2 3.3 4.2 3.3 4.2 3.2 4.4 3.1 4.7 2.5 4.7 2.5 4.7 2.5 5 2.5 5 2.5 5 4 5 4 5 4 5 4 4.7 4 4.7 3.7 4.7 3.5 4.6 3.5 4.4 3.5 4.3 3.5 4.3 3.6 4.2 3.6 4.2 3.9 3.4 3.9 3.4 3.9 3.4 5.9 3.4 5.9 3.4 5.9 3.4 6.3 4.4 6.3 4.4 6.3 4.4 6.3 4.5 6.3 4.5 6.3 4.7 5.9 4.7 5.8 4.7 5.8 4.7 5.8 5 5.8 5 5.8 5 7.7 5 7.7 5 7.7 5 7.7 4.7 7.7 4.7 7.7 4.7 7.6 4.7 7.6 4.7 7.1 4.7 7.1 4.7 7 4.5 7 4.5 5.3 0.2 5.3 0.2zM4.9 0.9C4.9 0.9 5.8 3.1 5.8 3.1 5.8 3.1 4 3.1 4 3.1 4 3.1 4.9 0.9 4.9 0.9z"/><path d="M13.3 0.2C13.3 0.2 7.2 0.2 7.2 0.2 7.2 0.2 7 2.5 7 2.5 7 2.5 7.3 2.5 7.3 2.5 7.4 0.9 7.6 0.5 9.1 0.5 9.3 0.5 9.5 0.5 9.6 0.6 9.8 0.6 9.8 0.7 9.8 0.9 9.8 0.9 9.8 6.2 9.8 6.2 9.8 6.5 9.8 6.7 8.8 6.7 8.8 6.7 8.4 6.7 8.4 6.7 8.4 6.7 8.4 7 8.4 7 8.8 6.9 9.8 6.9 10.3 6.9 10.7 6.9 11.7 6.9 12.2 7 12.2 7 12.2 6.7 12.2 6.7 12.2 6.7 11.8 6.7 11.8 6.7 10.7 6.7 10.7 6.5 10.7 6.2 10.7 6.2 10.7 0.9 10.7 0.9 10.7 0.7 10.7 0.6 10.9 0.6 11 0.5 11.3 0.5 11.5 0.5 13 0.5 13.1 0.9 13.2 2.5 13.2 2.5 13.5 2.5 13.5 2.5 13.5 2.5 13.3 0.2 13.3 0.2z"/><path d="M18.7 6.7C18.7 6.7 18.4 6.7 18.4 6.7 18.2 8.2 17.9 8.9 16.2 8.9 16.2 8.9 14.9 8.9 14.9 8.9 14.4 8.9 14.4 8.8 14.4 8.5 14.4 8.5 14.4 5.9 14.4 5.9 14.4 5.9 15.3 5.9 15.3 5.9 16.3 5.9 16.4 6.2 16.4 7 16.4 7 16.6 7 16.6 7 16.6 7 16.6 4.4 16.6 4.4 16.6 4.4 16.4 4.4 16.4 4.4 16.4 5.2 16.3 5.5 15.3 5.5 15.3 5.5 14.4 5.5 14.4 5.5 14.4 5.5 14.4 3.2 14.4 3.2 14.4 2.8 14.4 2.8 14.9 2.8 14.9 2.8 16.2 2.8 16.2 2.8 17.7 2.8 18 3.3 18.1 4.7 18.1 4.7 18.4 4.7 18.4 4.7 18.4 4.7 18.1 2.5 18.1 2.5 18.1 2.5 12.5 2.5 12.5 2.5 12.5 2.5 12.5 2.8 12.5 2.8 12.5 2.8 12.7 2.8 12.7 2.8 13.5 2.8 13.5 2.9 13.5 3.2 13.5 3.2 13.5 8.4 13.5 8.4 13.5 8.8 13.5 8.9 12.7 8.9 12.7 8.9 12.5 8.9 12.5 8.9 12.5 8.9 12.5 9.2 12.5 9.2 12.5 9.2 18.2 9.2 18.2 9.2 18.2 9.2 18.7 6.7 18.7 6.7z"/><path d="M21.7 3.1C21.7 3.1 23 1.1 23 1.1 23.3 0.8 23.6 0.5 24.5 0.5 24.5 0.5 24.5 0.2 24.5 0.2 24.5 0.2 22.1 0.2 22.1 0.2 22.1 0.2 22.1 0.5 22.1 0.5 22.5 0.5 22.7 0.7 22.7 0.9 22.7 1 22.7 1.1 22.6 1.2 22.6 1.2 21.5 2.8 21.5 2.8 21.5 2.8 20.2 0.9 20.2 0.9 20.2 0.9 20.1 0.8 20.1 0.8 20.1 0.7 20.4 0.5 20.8 0.5 20.8 0.5 20.8 0.2 20.8 0.2 20.4 0.2 19.7 0.2 19.3 0.2 19 0.2 18.4 0.2 18 0.2 18 0.2 18 0.5 18 0.5 18 0.5 18.2 0.5 18.2 0.5 18.8 0.5 19 0.5 19.2 0.8 19.2 0.8 21 3.6 21 3.6 21 3.6 19.4 6 19.4 6 19.2 6.2 18.9 6.7 17.9 6.7 17.9 6.7 17.9 7 17.9 7 17.9 7 20.3 7 20.3 7 20.3 7 20.3 6.7 20.3 6.7 19.8 6.7 19.7 6.4 19.7 6.2 19.7 6.1 19.7 6.1 19.8 6 19.8 6 21.2 3.9 21.2 3.9 21.2 3.9 22.8 6.3 22.8 6.3 22.8 6.3 22.8 6.3 22.8 6.4 22.8 6.5 22.6 6.7 22.2 6.7 22.2 6.7 22.2 7 22.2 7 22.5 6.9 23.2 6.9 23.6 6.9 24 6.9 24.5 7 24.9 7 24.9 7 24.9 6.7 24.9 6.7 24.9 6.7 24.7 6.7 24.7 6.7 24.2 6.7 24 6.6 23.8 6.3 23.8 6.3 21.7 3.1 21.7 3.1z"/></g></svg> math...
    </div>
</div>
<div class="license">
  <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-sa/4.0/80x15.png" /></a> This work by <a xmlns:cc="http://creativecommons.org/ns#" href="https://uberi.github.io/" property="cc:attributionName" rel="cc:attributionURL">Anthony Zhang</a> is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.
  Copyright 2013-2017 Anthony Zhang.
</div>
</body>
</html>
