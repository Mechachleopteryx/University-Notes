<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <title>CS486 | Anthony Zhang</title>
  <link rel="stylesheet" href="../css/base.css" type="text/css">
  <link rel="stylesheet" href="../css/note.css" type="text/css">
  <link rel="stylesheet" href="../highlight/styles/default.css">
  <link rel="stylesheet" href="../highlight/styles/paraiso-light.css">
  <script src="../highlight/highlight.pack.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
  <style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
  </style>
  <script src="../katex/katex.min.js" type="text/javascript"></script>
  <link rel="stylesheet" href="../katex/katex.min.css" />
  <script type="text/javascript">
  window.onload = function() {
    document.getElementsByClassName("status-banner")[0].style.display = "block";
    setTimeout(function() {
      renderMathElements(document.getElementsByClassName("math"));
      document.getElementsByClassName("status-banner")[0].style.display = "none";
    }, 50); // delay to allow status banner to show
  }

  function renderMathElements(mathElements) {
    var mathOptions = {
      macros: {
        "\\set": "\\left\\{ #1 \\right\\}",
        "\\tup": "\\left\\langle #1 \\right\\rangle",
        "\\abs": "\\left\\lvert #1 \\right\\rvert",
        "\\floor": "\\left\\lfloor #1 \\right\\rfloor",
        "\\ceil": "\\left\\lceil#1 \\right\\rceil",
        "\\mb": "\\mathbb{#1}",
        "\\rem": "\\operatorname{rem}",
        "\\ord": "\\operatorname{ord}",
        "\\sign": "\\operatorname{sign}",
        "\\imag": "\\bm{i}",
        "\\dee": "\\mathop{}\\!\\mathrm{d}",
        "\\lH": "\\overset{\\text{l'H}}{=}",
        "\\evalat": "\\left.\\left(#1\\right)\\right|",
        "\\sech": "\\operatorname{sech}",
        "\\spn": "\\operatorname{Span}",
        "\\proj": "\\operatorname{proj}",
        "\\prp": "\\operatorname{perp}",
        "\\refl": "\\operatorname{refl}",
        "\\magn": "\\left\\lVert #1 \\right\\rVert",
        "\\rank": "\\operatorname{rank}",
        "\\trace": "\\operatorname{trace}",
        "\\sys": "\\left[ #1 \\mid #2\\space \\right]",
        "\\range": "\\operatorname{Range}",
        "\\adj": "\\operatorname{adj}",
        "\\cof": "\\operatorname{cof}",
        "\\coord": "{\\left\\lbrack #1 \\right\\rbrack}_{#2}",
        "\\diag": "\\operatorname{diag}",
        "\\formlp": "\\operatorname{Form}(\\mathcal{L}^P)",
        "\\argmin": "\\operatorname{argmin}",
        "\\argmax": "\\operatorname{argmax}",
        "\\sgn": "\\operatorname{sgn}",

        // not yet available in KaTeX
        "\\bm": "\\mathbf", //wip: should be italic, but isn't
      },
    };
    for (var i=0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      try {
        katex.render(texText.data, mathElements[i], mathOptions);
      } catch (e) {
        console.error(e);
        console.log(mathElements[i]);
      }
    }
  }
  </script>
</head>
<body>
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-68271407-1', 'auto');
    ga('send', 'pageview');

  </script>
  <h1>Lecture Notes by <a href="/">Anthony Zhang</a>.</h1>
  <ul class="site_links">
    <li><a href="/blog/" class="page">blog</a></li>
    <span class="divider"></span>
    <li><a href="http://uberi.github.io/University-Notes" class="page">notes</a></li>
    <span class="divider"></span>
    <li><a href="/resume.pdf" class="page">résumé</a></li>
    <span class="divider"></span>
    <li><a href="https://github.com/Uberi" class="contact">github</a></li>
    <span class="divider"></span>
    <li><a href="https://www.linkedin.com/in/uberi/" class="contact">linkedin</a></li>
    <span class="divider"></span>
    <li><a href="mailto:me@anthonyz.ca" class="contact">email</a></li>
    <span class="divider"></span>
    <li><a href="https://www.facebook.com/anthony.zhang.user" class="contact">facebook</a></li>
    <span class="divider"></span>
    <li><a href="https://twitter.com/anthony926535" class="contact">twitter</a></li>
    <span class="divider"></span>
    <li><a href="https://keybase.io/uberi" class="info">public key</a></li>
  </ul>
<h1 id="cs486">CS486</h1>
<p>Introduction to Artificial Intelligence.</p>
<pre><code>Alan Tsang
Section 001
Email: akhtsang@uwaterloo.ca
Website: https://cs.uwaterloo.ca/~klarson/teaching/F17-486/, https://cs.uwaterloo.ca/~akhtsang/cs486.html
Office Hours: Tuesdays 1:15pm to 2:45pm in DC2306B, accessible via the AI Lab
Mondays/Wednesdays 4:00pm-5:20pm</code></pre>
<h1 id="section">11/9/17</h1>
<p>Info on course website, assignments on LEARN, questions on Piazza. 5 assignments worth 40% total, 15% midterm on October 18, 2017, 45% final exam, optional project worth 5% bonus marks. Assignments can be submitted up to 48 hours after deadline.</p>
<p>There's no real agreed-upon definition for AI, but in this course we'll consider it to be the mathematical study of intelligent action in a complex environment. We usually think about 4 different kinds of AI: systems that think like humans, act like humans, think rationally, or act rationally. The definitions usually differ along the dimensions of thinking vs. acting and human behaviour vs. idealized rational behaviour. In other words, we're trying to duplicate what the human brain does (passing the turing test), and what the human brain should do (acting rationally).</p>
<p>Overview of AI in popular perception: threats of superintelligence, automation displacing jobs, biases encoded in AI models, self-driving cars. Overview of the turing test, including issues like not being amenable to mathematical analysis, whether it's more important to understand underlying principles than mimicking true intelligence, and whether it's even a good test (we don't make planes by trying to make something so bird-like that it fools a bird). Overview of history of AI, like Turing's work, the Dartmouth meeting, AI winter, and modern computational AI.</p>
<p>Classically, AI was seen to mean general problem solving, like solving chess. More recently, we focus more on specific problems, such as perception, robotics, and deliberative reasoning (thinking about what to do based on available data), with a greater focus on statistical methods, decision theories, and probability.</p>
<p>In this course we'll talk about classic AI problems like search, as well as knowledge representation/reasoning, making decisions under uncertainty, and learning.</p>
<p>An <strong>agent</strong> is an entity that perceives and acts. We usually evaluate agents based on things like how well it achieves goals and how many resources it consumes. Agents exist within an environment, with actuators, sensors, and environmental constraints. Environments can be fully/partially observable (can we see the entire environment at once?), deterministic/stochastic (does the same action always result in the same result?), episodic/dynamic (do previous steps affect later steps? in classification no, since previous predictions are independent, but in chess yes, since previous moves are very important), discrete/continuous (chess AI could be considered discrete, whereas controlling a robot arm might be considered continuous), and single/multi agent (is the agent the only one in its environment?).</p>
<h1 id="section-1">13/9/17</h1>
<h2 id="search">Search</h2>
<p>Search is one of the first topics in AI, used in everything from theorem proving to robotic navigation. Search problems consist of a <strong>state space</strong> (the world we're searching within), a <strong>successor function</strong> (given a state, determines possible actions/next states and the cost of getting to those states), a <strong>start state</strong> (the state at the start of the search), and a <strong>goal test</strong> (determines whether we've reached the goal, and possibly also information about how close we are to the goal). A <strong>path</strong> is a sequence of states and operations. A solution is a path from the start state to a state that satisfies the goal test. An optimal solution is a solution of minimal cost.</p>
<p>Consider a pac-man AI. the state space would be the game map, the successor function would define actions like, moving left/right/up/down and how expensive those would be, the start state would be the initial game state, and the goal test would be whether we've gotten to the end of the level.</p>
<p>Overview of N-queens problem and why it's a search problem, as well as various other types of search problems. For our purposes, search problems won't have elements of chance, continuous states, adversaries, or partial observation - all of our problems will be deterministic, discrete, single-agent, and fully observable.</p>
<p>The world state includes every detail of the environment at a certain point in the search process. For something like pac-man, we might care about pac-man's position, the dots, and the ghosts, but pathfinding might only have current position in its state.</p>
<p>Search problems can be thought of as minimum-cost pathfinding on a graph of world states, so we can apply algorithms like breadth-first search and A*. The search tree is the tree made of possible paths within the graph of world states. The factors we care about in search algorithms are things like completeness (will it always find a solution?), optimality (does it always find the minimum cost solution?), time complexity, and space complexity. While DFS is light on memory and goes deep quickly (<span class="math inline">O(b^h)</span> time and <span class="math inline">O(h)</span> space, where <span class="math inline">b</span> is arity of tree and <span class="math inline">h</span> is tree height), cycles in the graph can result in infinite loops, so it's not complete. BFS is complete and can find shallower solutions quickly, but needs a lot of memory to store visited states (<span class="math inline">O(b^h)</span> space and time where <span class="math inline">b</span> is arity of tree and <span class="math inline">h</span> is tree height).</p>
<p>Instead, we can use <strong>iterative deepening search</strong> (IDA*), which is essentially a mix of both - do DFS with a small depth limit, then repeatedly increase the limit (usually linearly) and rerun DFS until we reach the goal. This is both memory-efficient and complete, since the depth limit prevents infinite loops. This sounds wasteful, but it isn't too bad in practice since the number of nodes we visit in each iteration tends to grow exponentially, so most nodes we visit will be in the later iterations.</p>
<p>Note that all three of these algorithms we've looked at so far are the same besides the order we visit nodes in - they're generic and aren't specialized for specific problems. We can do better in specific problems by taking advantage of their inherent structure.</p>
<h1 id="section-2">18/9/17</h1>
<p>Assignment 1 will be out this week.</p>
<p>Uninformed search is search that doesn't depend on any knowledge about the problem, like BFS or DFS, or iterative deepening. It's widely suitable, but performs poorly and is computationally expensive. We would like to do informed search instead, where we can take advantage of knowledge we have about the problem to make our searches more efficient.</p>
<p>Usually when searching a graph, we often have knowledge about the <strong>merit</strong> of a node - some approximation of how good the node is. There are often different ways to defined merit for any given problem, such as how much it would cost to use the node in our solution, or how computationally expensive it would be to use it. For example, uninformed search expands nodes based on the distance from the start node, but we might augment this by using a heuristic like Euclidean distance to the goal, if we're searching in a Euclidean space.</p>
<p>A <strong>heuristic</strong> estimates the distance from any given node to the goal node, such as Euclidean or Manhattan distance. In our search, if node A has a lower heuristic than node B, we might choose node A over B (and also factor in things like how far from the start nodes A or B are).</p>
<p>Best-first search is the most naive informed search. At each step, we expand exactly the node with the lowest heuristic function value - the node that seems closest to the goal function. This is not a complete search, since it might get stuck in a minimim-heuristic cycle of nodes (we don't keep track of which nodes we've already visited). Additionally, this is not optimal, since the resulting path isn't always the shortest one - the length of the path taken so far at any given node isn't taken into account.</p>
<p>While best-first search looked at the estimated forward cost (expand node with lowest estimated cost from current node to goal), breadth-first search looked at the backward cost (expand node with lowest cost from start to current node). We want to combine these two to make breadth-first search more efficient, and this gives us <strong>A* search</strong>. Essentially, let <span class="math inline">g(n)</span> be the backward cost for the node <span class="math inline">n</span> and <span class="math inline">h(n)</span> be an estimate of the forward cost for the node <span class="math inline">n</span>. We then perform essentially breadth-first search, repeatedly queueing up each visited nodes' neighbors, but always choose the node in the priority queue with the lowest value of <span class="math inline">f(n) = g(n) + h(n)</span>, stopping when the priority queue is empty (the priority queue starts off only having the start node).</p>
<p>A* is complete - it will give us a valid path for any start/goal vertex in any graph. A* is also optimal if the heuristic is <strong>admissible and we're searching in a tree</strong>, or if the heuristic is <strong>consistent</strong>. Admissible means that the forward cost heuristic's value is always less or equal to the true forward cost. For example, <span class="math inline">h(n) = 0</span> is an admissable heuristic for any graph. Proof of optimality:</p>
<blockquote>
<p>Let <span class="math inline">s</span> be the start node and <span class="math inline">e</span> be the goal node. Suppose A* gives us a non-optimal solution within a tree. Since it's a tree, we must have reached a different ending node, so there must be a non-optimal ending node <span class="math inline">e&#39;</span>.<br />
Let <span class="math inline">n</span> be some node along the path from <span class="math inline">s</span> to <span class="math inline">e</span>. Clearly, at some point both <span class="math inline">n</span> and <span class="math inline">g&#39;</span> must be in the A* priority queue at the same time, and A* must have removed <span class="math inline">e&#39;</span> over <span class="math inline">n</span>.<br />
So <span class="math inline">f(e&#39;) \le f(n)</span> and <span class="math inline">g(e&#39;) + h(e&#39;) \le g(n) + h(n)</span>. Let <span class="math inline">c(x, y)</span> be the true cost from node <span class="math inline">x</span> to <span class="math inline">y</span>. Since the path is non-optimal, <span class="math inline">g(e&#39;) &gt; g(e)</span>. Clearly, <span class="math inline">g(e) = c(s, n) + c(n, e) \ge g(n) + h(n) = f(n)</span>.<br />
So <span class="math inline">f(e&#39;) &gt; f(n)</span>, which isn't possible since <span class="math inline">f(e&#39;) \le f(n)</span>. So A* cannot have given us a non-optimal solution within a tree.</p>
</blockquote>
<p>Consistent means that the heuristic is admissible regardless of the goal node - <span class="math inline">h(n) \le c(n, n&#39;) + h(n&#39;)</span>, where <span class="math inline">c(x, y)</span> is the true cost of getting from node <span class="math inline">x</span> to node <span class="math inline">y</span> - sort of analogous to the triangle inequality. Most admissible heuristics will also be consistent, but not all of them.</p>
<p>A* has good performance in practice, but in the worst case needs to visit every node, and keep track of all the backtracking information for every node. The real-world performance of A* depends heavily on the heuristic function, but it's generally a lot better than plain breadth-first search.</p>
<p>For example, if we were to use A* to solve a sliding puzzle game (a 3x3 grid of squares with one tile missing, squares must be slid into a particular pattern), one heuristic we might use is to count the number of moves to get each tile to its proper place, assuming the tiles can occupy squares regardless of whether there's already a tile there - the sum of the Manhattan distances between each node to its destination. A somewhat worse heuristic would be to count the number of tiles not in their correct places.</p>
<p>To design a heuristic, we generally start by removing some constraints of the problem to get a relaxed version of the problem, finding a computationally cheap solution to the relaxed version (or precompute a database of solutions for relaxed problem instances), and then use that as the heuristic. For example, for the sliding puzzle game, we started by relaxing the requirements that there only be one tile per square, and then trivially found a solution by just moving the tiles to their correct places. There's often a big tradeoff between the cost of computing the heuristic, versus how good the heuristic is - finding a good relaxation of the problem is a bit of an art.</p>
<p>We often want to use A* while also limiting how much memory we use. One way to do this is using Iterative Deepening A*, which is sort of like iterative deepening search, but we consider nodes by order of their <span class="math inline">f(n)</span> value, limit the value of <span class="math inline">f(n)</span>, and progressively rerun the search with increasing <span class="math inline">f(n)</span> limit until the goal is reached. Another way is simplified memory-bounded A*, which simply starts dropping nodes from the priority queue with the highest <span class="math inline">f(n)</span> values (or oldest nodes, as a tiebreaker) when it's out of memory (this is still optimal/complete if distance to shallowest goal node is less or equal to memory size).</p>
<h1 id="section-3">20/9/17</h1>
<p>Assignment 1 due October 4.</p>
<h2 id="constraint-satisfaction">Constraint Satisfaction</h2>
<p>Informed search uses domain specific knowledge to make search faster. Constraint satisfaction problems (CSPs) use knowledge about the current state itself to help us find solutions. In a standard search problem the states are black boxes, with a goal test and successor function to deal with them. A constraint satisfaction problem has states defined by variables <span class="math inline">X_1, \ldots, X_n</span> whose values are from a domain <span class="math inline">D</span>, and the goal test is a set of constraints that tell us about what values a subset of the variables <span class="math inline">S \subseteq \set{X_1, \ldots, X_n}</span> must take on. We can then exploit knowledge of these constraints to improve our performance.</p>
<p>Consider the map coloring problem: coloring a map such that no neighboring regions have the same color. Here, the constraints are that neighboring regions cannot have the same color, <span class="math inline">X_1, \ldots, X_n</span> is the color of each region, and the domain might be <span class="math inline">D = \set{\text{red}, \text{green}, \text{blue}}</span>. We want to find a solution, which is an assignment of values to <span class="math inline">X_1, \ldots, X_n</span> satisfying all the constraints. Another famous constraint solving problem is 3-SAT, which tries to find boolean values that satisfy a set of constraints in a particular form.</p>
<p>We distinguish betwen finite-domain constraint satisfaction problems like 3-SAT (variables have finite possible values), and infinite-domain ones like linear systems (variables have infinite possible values). We also care about the arity of the constraints - for example, binary constraints operate over two variables, ternary over three, and so on. There's also soft constraints, which don't have to be satisfied but are preferred if possible - this forms a constrained optimization problem.</p>
<p>We can use standard search algorithms to solve these binary constraint satisfaction problems. For example, for the map coloring problem, all the constraints are binary, so we can perform a search - states in the graph are partial assignments of colors, the search graph is the map's graph, the initial state is the uncolored map, the successor function colors one currently uncolored region, and the goal is that all colors are assigned and no two adjacent regions have the same color.</p>
<p>In general, we can solve a CSP using search: states are partial assignments of variables, initial state is that no variables are assigned, successor function assigns a value to a variable that isn't currently assigned, and the goal is that all variables are assigned and all constraints are satisfied.</p>
<p>CSPs are commutative - the order that we assign values to variables doesn't matter. This can be used to significantly reduce our search space - at each node in the search tree, we only have to consider the value of a single variable, not the values of variables we looked at before it.</p>
<p>Backtracking search is the main way to solve general CSPs: select an unassigned variable <span class="math inline">X_i</span>, try every possible value of <span class="math inline">X_i</span> to make sure they satisfy constraints. If there's a value <span class="math inline">v</span> that does, set <span class="math inline">X_i = v</span> and repeat the whole process, and if there is no such <span class="math inline">v</span>, go back to the preceding variable <span class="math inline">X_j</span> and keep trying more values of <span class="math inline">X_j</span>.</p>
<p>Backtracking is basically the same as DFS, but there are a lot of ways to improve on it:</p>
<ul>
<li>Ordering: choose unassigned variables and variable values in a specific order using heuristics. This can help us avoid a lot of bad branches, if we look at promising branches first.
<ul>
<li>A common way to choose variables is to choose the one with the fewest remaining possible values (<strong>most constrained variable/minimum remaining values</strong> heuristic), and as a tiebreaker, select the one referenced by the largest number of unsatisfied constraints (<strong>most constraining variable</strong>). This helps us eliminate bad previous choices as fast as possible, because anything that will eliminate all possible variable values gets detected more quickly.</li>
<li>A common way to choose values for a variable is to choose the one that rules out the fewest values in the remaining variables (<strong>least-constraining value heuristic</strong>) - the value that allows variables that will be chosen next to have the largest number of values. In other words, we try different values of the variable, and pick the one that reduces the sum of the remaining variables' possible values the least.</li>
</ul></li>
<li>Filtering: detect when the state we're currently in definitely won't lead to a solution if we continue searching, so we can immediately backtrack.
<ul>
<li>Forward checking: keep track of remaining possible values of unassigned variables, and when any variable runs out of possible values, backtrack, since we know we won't find an assignment if we proceed from the current state.</li>
<li>Arc consistency: given two nodes in a search digraph <span class="math inline">A, B</span> with domains <span class="math inline">D_A, D_B</span> respectively, an arc <span class="math inline">A \to B</span> is <strong>consistent</strong> if and only if for every variable value <span class="math inline">x \in D_A</span>, there exists a variable value <span class="math inline">y \in D_B</span> such that <span class="math inline">x</span> and <span class="math inline">y</span> are consistent (don't violate any constraints when in a partial assignment together).</li>
</ul></li>
</ul>
<p>;wip: slides</p>
<p>Also, it turns out that if the digraph is a tree, then we can solve it in <span class="math inline">O(nd^2)</span>, where <span class="math inline">n</span> is the number of nodes and <span class="math inline">d</span> is the number of values in the domain. To do this, we use topological sort to get an ordering such that parents occur before their children.</p>
<p>;wip: slides</p>
<p>;wip: tree widths of 0 and 1 are still trees, but the larger the tree width, the less like a tree it is - the larger the width, the less tree-like it is, and worse the decomposition approach will work, also finding optimal tree decomposition is NP-hard</p>
<h1 id="section-4">25/9/17</h1>
<h2 id="local-searchoptimization">Local Search/Optimization</h2>
<p>So far we've looked at uninformed and informed search. Today we'll look at local search and optimization - searching in very large problems, where we're willing to settle for a non-optimal but workable solution. Local search is used when we can't use global search/optimization techniques like A* or linear programming.</p>
<p>As with the other search techniques so far, we have a domain, constraints, and a cost function <span class="math inline">V(P)</span>. We're trying to find a point <span class="math inline">P</span> in that domain that satisfies the constraints and has a low cost function value (not necessarily the lowest one, though).</p>
<p>Search algorithms so far have systemically explored the search graph, and the path is the solution, but in many cases we don't care about the path. For example, scheduling exams without conflicts (the thing we care about is the final exam schedule, not the path of partial schedules we used to arrive at a conflict-free schedule), or chip design (we care about routing wires around to minimize chip footprint, not which wires we should route first). Local search techniques take advantage of this to make larger problems feasible.</p>
<p>Local search problems tend to have a wide search tree and a cost function that tells us how good a given solution is. Many of these problems are NP-hard to find optimal solutions, but <strong>we just want feasible solutions that are good enough, even if they're not optimal</strong>.</p>
<p>For example, for N-queens, we might try to find a solution that removes a lot of the queens from attacking positions, rather than a solution that gets all of the queens out of attacking positions. When there are few constraints vs. variables, it's easy to solve - if we just throw the queens on the board somewhere, it's probably a valid solution. When there are lots of constraints vs. variables, it's easy to see it's infeasible - if there are no non-attacking positions left to put queens in. However, when there's roughly the same number of constraints and variables, there are some solutions, but they're hard to find - searching for any solutions becomes very computationally intensive. Local search problems tend to have these sorts of &quot;phase transitions&quot; - most problem instances tend to be really easy, but a certain smaller number of problem instances are really difficult.</p>
<h3 id="hill-climbing">Hill Climbing</h3>
<p>One way we commonly solve search problems is by iteratively improving a random initial state - a greedy <strong>hill climbing</strong> algorithm often also known as gradient descent. Essentially, we repeatedly move in the direction that would improve our current situation the most:</p>
<ol type="1">
<li>Run the following many times:
<ol type="1">
<li>Choose a random initial state <span class="math inline">S</span> with value <span class="math inline">V(S)</span>. This state isn't necessarily optimal or even feasible.</li>
<li>Generate a moveset <span class="math inline">\delta(S) = \set{S_1, \ldots, S_k}</span> - the set of all neighboring states.</li>
<li>Let <span class="math inline">S&#39; = \argmax_{S_i} V(S_i)</span> - the best possible next state.</li>
<li>If <span class="math inline">V(S&#39;) &gt; V(S)</span>, set <span class="math inline">S = S&#39;</span> and go back to step 2 (we might also limit the number of iterations here). Otherwise, return <span class="math inline">S</span> - we've found a maxima.</li>
</ol></li>
<li>Take the largest value of <span class="math inline">S</span> from all the iterations run in step 1.</li>
</ol>
<p>Hill climbing is easy to implement and light on memory, but is only complete or optimal when the problem space is convex - for non-convex problems, there can exist local maxima that are neither feasible nor optimal, and we might end up hitting those local maxima every iteration.</p>
<p>Also, there can be &quot;plateaus&quot; - regions where the cost function is constant, where we would get stuck. We can avoid this by allowing movement in directions that don't change our cost function (usually in the same direction as the previous step) - this lets us move through a plateau and hopefully find higher values on the other side.</p>
<h3 id="simulated-annealing">Simulated Annealing</h3>
<p>One way to improve hill climbing is to use <strong>simulated annealing</strong>. This is quite similar to hill climbing, but at each step, instead of always moving to the best neighboring state, we instead randomly choose a neighbor, and move to it with a probability based on how much better it is than the current solution, and the amount of steps we've taken:</p>
<ol type="1">
<li>Choose a random initial state <span class="math inline">S</span> with value <span class="math inline">V(S)</span>. This state isn't necessarily optimal or even feasible.</li>
<li>Repeat <span class="math inline">n</span> times:
<ol type="1">
<li>Generate a moveset <span class="math inline">\delta(S) = \set{S_1, \ldots, S_k}</span> - the set of all neighboring states.</li>
<li>Let <span class="math inline">S&#39;</span> be a random next state from <span class="math inline">\delta(S)</span>.</li>
<li>If <span class="math inline">V(S&#39;) &gt; V(S)</span>, let <span class="math inline">S = S&#39;</span>. Otherwise, let <span class="math inline">S = S&#39;</span> with probability <span class="math inline">\exp\left(\frac{V(S&#39;) - V(S)}{T}\right)</span> (otherwise, do nothing). Here, <span class="math inline">T</span> is the <strong>temperature</strong> - a function of the current iteration that starts high in the first iteration and gradually goes toward 0 as we get toward <span class="math inline">n</span> iterations (usually exponentially by multiplying by a constant <span class="math inline">0 &lt; \alpha &lt; 1</span> at each step).</li>
</ol></li>
<li>Take the value of <span class="math inline">S</span> as a solution.</li>
</ol>
<p>Note that in step 2.3, if the new step was better we would accept it every time, and if it was the same or worse there's a certain chance we accept it anyways. Note that the chance decreases when the next state is much worse (and increases when the next state is only a little worse), and decreases significantly as the temperature gets closer to 0.</p>
<p>So when <span class="math inline">T</span> is large, we're often choosing next states that might be better or worse (exploration stage), and as <span class="math inline">T</span> gets smaller, we're often choosing next states that are strictly better (exploitation phase). As it turns out, if we decrease <span class="math inline">T</span> slowly enough (if we run enough iterations), we are guaranteed to find an optimal solution.</p>
<h3 id="genetic-algorithms">Genetic Algorithms</h3>
<p>Genetic algorithms are sort of like an extension of simulated annealing, in the exploration stage. Genetic algorithms can be used when states have a &quot;fitness&quot; function <span class="math inline">f(S)</span>, it's possible to &quot;combine&quot; two states to form a new state <span class="math inline">c(S_1, S_2)</span>, and it's possible to mutate a state to get a slightly different state <span class="math inline">m(S)</span>. Essentially, we have a a population of initial states, and repeatedly combine high-fitness states and mutate them until we have an acceptable result:</p>
<ol type="1">
<li>Let <span class="math inline">P = {S_1, \ldots, S_n}</span> be a population of random initial states.</li>
<li>Repeat until we have a state <span class="math inline">S_i</span> with high-enough <span class="math inline">f(S_i)</span> value:
<ol type="1">
<li>Randomly choose two distinct states <span class="math inline">S_i, S_j</span> from <span class="math inline">P</span>, where the choice is weighted by each element's <span class="math inline">f</span> value.</li>
<li>Combine <span class="math inline">S_i</span> and <span class="math inline">S_j</span> to produce a child <span class="math inline">S&#39; = c(S_i, S_j)</span>.</li>
<li>With some small probability <span class="math inline">m</span>, mutate <span class="math inline">S&#39;</span> to get <span class="math inline">m(S&#39;)</span>. Also, in some variations of genetic algorithms, we instead mutate a randomly chosen state in <span class="math inline">P</span> instead.</li>
<li>Add <span class="math inline">S&#39;</span> to <span class="math inline">P</span>. In most variations, we also kick out the lowest-fitness <span class="math inline">S_i</span> once <span class="math inline">P</span> has reached a certain size.</li>
</ol></li>
<li>The solution is the highest-fitness <span class="math inline">S_i</span> in <span class="math inline">P</span>.</li>
</ol>
<p>States are often represented as binary strings. The crossover function <span class="math inline">c(S_1, S_2)</span> might then be something like &quot;generate a random bitmask <span class="math inline">m</span>, then <code>new_S = (S_1 &amp; m) | (S_2 &amp; ~m)</code>&quot;, and the mutation function might then be something like flipping a random bit.</p>
<p>Softmax is nice because it handles outliers well.</p>
<h1 id="section-5">27/9/17</h1>
<h2 id="adversarial-search">Adversarial Search</h2>
<p>We now introduce an adversary with conflicting goals. Usually this means a game. Games are well-studied in AI because they're easy to represent, aren't too easy, and are easy to evaluate performance on.</p>
<p>We classify games into categories like perfect vs. imperfect information, deterministic (fully controlled by players) vs. stochastic (element of change). As a search problem, a 2-player perfect information game has a state defined by the board state and whose turn it is, the successor function is the set of the results of every legal moves at a given state, the terminal/leaf states are states where players win/lose/draw, the heuristic is the player's estimated utility function. A solution to the game is a strategy that tells you how to move at every state in the game - a way to get to a goal node from every node in the tree.</p>
<p>This is a challenging problem because there's a malicious opponent, and we need to take that into account with our search. A <strong>max-player</strong> is trying to maximize its own utility, while a <strong>min-player</strong>, the opponent, is trying to minimize it. We're trying to find an optimal strategy - a strategy that guarantees that gives us an outcome that is as favourable as possible, given that the opponent is playing as good as possible (we won't necessarily always win). Note that this assumes an optimal opponent, so we might prefer different strategies when the other player isn't playing optimally.</p>
<p>In this course, we'll mostly focus on zero-sum games (games where one player winning means the other loses) with perfect information. In theory these are easy to solve given infinite time, and most of the challenge comes from making this practically fast.</p>
<p>Consider the centipede game: players take turns either incrementing a counter, or deciding to end the game, which lets them take all the value in the counter and win. Clearly, we want to take the counter to get the value, but we want to keep going for more turns to get more value when we decide to take it.</p>
<p>We can analyze this using <strong>minimax search</strong>. We represent the value of each node in a tree as <span class="math inline">\text{minimax}(s) = \begin{cases} \text{utility}(s) &amp;\text{if } s \text{ is a terminal state} \\ \max\set{\text{minimax}(s&#39;) : s&#39; \in \text{successors}(s)} &amp;\text{if } s \text{ is a max-node (our turn)} \\ \min\set{\text{minimax}(s&#39;) : s&#39; \in \text{successors}(s)} &amp;\text{if } s \text{ is a min-node (their turn)} \end{cases}</span> (this is also called the <strong>security value</strong>). Essentially, it represents the worst-case utility of the game starting from the current state, assuming both players are playing optimally (they're both minimaxing). Essentially, each player is minimizing the amount it can possibly lose by.</p>
<p>Minimax search is essentially DFS through this minimax tree. This isn't complete since it doesn't work if the game doesn't terminate, but it has pretty good space complexity.</p>
<p>In the centipede game, the course of action that minimizes the amount we lose by in the worst-case is to betray at the first possible turn. However, real people will generally play more than that. For chess, the branching factor was empirically determined to be around 35, and the tree depth is around 100, at least. This is impractical to search exhaustively.</p>
<p>For certain types of games, we can do <strong>alpha-beta pruning</strong>. Essentially, at each node <span class="math inline">\alpha</span> is the max-player's best (highest) utility guaranteed so far, and beta is the min-player's best (lowest) utility guaranteed so far (including sibling nodes we just visited). We then skip visiting nodes where <span class="math inline">\beta \le \alpha</span> - taking this move will make the current player worse off than before, because the min-player's guaranteed low score is no longer greater than the max-player's guaranteed high score, so a win or draw is guaranteed for the other player. In a minimax tree, this gives us the same result as an exhaustive DFS. Usually, the effect of alpha-beta pruning is to stop looping through a node's children when we've found a child that guarantees us a win.</p>
<p>Alpha-beta pruning lets us eliminate branches from consideration. If we look at branches in a game tree left to right, we'd update <span class="math inline">\alpha</span> and <span class="math inline">\beta</span> for each node as we traverse the tree, and alpha-beta prune branches as we move from left to right. Alpha-beta pruning in this case will tend to remove branches from the right hand sides of nodes that would give poor results.</p>
<p>This is very helpful - minimax search on chess can look 5 moves ahead, but alpha-beta pruning gives us enough performance to look up to 10 moves ahead! This still isn't good enough - alpha-beta pruning requires us to go all the way down to terminal nodes in order to get the alpha/beta values. We can supplement this using heuristics to estimate alpha and beta. A fast evaluation function gives the true utility for a terminal state, and the estimated expected utility for any other state. There's a lot of AI research in making a good evaluation function using expert knowledge and experience - one that makes good estimates of how much payoff we'll get from a given game state.</p>
<p>We also don't have to go down all the way to the terminal states, and cut off the search at certain thresholds. We'll usually want to cut off the search when we reach a certain depth, or when the state becomes relatively stable (not really leading towards a win/loss), though we'll often want to give <strong>singular extensions</strong> - additional depth for moves that seem particularly promising (this helps avoid some cases where we'll miss promising options just beyond the cutoff). The parameters for the cutoff would also depend on the opponent - a novice chess player might only need 5 move lookeahead, while a grandmaster might need even 14, with exhaustive search near the endgame and a good evaluation function.</p>
<p>For stochastic games, we treat chance itself as a player, so there's the max-player (us), the min-player (the opponent), and the chance player (the whims of nature), where the chance player makes its move with probabilities defined by the path in the tree, in between every max-player and min-player turns. We then construct an <strong>expectiminimax tree</strong> and consider it a search problem. The expectiminimax tree is the same as the minimax tree, but with another case: <span class="math inline">\text{expectiminimax}(s) = \begin{cases} \text{utility}(s) &amp;\text{if } s \text{ is a terminal state} \\ \max\set{\text{expectiminimax}(s&#39;) : s&#39; \in \text{successors}(s)} &amp;\text{if } s \text{ is a max-node (our turn)} \\ \min\set{\text{expectiminimax}(s&#39;) : s&#39; \in \text{successors}(s)} &amp;\text{if } s \text{ is a min-node (their turn)} \\ E(\set{\text{expectiminimax}(s&#39;) : s&#39; \in \text{successors}(s)}) &amp;\text{if } s \text{ is a chance-node (nature&#39;s turn)} \end{cases}</span>, where <span class="math inline">E(X)</span> is the expected value.</p>
<p>For Go, the branching factor is about 250, and the terminal states are usually up to 150 moves deep. For this we can use techniques like Monte-Carlo tree search.</p>
<h1 id="section-6">2/10/17</h1>
<h2 id="utility-theory">Utility Theory</h2>
<p>A <strong>preference ordering</strong> for an agent over a set of states <span class="math inline">S</span> is a weak ordering <span class="math inline">\succsim</span> of all the states in <span class="math inline">S</span> - a ranking <span class="math inline">S_1 \succsim S_2</span> that determines whether <span class="math inline">S_1</span> is <strong>at least as good as</strong> <span class="math inline">S_2</span>. Likewise, <span class="math inline">S_1 &gt; S_2</span> means that <span class="math inline">S_1</span> is <strong>strictly better than</strong> <span class="math inline">S_2</span>, and <span class="math inline">S_1 \sim S_2</span> means that <span class="math inline">S_1</span> is <strong>equally preferable</strong> to <span class="math inline">S_2</span>. By weak ordering, we mean that every state is ranked, but there may exist ties (unlike a total ordering, where there can exist ties).</p>
<p>For deterministic worlds, the consequences of an action is always the same, so we can know exactly what the state of the world will be after taking that action. For non-deterministic worlds, there's an element of chance for actions, so we can only know what the probability of each world state occurring will be after taking that action. We can represent a nondeterministic outcome as a <strong>lottery</strong> <span class="math inline">L = [p_1, S_1; \ldots; p_k, S_k]</span>, where <span class="math inline">p_i</span> is the probability of <span class="math inline">S_i</span> occurring. Essentially, this is a probability distribution over all outcomes.</p>
<p>Any preference ordering must obey certain properties. Given world states <span class="math inline">A, B, C</span>, the following must all be true:</p>
<ul>
<li>Orderability - one or more of the following must be true: <span class="math inline">A \succsim B</span>, <span class="math inline">B \succsim A</span>, or <span class="math inline">A \sim B</span>.</li>
<li>Transitivity - if <span class="math inline">A \succsim B</span> and <span class="math inline">B \succsim C</span>, then <span class="math inline">A \succsim C</span>.</li>
<li>Continuity - if <span class="math inline">A \succsim B \succsim C</span>, then there must exist a probability <span class="math inline">p</span> such that <span class="math inline">[p, A; 1 - p, C] \sim B</span> (there must exist a mixture of outcomes between <span class="math inline">A</span> and <span class="math inline">C</span> that is identically preferable to <span class="math inline">B</span>).</li>
<li>Substitutability - if <span class="math inline">A \sim B</span>, then there must exist a probability <span class="math inline">p</span> such that <span class="math inline">[p, A; 1 - p, C] \sim [p, B; 1 - p, C]</span>.</li>
<li>Monotonicity - if <span class="math inline">A \succsim B</span>, then for any probabilities <span class="math inline">p, q</span>, <span class="math inline">p \ge q</span> exactly when <span class="math inline">[p, A; 1 - p, B] \succsim [q, A; 1 - q, B]</span>.</li>
<li>Decomposability - <span class="math inline">[p, A; 1 - p, [q, B, 1 - q, C]] \sim [p, A; (1 - p)q, B; (1 - p)(1 - q), C]</span>.</li>
</ul>
<p>These conditions are required in order to maintain basic rationality requirements. For example, if we didn't have transitivity, then there could exist a <span class="math inline">A &gt; B &gt; C &gt; A</span>, so we could get arbitrarily-large amounts of utility by repeatedly taking actions to go from <span class="math inline">A</span> to <span class="math inline">C</span> to <span class="math inline">B</span> and to <span class="math inline">A</span> again.</p>
<p>A <strong>decision problem under certainty</strong> is a tuple <span class="math inline">\tup{D, S, f, \succsim}</span>, where <span class="math inline">D</span> is the set of possible decisions/actions to take, <span class="math inline">S</span> is the set of possible states/outcomes, <span class="math inline">f: D \to S</span> is a function mapping actions to their resulting world states (the outcome function), and <span class="math inline">\succsim</span> is a preference ordering over <span class="math inline">S</span>.</p>
<p>A <strong>solution to a decision problem under certainty</strong> is an action <span class="math inline">d^* \in D</span> such that <span class="math inline">f(d^*) \succsim f(d)</span> for any <span class="math inline">d \in D</span> - an action that is equally or more preferable than any other possible action.</p>
<p>When actions don't have deterministic outcomes, we can't easily compare outcomes directly, since we're representing them as lotteries. Instead, we want to quantify how much we prefer each world-state to each other world-state, so we can determine whether one lottery is preferable to another.</p>
<p>We do this by introducing a <strong>utility function</strong> <span class="math inline">U: S \to R</span> - a function that, given a state <span class="math inline">s \in S</span>, produces a real value that measures how much we prefer <span class="math inline">s</span> over some arbitrary reference state. Also, let's define <span class="math inline">\succsim_U</span> to be a preference ordering where <span class="math inline">s_1 \succsim_U s_2</span> exactly when <span class="math inline">U(s) \ge U(t)</span>. Now for a lottery <span class="math inline">l = [p_1, s_1; \ldots; p_k, s_k]</span>, we can define <span class="math inline">U(l) = \sum p_i U(s_i)</span> - the <strong>expected utility</strong> of a lottery is the expected value of the utility function over all the possible outcomes in the lottery.</p>
<p>When we have any nondeterministic outcomes (when we have lotteries as outcomes), we have a <strong>decision problem under uncertainty</strong>. We represent this as a tuple <span class="math inline">\tup{D, S, f, U}</span>, where <span class="math inline">D</span> and <span class="math inline">S</span> have the same meanings as for decision problems under certainty, <span class="math inline">f: D \to \Delta(S)</span> is a function mapping actions to lotteries over states (distributions over states), and <span class="math inline">U</span> is a utility function.</p>
<p>According to expected utility theory, a <strong>solution to a decision problem under uncertainty</strong> is an action <span class="math inline">d^* \in D</span> such that <span class="math inline">U(f(d^*)) \ge U(f(d))</span> for any <span class="math inline">d \in D</span> - the action whose resulting lottery maximizes expected utility.</p>
<p>Note that <span class="math inline">U</span> is invariant under positive affine transforms - <span class="math inline">d^* = \argmax_{d \in D} U(f(d)) = \argmax_{d \in D} aU(f(d)) + b</span> for any <span class="math inline">a &gt; 0, b</span>. That means we can multiply <span class="math inline">U</span> by a positive constant or add any constant to <span class="math inline">U</span>, and the set of solutions to any decision problem using <span class="math inline">U</span> would not change.</p>
<p>Decision problems are conceptually simple to define and solve. In practice, they are often hard to solve because <span class="math inline">S</span> is so large and it is hard to compute <span class="math inline">f</span>. Additionally, even if a single decision problem is feasible to solve, we often want to solve a sequence of decision problems to determine the best sequence of actions that we should take.</p>
<p>New notation: <span class="math inline">P_a(s_2 \mid s_1)</span> is the probability of moving to state <span class="math inline">s_2</span> given that we are taking action <span class="math inline">a</span> while at state <span class="math inline">s_1</span>.</p>
<p>For sequences of actions, we can draw a tree containing all possible sequences of actions - the root node is the initial state connected to &quot;chance&quot; nodes (like in a game tree), via edges labelled by each possible action at that state. Chance nodes then connect to outcomes of those actions, via edges labelled by the probability of that outcome occuring given that action. The outcomes then connect to their own chance nodes, and those to more outcomes, and so on. Any particular sequence of actions can then be thought of as moves downward through this tree, much like playing a game moves downward through its game tree.</p>
<p>By thinking of all possible sequences of actions as a tree, it's clear that the expected utility of a sequence is the expected utility of the <strong>final outcome distribution</strong> - a weighted average over the utilities of all the leaf nodes (which are states) that can possibly occur by taking our sequence of actions. For example, if we're playing tic-tac-toe, a sequence of actions that leads to losing might have very little utility, while a sequence that leads to winning might have a lot.</p>
<p>To compute this final outcome distribution, we can recurse through the tree:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> expected_utility(state, action_sequence, probabilities_for_action_at_sequence):
    <span class="co"># concrete outcomes have a real utility value</span>
    <span class="cf">if</span> is_real_state(state):
        <span class="cf">return</span> utility(state)

    <span class="co"># distributions of outcomes have an expected utility value</span>
    current_action, new_action_sequence <span class="op">=</span> action_sequence[<span class="dv">0</span>], action_sequence[<span class="dv">1</span>:]
    <span class="cf">return</span> <span class="bu">sum</span>(
        probability <span class="op">*</span> expected_utility(new_state, new_action_sequence, probabilities_for_action_at_sequence)
        <span class="cf">for</span> probability, new_state <span class="kw">in</span> probabilities_for_action_at_sequence
    )</code></pre></div>
<p>For nondeterministic decision problems, we usually don't want to explicitly plan out sequences of actions in advance, because after we perform the first few actions in the sequence, we have more information than we did before, and could potentially make a better decision than we originally planned. For example, suppose we originally planned to flip a coin (action 1), and then guess that it would land heads face-down (action 2), and the utility is 1 if we get it right and 0 if we don't. Clearly, this is the best we can plan to do since we don't know the outcome of the coin in advance. However, after performing action 1, we know the new state of the world, and can look at the face-up side to guess what the face-down side is.</p>
<p>We can generalize this into the idea of <strong>policies</strong> - a sequence of states and actions to take if our previous actions lead us to those states. For example, here's a policy for the coin guessing decision problem above: &quot;flip a coin (action 1); if the coin outcome is heads, then guess that tails is face-down (action 2a), if the coin outcome is tails, then guess that heads is face-down (action 2b)&quot;.</p>
<p>Let <span class="math inline">A</span> be the set of actions, <span class="math inline">O</span> be the set of outcomes. Then there are <span class="math inline">\abs{A}^k</span> possible plans consisting of <span class="math inline">k</span> actions, and <span class="math inline">(\abs{A}\abs{O})^k</span> possible policies.</p>
<p>A decision tree for a decision problem under uncertainty is a tree consisting of alternating layers of state nodes (also known as choice nodes, because we have to make choices at each state) and chance nodes, where the root is the initial state. Each final outcome/state <span class="math inline">s</span> has an associated utility value <span class="math inline">U(s)</span> (the leaf nodes). Using something like Minimax search, we can then define our utility function at every node, rather than just the leaf nodes:</p>
<ul>
<li>For a chance node <span class="math inline">c = [p_1, s_1; \ldots; p_k, s_k]</span> (possible outcomes and their probabilities of occurring), we take the weighted average to get <span class="math inline">U(c) = \sum p_i U(s_i)</span>.</li>
<li>For a choice node <span class="math inline">s = \set{a_1, \ldots, a_k}</span> (actions and their associated chance nodes), we take the expected utility of the action with the greatest expected utility to get <span class="math inline">U(s) = \max U(a_i)</span>.</li>
</ul>
<p>When implementing the above, we would usually compute expected utilities bottom-up, in a dynamic programming fashion. This allows us to compute the utility value of each node in the tree in <span class="math inline">O((\abs{A} \abs{O}))^d</span> time where <span class="math inline">d</span> is the depth of the tree, so it's in the same order as the number of possible policies. Note that the naive, top-down implementation that computes <span class="math inline">U(s)</span> for each node would instead take <span class="math inline">O(n^d m^{2d})</span>.</p>
<p>A policy essentially defines a decision for choice nodes in the tree. We can represent it as a mapping from states to actions. Note that because we took only the maximum-expected-utility action at each choice node as we were evaluating our utility function, the nodes under every other possible action will never be reached. Therefore, what action we assign there is more-or-less arbitrary. Two policies are <strong>implementationally indistinguishable</strong> if they assign the same actions to all choice nodes that are reachable from the start node, when applying either policy (though they may be different for unreachable ones).</p>
<p>Most decision problems aren't fully observable - we usually work around this by using heuristics. Additionally, representing outcome distributions often requires a huge number of states and probabilities - these are usually worked around by using decision networks or Bayes net representations, which we'll cover later on.</p>
<p>A decision problem is <strong>fully observable</strong> if and only if we know the initial state, and it's possible to know the outcome of every action (which state we ended up being in). For partially observable decision problems, we might only know the outcomes of some actions after taking them. If we have a partially observable decision problem, we can sometimes design the choices to move the unobservable outcomes into leaf nodes, where we won't need to observe their outcomes: for example, instead of a decision tree that decides whether to proceed as if a patient has malaria or the flu, and then decides whether to prescibe drug A or B in each case, we might have a decision tree that decides whether to prescribe drug A or B, and then decides whether to proceed as if the patient has malaria or the flu.</p>
<h1 id="section-7">4/10/17</h1>
<h2 id="markov-decision-processes">Markov Decision Processes</h2>
<p>;wip</p>
<p>;wip: missed due to interviews</p>
<h1 id="section-8">13/10/17</h1>
<p>Midterm next week, includes everything up to and including today's content, Markov Decision Processes. Assignment 2 is due week after that.</p>
<p>We usually apply a future discount to utility values - utility gains in the future are discounted proportionally to how far in the future they are. For example, getting 10 dollars now is worth more than getting 10 dollars in a year.</p>
<p>;wip: missed due to interviews</p>
<h1 id="section-9">16/10/17</h1>
<p>;wip: missed due to interviews</p>
<h1 id="section-10">18/10/17</h1>
<p>Midterm tonight.</p>
<p>;wip: missed due to interviews</p>
<h1 id="section-11">23/10/17</h1>
<p>Midterm average is 68 in this section.</p>
<p>Chain rule: <span class="math inline">P(A_1 \land \ldots \land A_n) = P(A_1 \mid A_2 \land \ldots \land A_n) P(A_2 \land \ldots \land A_n)</span>.</p>
<h2 id="bayesian-networks">Bayesian networks</h2>
<p>Suppose we have a variable <span class="math inline">C</span> representing whether you have a cough, <span class="math inline">F</span> for whether you have a fever, and <span class="math inline">I</span> for whether you have influenza. Clearly, <span class="math inline">C</span> and <span class="math inline">F</span> are conditionally independent of each other given that <span class="math inline">I</span> occurs - <span class="math inline">P(C \mid F \wedge I) = P(C \mid I)</span> and <span class="math inline">P(F \mid C \wedge I) = P(F \mid I)</span>. In other words, if we know that a person has influenza, we already can make a prediction about <span class="math inline">C</span> and <span class="math inline">I</span> - a cough depends on having the flu, not having a fever, and vice versa.</p>
<p>Since they're conditionally independent, <span class="math inline">P(C \land F \mid I) = P(C \mid I) P(F \mid I)</span>. Likewise, <span class="math inline">P(C \land F \land I) = P(C \mid F \wedge I) P(F \land I) = P(C \mid I) P(F \mid I) P(I)</span>. This is really useful, because each factor is easier to measure individually - <span class="math inline">P(C \mid I)</span>, for example, is just the fraction of flu patients that cough.</p>
<p>Note that the joint distribution for <span class="math inline">P(C \mid I) P(F \mid I) P(I)</span> has 7 possible values for each possible assignment of values to <span class="math inline">C, F, I</span> (minus one because you can compute the last value by using 1 minus the rest of the probabilities).</p>
<p>Note that the number of possibilities in the distribution grows exponentially with respect to the number of variables being joined. This makes it quickly infeasible to store the entire joint distribution in memory, or gather data for each of those possibilities.</p>
<p>Two random variables <span class="math inline">A, B</span> are <strong>independent</strong> if knowledge of <span class="math inline">A</span> doesn't change uncertainty in <span class="math inline">B</span>. So <span class="math inline">P(A \mid B) = P(A) = P(B \mid A)</span>, <span class="math inline">P(A \wedge B) = P(A) P(B)</span>, or in general, <span class="math inline">P(X_1 \wedge \ldots \wedge X_n) = \prod P(X_i)</span>.</p>
<p>Two random variables <span class="math inline">A, B</span> are <strong>conditionally independent given another random variable <span class="math inline">Z</span></strong> if and only if knowledge of <span class="math inline">A</span> doesn't influence <span class="math inline">P(B \mid Z)</span> and knowledge of <span class="math inline">B</span> doesn't influence <span class="math inline">P(A \mid Z)</span>. So <span class="math inline">P(A \mid B \wedge Z) = P(A \mid Z)</span> and <span class="math inline">P(B \mid A \wedge Z) = P(B \mid Z)</span>. In other words, conditional independence means that given <span class="math inline">Z</span>, <span class="math inline">A</span> and <span class="math inline">B</span> are independent.</p>
<p>If we have <span class="math inline">n</span> independent boolean variables, then we can specify their joint distribution with <span class="math inline">O(n)</span> memory rather than <span class="math inline">O(2^n)</span>. A similar optimization can be done for conditionally independent variables, and many things in the real world are conditionally independent. Bayes networks use this to concisely represent joint distributions.</p>
<p>Notation concerns: <span class="math inline">P(X)</span> is the distribution of the random variable <span class="math inline">X</span> - a function of <span class="math inline">x</span> that returns the probability that <span class="math inline">X = x</span>, denoted <span class="math inline">P(x)</span> or <span class="math inline">P(X = x)</span>. <span class="math inline">P(X \mid Y)</span> is a family of distributions of the random variable <span class="math inline">X</span> - a set of functions of <span class="math inline">x</span>, one for each value of <span class="math inline">y</span>.</p>
<p>If I wake up early (<span class="math inline">E</span>), then I'll likely be unhappy (<span class="math inline">U</span>), and if I'm unhappy, then I'll likely do poorly on an exam (<span class="math inline">X</span>). If we have knowledge about waking up early or being unhappy, then that affects our uncertainty about doing poorly on the exam, so doing poorly is dependent on both. So the joint probability distribution is <span class="math inline">P(E \land U \land X) = P(X \mid E \land U) P(U \mid E) P(E)</span>. However, if we simply know I'm unhappy, then knowing whether I woke up early or not doesn't affect my uncertainty about doing poorly on the exam, because it doesn't add any new information. So <span class="math inline">P(E \land U \land X) = P(X \mid U) P(U \mid E) P(E)</span>. Basically, we managed to change the exponential number of table entries into linear!</p>
<p>This can be generalized to any number of chained dependent variables - knowing one part of the chain (such as being unhappy) means that the later parts (such as doing poorly on an exam) no longer depends on previous parts (waking up early). So <span class="math inline">P(x) = \sum P(g \mid u_i) P(u_i) = \sum_i P(g \mid u_i) \sum_j P(u_i \mid e_j) P(e_j)</span>.</p>
<p>A <strong>Bayesian/belief/causal/probabilistic network</strong> is a graphical representation of direct dependencies between random variables. A Bayesian network over random variables <span class="math inline">X_1, \ldots, X_n</span> is a directed acyclic graph where nodes are variables and an arc <span class="math inline">X_i \to X_j</span> exists whenever <span class="math inline">X_j</span> directly depends on <span class="math inline">X_i</span> (representing conditional probability distributions).</p>
<p>As usual we have the set of parent nodes <span class="math inline">\operatorname{Par}(X_i)</span> (direct parents, not all ancestors), children, descendents, and ancestors for a node in a Bayesian network. We also have the <strong>family</strong> of a node, which is just the node plus its parents <span class="math inline">\set{X_i} \cup \operatorname{Par}(X_i)</span>.</p>
<p>The Bayesian network lets us easily find the joint distribution of any set of random variables. This works because given a node</p>
<p>;wip: all the way to constructing a BN</p>
<p>Bayesian networks are not unique for their variables - the order that we choose variables in while constructing it is very important, and some orders are better than others. Usually Bayesian networks are simpler when the variables are chosen by following causal intuitions - we want variables representing causes to precede variables representing effects.</p>
<p>;wip: all the way to but not including blocking</p>
<h1 id="section-12">25/10/17</h1>
<p>;wip</p>
<h1 id="section-13">30/10/17</h1>
<p>;wip</p>
<h1 id="section-14">1/11/17</h1>
<p>;wip</p>
<h1 id="section-15">6/11/17</h1>
<h2 id="multi-agent-systems">Multi-agent Systems</h2>
<p>So far, there has only been a single agent in our environments. Not only are we reasoning about our own agents, we're reasoning about how other agents are reasoning. Our goal is to model how agents behave in this setting, and design systems so they'll behave in a certain way.</p>
<p>We assume that all agents are self-interested - acting according to their own goals and on their own world states.</p>
<p>Game theory problems deal with analyzing how groups of <strong>rational</strong>, <strong>strategic</strong> agents <strong>interact</strong> with each other (a special case is the decision problem, where there is only one agent). Rational means that agents choose the best possible action they know of, strategic means that they take their actions into account in the wider context of the game, and interaction means that the actions of one agent will affect other agents.</p>
<p>A <strong>strategic/matrix-form/normal-form game</strong> is defned as a set of agents <span class="math inline">I = \set{1, \ldots, N}</span> with utility functions <span class="math inline">u_1, \ldots, u_n</span>, a set of possible actions for each player <span class="math inline">i</span> <span class="math inline">A_i = \set{a_i^1, \ldots, a_i^m}</span>, an outcome profile <span class="math inline">a = (a_1^{j_1}, \ldots, a_N^{j_N})</span> (a tuple of actions that ended up being chosen by the agents, representing a game state).</p>
<p>We can often represent these sorts of games as a payoff matrix. Here's an example for playing Chicken:</p>
<pre><code>                           Agent 2
                  | Turn Away | Continue  |
Agent 1 Turn Away | 10, 10    | 0, 20     |
         Continue | 20, 0     | 5, 5      |</code></pre>
<p>This is a variation on the prisoner's dilemma, where two people are driving toward each other. If they both turn away, they don't die and built rapport with each other. If one turns away, they are embarassed while the other looks brave. If they both continue, they both die but look brave. Agent 1 is usually called the <strong>row player</strong>, while Agent 2 is usually called the <strong>column player</strong>.</p>
<p>When written in this form, a zero-sum game is simply a game where <span class="math inline">\sum_{o \in \text{outcomes}} U(o) = 0</span>.</p>
<p>Let <span class="math inline">a_{-i}</span> represent the actions of every player other than <span class="math inline">i</span>. Let <span class="math inline">p_i(a_{-1})</span> represent what agent <span class="math inline">i</span> thinks all the other players will do. Then an agent <span class="math inline">i</span> is <strong>rational</strong> if it chooses actions according to <span class="math inline">\argmax_{a_i} u_i(a_i, p(a_{-i})) p(a_{-i})</span>.</p>
<p>;wip: apply belief about others' actions to prisoner's dilemma.</p>
<p>An action strictly dominates another action if it gives more utility regardless of what other agents do - if <span class="math inline">u_i(a_i&#39;, a_{-i}) &gt; u_i(a_i, a_{-i})</span> for all <span class="math inline">a_{-i}</span>, then <span class="math inline">a_i&#39;</span> strictly dominates <span class="math inline">a_i</span>. If an action is strictly dominated by another action, a rational agent will never play it. However, the converse isn't necessarily true, so this doesn't give us the full picture about what rational agents will do.</p>
<p>A <strong>Nash equilibrium</strong> is an action profile where no agent would want to change their action given that no other agents change their action. In other words, if we asked them after the fact whether they would have picked a different decision, every agent would say no. Formally, for every agent <span class="math inline">i</span> and a current action profile <span class="math inline">a^*</span>, for any possible action <span class="math inline">a_i&#39;</span>, <span class="math inline">u_i(a_i^*, a_{-i}^*) \ge u_i(a_i&#39;, a_{-i}^*)</span>.</p>
<p>A Nash equilibrium is essentially an outcome where no one player can take advantage of any others, even in hindsight.</p>
<p>For randomized games, we can't use the pure Nash equilibria, because we don't know what the outcome might be. Instead, we can analyze the <strong>mixed Nash equilibria</strong>, which is the same thing but with expected utilities instead of the known utilities, and strategies (probability distributions over actions) instead of actions.</p>
<p>For the prisoner's dilemma, there's only one Nash equilibrium at defect-defect, and interestingly, it turns out not to be the globally optimal outcome. At any other outcome, at least one of the players would rather have defected than cooperated.</p>
<p>Nash's theorem: every game with finite-sized action sets, has at least one mixed-strategy Nash equilibrium. The proof is non-constructive, however, and finding Nash equilibria remains very hard. 2-player zero-sum games can be solved in polynomial time with linear programming, while for arbitrary problems it's even harder than NP-hard, in PPAD.</p>
<p>Prisoner's dilemma changes once agents play against each other multiple rounds, because each agent gives information to the other agent through their history of cooperating/defecting. With history, the strategy becomes a lot more complex, because agents can reward/punish past behaviour, and consider things like reputation and trust.</p>
<p>Some common repeated prisoner's dilemma strategies are &quot;grim&quot;, where an agent cooperates until the first defect, and defects forever afterward, and &quot;tit-for-tat&quot;, where an agent starts off coorperating, and copies the opponent's action in the previous round.</p>
<p>While normal-form games have agents choose actions simultaneously, an <strong>extensive form game</strong> allows the order of actions to affect the outcome, such as games where players take turns. Minimax is a strategy designed for extensive-form games.</p>
<p>Extensive form games still have the set of agents, action sets, and utility functions from normal form games, but we have terminal and choice nodes instead of action profiles. Just like we looked at with minimax, extensive form games can be represented with a tree.</p>
<p>Every extensive form game can be transformed into a normal form game by having each agent simultaneously commit to playing certain policies. Those commitments can then be thought of as the agent actions, and the utility of the final game state can be thought of as the actions' utility values.</p>
<p>An interesting case of Nash equilibria behaving unusally is mutually assured destruction. Consider two nuclear-capable countries A and B. Missiles are on the way from A to B, and B has to decide whether to retaliate.</p>
<p>Subgame perfect equilibria for extensive form games are ;wip: MAD example</p>
<p>;wip: centipede game, nobody actually plays like</p>
<p>;wip: ultimatum game: I must split $10 with you, you say yes/no, if no then nobody gets the money. optimal is to always say yes, but make them think you'll say no</p>
<p>;wip: cultural aspects- wall street bankers always defect, rural fisherman always cooperate</p>
<h1 id="section-16">8/11/17</h1>
<h2 id="mechanism-design">Mechanism Design</h2>
<p>Mechanism design is the design of systems that make rational agents behave in a certain way. Instead of talking about agents given a game, we're talking about games given an agent - sort of like a reverse game theory.</p>
<p>We can represent mechanisms with something very similar to games - a set of agents <span class="math inline">I = \set{1, \ldots, N}</span> with utility functions <span class="math inline">u_1, \ldots, u_n</span>, a set of possible actions for each player <span class="math inline">i</span> <span class="math inline">A_i = \set{a_i^1, \ldots, a_i^m}</span>, an ;wip: update from slides? <span class="math inline">a = (a_1^{j_1}, \ldots, a_N^{j_N})</span> (a tuple of actions that ended up being chosen by the agents, representing a game state). Additionally, each agent has a <strong>type</strong> <span class="math inline">\theta_1, \ldots, \theta_N</span> that represents all of the agent's private information that might affect their utility function, and there's a <strong>social choice</strong> function <span class="math inline">f: \Theta_1 \times \ldots \times \Theta_N \to O</span> that reads the agents' minds and returns an outcome. ;wip: rather than action profile, we have outcomes</p>
<p>Social choice functions might include voting one candidate from a group of candidates, choosing which resources to assign to which agents, or whether to buy something as a group. This might depend on the types of all the agents, so the social choice function's value is unknown to any individual agent. We really want the social choice function, but it's not available within a game.</p>
<p>Since agents can't actually read each others' minds, we have to go through a trusted institution. A <strong>mechanism</strong> is a formal representation of this. A mechanism <span class="math inline">M</span> is an assignment of strategies to each agent <span class="math inline">S_1, \ldots, S_N</span>, and an outcome function <span class="math inline">g</span> ;wip: outcome function</p>
<p>Each agent submits a report to the mechanism, and the mechanism aggregates all those reports into an outcome using the outcome function. For example, for voting we might have ;wip.</p>
<p>A mechanism implements a social choice function <span class="math inline">f(\theta)</span> if there exists a Nash equilibrium <span class="math inline">s^* = \tup{s_1^*(\theta_1), \ldots, s_N^*(\theta_N)}</span>. ;wip: rest of slide</p>
<p>Even better is the direct mechanism, where <span class="math inline">S_i = \Theta_i</span> for every agent <span class="math inline">i</span>. If this is the case, then <span class="math inline">g(\theta) = f(\theta)</span> for all <span class="math inline">\theta \in \Theta_1 \times \ldots \times \Theta_N</span>. In other words, if the strategy space is the same as the ;wip</p>
<p>A direct mechanism is <strong>incentive compatible</strong> if and only if there's an equilibrium <span class="math inline">s^*</span> where <span class="math inline">s_i^*(\theta)</span> ;wip. For the voting example, it's incentive compatible whenever it's in every agent's best interest to just submit their voting preference (their type information) directly than .</p>
<p>A direct mechanism is <strong>strategy proof</strong> if it is incentive compatible ;wip</p>
<p>;wip: revelation principle slide: if it's implementable in dominant strategies, then it's directly</p>
<p>Gibbard-Satterwaite theorem - if : ;wip</p>
<ul>
<li><span class="math inline">O</span> is finite and has more than 2 options.</li>
<li>Each outcome in <span class="math inline">O</span> ;wip</li>
<li><span class="math inline">\theta</span> includes all possible strict orderings over <span class="math inline">O</span>.</li>
</ul>
<p>So we either we allow the voting system to not be strategy-proof, or we make the voting system dictatorial - there would exist an agent whose top-rated outcome becomes the outcome of the voting system. To get around this in practical situations, we can use a weaker definition of an equilibrium - we can design mechanisms where it can be NP-hard to figure out how to manipulate the voting system.</p>
<p>Alternatively, if the agent preferences follow certain patterns, we can still guarantee strategy-proofing. ;wip: single peaked and quasilinear preferences (quasilinear means that we introduce money, money is worth the same to everyone, and we can transfer utility between people just by sending money around).</p>
<p>;wip: groves mechanisms for quasilinear mechanisms, transferring money so everyone is happy but balancing that out with <span class="math inline">h_i(\theta_{-i})</span> such that the game still works. If we just set <span class="math inline">h_i</span> to 0, then we just equalized everyone's utility, as if we split all the utility equally. ;wip: is that true? groves mechanisms are efficient and strategy proof under the quasilinear assumption</p>
<p>;wip: vickrey-clarke-groves are groves mechanisms that defines a good h_i function. marginal contribution - how much happier/sadder did you make everyone else by participating in the game vs. not participating in the game? that is how much you are paid/getting paid (e.g., I made everyone at an auction sadder by winning and walking away with the prize).</p>
<p>;wip: Vickrey auction: we pay the lowest amount we could've won with, by paying the second highest bidder's bid. this eliminates overbiddin regret - &quot;I wish I bid a little lower, because I would have still won anyways&quot; - because if they paid any less than the second highest bidder's bid, they would've lost ;wip: show that this is a vickrey-clarke-groves mechanism according to the formula</p>
<p>;wip</p>
<h1 id="section-17">13/11/17</h1>
<h2 id="machine-learning">Machine Learning</h2>
<p>So far we've looked at classical approaches for AI - top-down, mathematically-backed approaches to problems like playing games. Real-world problems, however, are not as appropriate for these approaches because they're ill-defined/messy, and even when we can define them, there are often so many details that designing things top-down becomes impractical for humans to do. For example, just fully describing the problem that self-driving cars are solving would be impractical for any human to do - there's so many possible things that can happen that we can't list them all.</p>
<p>Machine learning is a closely related field to AI that deals with more unstructured, bottom-up approaches to solving these problems. For example, supervised learning (induction from pre-labelled data, like classifier algorithms), unsupervised learning (learning patterns in input, like clustering algorithms), and reinforcement learning (learning from feedback that comes later, like TD-learning and Q-learning). Others include semi-supervised learning, active learning (figure out what questions to ask the user), transfer learning (transferring knowledge into other models, such as higher-performance ones or in other domains), apprenticeship/inverse-reinforcement learning (learning from an existing expert, such as learning how a pilot flies a helicopter).</p>
<p>One big problem in ML is representation - what are we trying to learn? How do we represent the model's knowledge? Some common forms are linear polynomials, propositional logic, first-order logic, bayes nets, etc.</p>
<h3 id="supervised-machine-learning">Supervised machine learning</h3>
<p>Given a training set of examples <span class="math inline">\set{\tup{x_1, f(x_1)}, \ldots}</span>, a supervised learning algorithm will return a function <span class="math inline">h</span> that approximates <span class="math inline">f</span> well, even on unseen examples, known as a <strong>hypothesis</strong>. The <strong>hypothesis space</strong> is the set of all possible hypotheses that can be returned by the algorithm (e.g., all quadratic functions for a quadratic regression algorithm). This is a hard problem because good hypotheses are rare in most hypothesis spaces.</p>
<p>Two functions are <strong>consistent</strong> with a training set if they give the same value for points in the training set. There are often many possible hypotheses for a given training set, but we want one that will perform well on unseen samples. To approximate this, we use Ockham's razor - we want to</p>
<p>A <strong>realizable problem</strong> is a supervised learning problem where <span class="math inline">f</span> is inside the hypothesis space. We generally want our problems to be realizable because that makes it possible to get the actual answer, so we would want to expand our hypothesis space. However, we don't want to expand it too much because it makes hypothesis more complex and the algorithms less effective - it's a delicate balance (e.g., if we made the hypothesis space all Turing machines, it would be very difficult to learn anything about any input).</p>
<p>A <strong>loss function</strong> <span class="math inline">L(y, h(\vec x))</span> is a function that represents how off a hypothesis is from a true value, where <span class="math inline">y</span> is the true value and <span class="math inline">h(\vec x)</span> is the predicted value.</p>
<p>Linear threshold classifiers find a hyperplane <span class="math inline">h(\vec x) = \vec w \cdot \vec x \ge 0</span> to approximate an unknown function <span class="math inline">f(\vec x) \in \set{0, 1}</span>. Our loss function is usually the sum of squared errors in the training set. One way to learn a linear threshold classifier is to use gradient descent along the space of all hyperplanes.</p>
<p>Decision trees classify points by consecutively testing them with some fixed test at each node in the tree, and taking a corresponding branch in the tree. The leaf node that the point ends up at is the output class. ;wip: training decision trees</p>
<p>The <strong>entropy</strong> of a random variable <span class="math inline">V</span> is the information content of that random variable, defined as <span class="math inline">I(V) = -\sum P(V = v_i) \log_2(P(V = v_i))</span> (the proof for this is very much out of the scope of this course). The unit of entropy is bits - a coin flip has 1 bit of entropy, while a random ASCII character has 7 bits of entropy.</p>
<p>Uncertainty matters a lot depending on the problem. For example, if there's a 50% chance that A is true, and a 90% chance that B is true, it's worth more to know the answer to A than B, because it eliminates more uncertainty. In this case, A has 1 bit of entropy, and B has about 0.47, by the formula above.</p>
<h1 id="section-18">15/11/17</h1>
<p>For a decision tree, at each node we want to test on an attribute that gives us the most possible information gain. Ideally, the attribute would split all of the values into two roughly equal-sized sets, one of which is all in the positive class, and the other all in the negative class - this is the outcome that maximizes the entropy of making the decision.</p>
<p>Aside: a decision stump is a decision tree with only one node.</p>
<p>;wip: information gain slide, about choosing an attribute by measuring how much information testing that attribute could give us (information gain) - this is determined by computing the remaining entropy at each step and subtracting it from the previous remaining entropy</p>
<p>The entropy at a given node is <span class="math inline">I(\frac{p}{p + n}, \frac{n}{p + n}) = -\frac{p}{p + n} \log_2 \frac{p}{p + n} - \frac{n}{p + n} \log_2 \frac{n}{p + n}</span>. This has a maximum at <span class="math inline">n = p</span> with value 1 - there is 1 bit of entropy if the attribute test splits the values perfectly into two classes that are all in the positive class and all in the negative class, respectively.</p>
<p>;wip: how to compute information gain?</p>
<p><strong>Algorithm C4.5</strong>: when we introduce a node, we want to ask the question that will reduce the remaining entropy the most. We usually combine this with a greedy algorithm (repeatedly grow the tree wherever we can reduce remaining entropy the most, as long as we still have examples or attribute tests to try), which works pretty well in practice. The resulting decision tree will work for the data, but can often be different from the true, underlying decision tree that generated the data (generally, a simpler tree or a slightly more complex tree than the original). The output of this algorithm is a decision tree (the decision tree is a hypothesis).</p>
<p>We usually use the training-set/testing-set split to evaluate our hypothesis. The learning algorithm is good if the classifiers it generates will give good results on the unseen data in the testing set(data that the classifier has never had access to). Note that the fact that it needs to be unseen means that we must use a new testing set every time we're evaluating our hypotheses.</p>
<p>If a hypothesis performs well on the training set but poorly on the testing set, this usually means we were <strong>overfitting</strong> - making out hypothesis too dependent on extraneous details that only coincidentally happen to correlate (for classifiers, these are often spurious correlations). Formally, a hypothesis overfits a training set if there exists another hypothesis that does worse on the training points, but better on the entire distribution of points.</p>
<p>If we train decision trees naively, overfitting causes a 10%-25% loss in accuracy. How do we prevent decision trees from making these spurious correlations? One technique is to use pruning. We introduce the null hypothesis &quot;there is no pattern in this data&quot; (;wip: formulas from avoiding overfitting slide), and then compute the probability that a sample of size <span class="math inline">p + n</span> would have the result. We can then do a chi-squared test to figure out whether the null hypothesis is plausible. If the null hypothesis is plausible, we can then prune the corresponding decision tree node.</p>
<p>When a testing set isn't available, or we can't afford enough testing sets to test hypotheses as much as we need, we can use techniques like cross-validation, where we train on parts of the training set, and test on the remaining parts. The most common cross-validation technique is <span class="math inline">K</span>-fold cross-validation, where we split the training set into <span class="math inline">K</span> sets, then run <span class="math inline">K</span> experiments where each set is the validation set and the rest of the sets are combined to get the training set. Leave-one-out cross-validation is just <span class="math inline">n</span>-fold cross-validation where <span class="math inline">n</span> is the training set size.</p>
<p>So far we looked at algorithms that choose a single hypothesis from the hypothesis space. <strong>Ensemble methods</strong> choose a lot of hypotheses and then combine their predictions to make the final prediction. The idea is that individual hypotheses might be wrong, but they might have partial information that would make the majority less likely to be wrong - ensemble methods can help improve model performance. Additionally, ensembles can be used to expand the hypothesis space itself - for example, an ensemble of linear classifiers where positive predictions are the AND of all the linear classifiers can have arbitrary polyhedrons as their hypothesis, rather than a hyperplane.</p>
<p>One way to combine predictions in ensemble methods is <strong>bagging</strong> - all the hypotheses take a majority vote and the winner is the final prediction. If we pretend that each of <span class="math inline">n</span> hypothesis is wrong with probability <span class="math inline">p</span> independently (even though this doesn't really happen in real life), we get <span class="math inline">P(k \text{ hypotheses are wrong}) = {n \choose k} p^k (1 - p)^{n - k}</span>. We then sum up the wrong cases to get <span class="math inline">P(\text{ensemble is wrong}) = \sum_{k = \ceil{n / 2}}^n P(k \text{ hypotheses are wrong}) = \sum_{k = \ceil{n / 2}}^n {n \choose k} p^k (1 - p)^{n - k}</span>.</p>
<p>One way to improve this is to weight hypotheses differently, for example by reducing the weight of correlated hypotheses (because their outputs would be overrepresented in the output) and increase the weights of high-performing hypotheses. This is a technique called <strong>boosting</strong> - we weight each hypothesis, and weight each point in the training set. Whenever we test a single hypothesis on a single training set point, we increase the weight of that training set point and decrease the weight of that hypothesis if it was misclassified, and decrease the weight of the training set point and increase the weight of that hypothesis otherwise. The other hypotheses will then have to do better on those misclassified ones in order to perform well.</p>
<p>;wip: adaboost algorithm</p>
<p>AdaBoost is very powerful and often used in industry. It takes in a weak learning algorithms and returns a hypothesis that classifies with accuracy 100% as the size of the ensemble increases.</p>
<p>Boosting means we don't need to learn perfect hypotheses. It's also easy to implement, generalizes well to the testing set (reduces overfitting), and is widely applicable to any weak learning algorithm. This is used everywhere from sensor fusion to radio decoders.</p>
<h1 id="section-19">20/11/17</h1>
<p>Machine learning is essentially about approximating real-world functions, such as the temperature of a region, the next frame of a video, and so on. Real-world functions can be extremely complex, so we need to design our machine learning methods to handle arbitrary relationships, run efficiently, and avoid overfitting.</p>
<p>Neurons in the brain are cells that act like wires, memory, and processors, and the brain is a densely connected network of neurons. Neurons receive chemical signals from their soma, perform rudimentary functions on those incoming signals in the soma/body (such as &quot;10 or more signals received in last second&quot;), and then might fire a signal down its axon, where it is broadcast chemically via synapses. The function that occurs in the soma isn't very well understood, but the rest of the process is, more or less. Fundamentally neurons are really simple, and can approximate arbitrarily complex functions if we use enough of them.</p>
<p>Biological neurons are either firing or not firing - the information content is encoded in the frequency and timing of the firings. When the brain learns, the connection between synapses and other neurons' dendrites are chemically altered to be more or less efficient. Artificial neural networks are generally based on really early, simplified models, though biologists will often use more biologically accurate neural networks based on models from theoretical neuroscience.</p>
<p>A typical artificial neural network models each neural as follows: each neuron has a weight vector <span class="math inline">w</span>, and outputs a value <span class="math inline">g(\vec w \cdot \vec x)</span>, where <span class="math inline">\vec x</span> is the vector of values from some other neurons and <span class="math inline">g(v)</span> is the activation function (e.g., <span class="math inline">\tanh(v)</span> for hyperbolic tangent, <span class="math inline">\max(0, v)</span> for ReLU, <span class="math inline">\frac{1}{1 + \exp(v)}</span> for sigmoid, or <span class="math inline">\begin{cases} 1 &amp;\text{if} v \ge b \\ 0 &amp;\text{otherwise}\end{cases}</span> for threshold where <span class="math inline">b</span> is a constant).</p>
<p>Suppose we wanted the logical AND of two Boolean inputs <span class="math inline">x_1, x_2</span> using a single neuron with threshold activation. In other words, the neuron should fire if and only if <span class="math inline">x_1 = x_2 = 1</span>. We can write our goal as a system of inequalities <span class="math inline">w_1 + w_2 + b \ge 0, w_1 + b &lt; 0, w_2 + b &lt; 0, b &lt; 0</span>, one equation per truth table entry.</p>
<p>A feedforward network is a directed acyclic graph of neurons. A recurrent network is a directed cyclic graph of neurons. Feedforward networks are fully described by their structure and weights, while recurrent networks additionally have their previous neuron outputs as state - this makes them harder to train but capable of remembering past inputs.</p>
<p>Input units are neurons that get their inputs directly from the network inputs. Hidden units are neurons that get their inputs entirely from other neurons, and give their output entirely to other neurons. Output units are neurons that give their output directly to the network outputs.</p>
<p>A perceptron is a single-layer feedforward network, where all neurons get their inputs directly from the network inputs and their outputs are given directly to the network outputs, and all neurons use the threshold activation function. The error/cost/loss of a perceptron is <span class="math inline">E = \frac 1 2 \sum (y_i - g(\vec w \cdot \vec x_i))^2</span>, where <span class="math inline">\vec x</span> is the network inputs and <span class="math inline">g(v)</span> is the network outputs (the outputs of the activation function from each neuron). We use squared loss because we'd rather have lots of small errors than some large errors.</p>
<p>We train neural networks using local search techniques. The most commonly used one is gradient descent - we take our loss function <span class="math inline">E</span>, take its derivative with respect to the weights <span class="math inline">\Delta E = \frac{\dee E}{\dee \vec w}</span>, and then update those weights according to <span class="math inline">\vec w \leftarrow \vec w - \alpha \Delta E</span>, where <span class="math inline">\alpha</span> is the learning rate.</p>
<p>For example, for the perceptron we have <span class="math inline">\Delta E = \frac 1 2 \frac{\dee}{\dee \vec w} \sum (y_i - g(\vec x_i))^2 = \sum (y_i - g(\vec x_i))(-\frac{\dee g}{\dee v} (\vec w \cdot \vec x)) \vec x</span>.</p>
<h1 id="section-20">22/11/17</h1>
<p>;wip: catch up on this class, the one before statistical learning</p>
<h1 id="section-21">27/11/17</h1>
<h2 id="learning-probabilistic-models">Learning probabilistic models</h2>
<p>Recall that agents model uncertainty in the world and the utility gained from different action plans, and that Bayes nets are one way to represent agent knowledge.</p>
<p>Bayes nets grow exponentially. In the past, some Bayes nets were painstakingly constructed by hand, such as the Pathfinder network for diagnosing lymph node diseases. The main bottleneck here is acquiring knowledge from experts, which is very expensive.</p>
<p>In contrast, data is often a lot cheaper. What if we build models (such as Bayes nets) directly from data?</p>
<p>Suppose we have a bag of candy with an unknown ratio of two different flavours (the ratio is known to be either 25%, 50%, or 75%, however). After eating <span class="math inline">k</span> candies, can we estimate the flavour ratio? Can be predict the next candy's flavour?</p>
<p>With Bayesian learning, we then have three hypotheses: <span class="math inline">H_1</span> &quot;the flavour ratio is 25%&quot;, <span class="math inline">H_2</span> &quot;the flavour ratio is 50%&quot;, and <span class="math inline">H_3</span> &quot;the flavour ratio is 75%&quot;. When we take a piece of candy out, we get evidence <span class="math inline">d</span>, telling us a particular piece of candy has a particular flavour.</p>
<p>We also have our prior probabilities <span class="math inline">P(H_1), P(H_2), P(H_3)</span> for each of the hypotheses being true, any and all previous knowledge about the probability of each hypothesis being true. We also have the likelihood of seeing a particular piece of evidence <span class="math inline">d</span> given any hypothesis, <span class="math inline">P(d \mid H_1), P(d \mid H_2), P(d \mid H_3)</span>.</p>
<p>Suppose we took 10 candies out, and they were all the first flavour. If we assume the candy distributions are independent and identically distributed (this is roughly accurate for a large bag of candy), we can say that the likelihoods are <span class="math inline">P(d \mid H_1) = 0.25^{10}, P(d \mid H_2) = 0.5^{10}, P(d \mid H_3) = 0.75^{10}</span>. We then update our priors that each probability is correct using <span class="math inline">P(H_1 \mid d) = P(d \mid H_1) ;wip</span>.</p>
<p>Predictions are essentially weighted averages over all of the hypotheses, where the weight is how likely we think the hypothesis is to be true. The predicted hypothesis at any point is the one that has the highest <span class="math inline">P(H \mid d)</span> value so far. It's mathematically guaranteed to be the best guess we can make given the evidence available (it'll be correct more often than any other predictor). Not only that, but we can also perform regularization by adjusting priors - simply reduce the prior probability of each hypothesis by its complexity.</p>
<p>This is usually intractable if there are a lot of hypotheses. We can approximate this using MAP (maximum a posteriori estimation): instead of predicting based on the prediction of every hypothesis, we only predict based on the most probable hypothesis. In other words, <span class="math inline">h_{MAP} = \argmax_{H_i} P(H_i \mid d)</span> (the hypothesis that maximizes the probability that the evidence occurs), and then only updates/predicts based on that most likely hypothesis. MAP prediction is less accurate than Bayesian, but it's guaranteed to converge to the same answer as Bayesian learning given enough evidence. Additionally, we can still do regularization by reducing prior probabilities according to their hypothesis complexity.</p>
<p>However, MAP may still be intractable if the <span class="math inline">\argmax</span> can't be efficiently computed. ;wip: rest of MAP computation slide, and linearizing the values and then solving it using linear programming on the log likelihood function <span class="math inline">\ln P(H) + \sum_i \ln(P(d_i \mid H))</span>.</p>
<p>If we don't have any priors, we still need to assume something to start with. In this case, we usually use a <strong>uniform prior</strong>, as if every hypothesis is equally probable. If we use a uniform prior, then the only thing that matters is the evidence. In this case, this is called maximum likelihood learning - doing <span class="math inline">\argmax_{H_i} P(d \mid H_i)</span> rather than <span class="math inline">\argmax_{H_i} P(H_i) P(d \mid H_i)</span>, because every <span class="math inline">P(H_i)</span> is equal. This tends to overfit, since there's no priors to penalize complex hypotheses. Maximum likelihood learning is more tractable than MAP learning, because we don't need to keep track of priors anymore.</p>
<p>Maximum likelihood learning, and to a lesser extent MAP learning, tend to jump to conclusions quickly, because they take an idea and tend to run with it until it fails enough. However, both will eventually converge to the same answer as Bayesian learning.</p>
<p>For the candy example but allowing arbitrary ratios (rather than just the 3 ratios presented), the maximum likelihood learning gives the maximum likelihood estimator that says &quot;the true ratio of candies is most likely to be the actual ratio we've observed&quot;. We can compute this by constructing the likelihood function, then finding the value that maximizes this likelihood function.</p>
<p>These algorithms can be extended to arbitrary Bayes nets. One issue to be aware of is that if we don't see an instance of a particular class, we set that class's probability to 0, which causes a lot of problems for Bayes nets (zero probabilities eliminate entire trees in Bayes nets). We can get around this with Laplace smoothing - start counting from a small number instead of from 0. For example, if we're predicting English word frequencies in a text document, we're unlikely to see all English words, so we would predict 0 probability for any unseen words. With Laplace smoothing, we instead pretend as if we've seen every English word once, and then count starting from that.</p>
<p>A naive Bayes model is a special case of a Bayes net where we assume that every attribute is independently distributed. This is often not a valid assumption, but it works pretty well in practice. Naive Bayes nets require much fewer probabilities because all the conditional probabilities are equal to their corresponbding unconditional probabilities, of which there are much fewer.</p>
<h1 id="section-22">29/11/17</h1>
<p>Naive Bayes scales well (i.e., linearly) as the number of features increases, and performs really well in practice, often even in situations where the independence assumption doesn't hold very well. We often use it for things like text classification, where high performance is needed.</p>
<p>Common use of Naive Bayes include spam detection, plaigerism detection, and search engines. The information we're given is a set of documents, and the goal is to classify those documents into two classes (e.g., spam vs. not spam, plaigerized vs. not plaigerized, relevant vs. not relevant). In these situations it's called a bag-of-words model: for each document, we just stem words, remove stopwords, and perform a word count.</p>
<p>We then assume words are independent fo each other - order of words doesn't matter, co-occurrences of words, don't matter (bad assumption, but works decently in practice). Under this assumption, for a document <span class="math inline">d_i</span> and a class <span class="math inline">y</span>, the probability of <span class="math inline">d_i</span> being in <span class="math inline">y</span>, <span class="math inline">P(y \mid d_i)</span>, is the product of all the probabilities <span class="math inline">P(w_j \mid y) = \frac{\text{number of documents with class } y \text{ that contain the word } w_j}{\text{number of documents with class } y}</span> for each word <span class="math inline">w_j</span> in <span class="math inline">d_i</span>.</p>
<p>To avoid zero probabilities, we apply Laplace smoothing by initializing word counts to 1 instead of 0.</p>
<h2 id="expectation-maximization">Expectation Maximization</h2>
<p>So far we've had access to all the attributes in our data. In real life this is rarely the case. Consider a Bayes net for diagnosing diseases. The Bayes net can see symptons and suggest treatments, but it wouldn't be able to access the underlying diseases.</p>
<p>If we ignore hidden variables, the Bayes net would have to directly associate symptons with different treatments, which massively increases the size of the Bayes net and the number of probabilities we need. If we ignore problem instances with missing values, we might end up without any values (and probably a lot of bias as well). In real life, we need to be able to consider hidden variables.</p>
<p>In direct ML on Bayes nets, we find the maximum likelihood estimate <span class="math inline">h_{ML} = \argmax_h P(E \mid h) = \argmax_h \prod_i \mathrm{CPT}(V_i)</span> of the hidden variables given the evidence <span class="math inline">E</span>. Usually, we would solve this by introducing a log, so <span class="math inline">h_{ML} = \argmax_h \sum_i \ln \mathrm{CPT}(V_i)</span> - the linearized version can be maximized more easily.</p>
<p>When we have hidden variables <span class="math inline">Z</span>, we instead have <span class="math inline">h_{ML} = \sum_Z \argmax_h P(E, Z \mid h)</span>. We can't use the log trick here anymore, because of the sum. Instead, we use <strong>expectation maximization</strong> (EM): guess the values of the hidden variables (using the expected value), and then compute the new maximum likelihood hypothesis based on those guesses. We then repeat the process until we converge to an approximation of the true maximum likelihood hypothesis:</p>
<ol type="1">
<li>Make a guess for <span class="math inline">h_{ML}</span>, the maximum likelihood hypothesis.</li>
<li>Repeat until convergence:
<ol type="1">
<li>Based on <span class="math inline">h_{ML}</span>, compute the expected value of any hidden/missing variables.</li>
<li>Based on the expected value of those hidden/missing variables, compute a new <span class="math inline">h_{ML}</span>.</li>
</ol></li>
</ol>
<p>Formally, we can write this as <span class="math inline">h_{i + 1} = \argmax_h \sum_{Z} P(Z \mid h_i, e) \ln P(e, Z \mid h_i)</span>. In practice, we would be computing the expectation and the maximization steps separately, even though this formula shows them occuring together. ;wip: check this formula</p>
<p>Expectation maximization is nice because we can linearize <span class="math inline">P(e, Z \mid h)</span> (which is the product of bunch of variables) by moving the <span class="math inline">\ln</span> inside the product and turning it into a summation - this makes it a lot easier to compute.</p>
<p>Expectation maximization guarantees that <span class="math inline">P(e \mid h_{i + 1}) \ge P(e \mid h_i)</span>. ;wip: examples from slides</p>
<h1 id="section-23">4/12/17</h1>
<p>So far we've looked at what we can do with AI. Today we look at what we should do with AI.</p>
<ul>
<li>Autonomous vehicles:</li>
<li>Accessibility.</li>
<li>Trolley problem.</li>
<li>We have to solve the hardest problem first (mixed human/AI drivers on roads) before we can make it easier (AI drivers only).</li>
<li>Accounting for local conventions, even if those conventions are illegal (e.g., the Pittsburgh left, driving slightly above the speed limit).</li>
<li>Who's responsible for inevitable accidents?</li>
<li>Automation of employment:</li>
<li>Technical unemployment.</li>
<li>Self-driving cars could eliminate 3.5 million trucking jobs.</li>
<li>Militarization of AI:</li>
<li>UN &quot;lethal autonomous weapon systems&quot;.</li>
<li>Human in the loop systems, and future autonomous weapons skipping that step.</li>
<li>Social aspects of AI:</li>
<li>Explainability, transparency, bias in AI.</li>
<li>Training data encodes our current biases, for example predictive policing that's based on current policing practices, which would encode current policing biases into the model. Bias is introduced when biased people make different decisions when building models.</li>
<li>AI safety:</li>
<li>Avoid unintended behaviours.</li>
<li>Asimov's three laws - many of Asimov's stories are about how these simple, seemingly reasonable laws go wrong.</li>
<li>Humanity and AI.</li>
<li>Threats to privacy and human dignity:</li>
<li>AI can be used to perform mass surveillance, where it was previously infeasible for humans to read so many messages.</li>
<li>AI rights, strong AI.</li>
<li>Superintelligence.</li>
</ul>
<p>Tips for final exam:</p>
<ul>
<li>Reinforcement learning, multi-agent systems, bayes nets, hidden markov models, machine learning. Focus on multi-agent systems (both profs work on them for their research).</li>
<li>&quot;How would you use AI techniques to solve problem X?&quot;.</li>
<li>Same format as midterm, but focusing on post-midterm material.</li>
<li>Ask lead TA if you have questions.</li>
</ul>
<h1 id="section-24">17/12/17</h1>
<p>Notes from exam review session by yours truly.</p>
<ul>
<li>search:
<ul>
<li>algorithms: DFS, BFS, A<em>, IDA</em>, SMA*</li>
<li>completeness (always finds an answer)</li>
<li>optimality (always finds best answer)</li>
</ul></li>
<li>constraint satisfaction:
<ul>
<li>backtracking</li>
<li>ordering: most constrained variable (least remaining values), most constraining variable (most unsatisfied constraints references), least constraining value (precludes least number of other variable values)</li>
<li>filtering: forward checking, arc consistency</li>
</ul></li>
<li>local search: hill climbing, simulated annealing, genetic algorithms</li>
<li>adversarial search:
<ul>
<li>computing minimax utility bottom-up</li>
<li>minimax search (dfs through minimax tree, possibly where minimax utility values are estimated after a certain depth)</li>
<li>alpha-beta pruning (stop looping through the children of a node when we're guaranteed that one option is guaranteed win in minimax)</li>
<li>estimating minimax utility when the tree is too large to expand, singular extensions (keep expanding past depth limit for especially promising options)</li>
<li>for stochastic games: chance player and the expectiminimax tree (chance player takes average of all children weighted by their probabilities)</li>
</ul></li>
<li>utility theory:
<ul>
<li>preference ordering, lotteries (mapping from outcomes to probabilities of those outcomes; a distribution over outcomes)</li>
<li>decision problems under certainty/uncertainty: a set of actions <span class="math inline">D</span>, a set of action outcomes <span class="math inline">S</span>, a mapping from actions to outcomes/lotteries <span class="math inline">f</span>, and a utility function <span class="math inline">U</span></li>
<li>policies: mapping from outcomes to actions to take at those outcomes (these are better than sequences of actions because they allow us to incorporate new info available to us after taking each action)</li>
<li>computing policies from expected utility tree (bottom up, from leaf nodes using dynamic programming)</li>
</ul></li>
<li>markov chains:
<ul>
<li>markov property: distribution of next states depends only on current state, not how we got to current state</li>
<li>markov transition matrix: matrix <span class="math inline">P</span> where <span class="math inline">P_{ij}</span> is probability of going into state <span class="math inline">j</span> given that we're currently on state <span class="math inline">i</span>, rows and columns represent outcomes/states</li>
<li>markov problem: set of states <span class="math inline">S</span>, state rewards <span class="math inline">R</span>, time discount factor <span class="math inline">\gamma</span>, transition probability function (probability of going to state <span class="math inline">j</span> given that we're on state <span class="math inline">i</span> and just took action <span class="math inline">k</span>)</li>
<li>expected utility: reward at current state plus (expected utility of each possible next state, weighted by probability of reaching that state) multiplied by time discount factor</li>
<li>solving markov problems: goal is to find policy that maximizes expected utility</li>
<li>value iteration: repeatedly compute maximum expected discounted future reward for each action for 1 iteration, 2 iterations, etc. until reward value converges (dynamic programming)</li>
<li>bellman's equation: <span class="math inline">V^{t + 1}(s_i) = \max_k \set{r_i + \gamma \sum_j P_{ij}^k V^t(s_j)}</span> where <span class="math inline">P_{ij}^k</span> is probability of going to state <span class="math inline">j</span> from <span class="math inline">i</span> when taking action <span class="math inline">k</span> (this equation is suitable for solving wih dynamic programming, or linear system solver due to being linear equation)</li>
<li>partially-observable MDPs: state for MDP replaced by distribution over states called belief states, solutions are policies mapping belief states to actions</li>
</ul></li>
<li>reinforcement learning:
<ul>
<li>learning actions that maximize expected discounted reward, difficult because of time-delayed reward</li>
<li>problem formulation: states <span class="math inline">S</span>, actions <span class="math inline">A</span>, rewards <span class="math inline">R</span> (usually delayed from actions by multiple steps), time is usually discretized</li>
<li>goal is to find optimal policy, just like MDPs but we initially don't know the transition probability function or the rewards, can only execute actions and observe results later</li>
<li>model-based (learns transition probability function and rewards to compute policy) vs. model-free (learns policy directly)</li>
<li>passive (given a policy, evaluate it) vs. active (output actions, and given action results, output policy)</li>
<li>on-policy reinforcement learning (behaviour while training converges to the final learned policy) vs off-policy reinforcement learning (behaviour while training can always differ from final learned policy)</li>
<li>explore (try new unknown actions) vs. exploit (take current best action)</li>
<li>boltzmann exploration: choose best action with probability <span class="math inline">\frac{\exp(Q(s, a)/T)}{\sum_a \exp(Q(s, a)/T)}</span> and random action otherwise (sort of like lowering temperature in simulated annealing)</li>
<li>RL techniques:
<ul>
<li>direct estimation: run a policy many times, average utility values from each run to get expected utilities</li>
<li>adaptive dynamic programming: solve as per MDPs using bellman's equation - we always have <span class="math inline">r(s_i)</span>, and we can estimate <span class="math inline">P_{ij}^k</span> by the fraction of times performing action <span class="math inline">k</span> at state <span class="math inline">i</span> moved us to state <span class="math inline">j</span>, and with these we have enough data to perform the update</li>
<li>temporal difference: use observed transitions to update expected utilities as per Bellman's equation <span class="math inline">V^\pi(s_i) \leftarrow (1 - \alpha) V^\pi(s_i) + \alpha(r(s_i) + \gamma V^\pi(s_j))</span>, decrease <span class="math inline">\alpha</span> over time to ensure convergence, <span class="math inline">\pi</span> is the current policy</li>
<li>Q-learning: replace <span class="math inline">V^\pi(s_i)</span> with <span class="math inline">Q(s, a)</span> to get <span class="math inline">Q(s_i, a) = (1 - \alpha) Q(s_i, a) + \alpha(r(s_i) + \gamma \sum_{s_j} \max_{a&#39;} Q(s_j, a&#39;))</span> - current reward plus discounted expected Q value - this is like temporal difference except more suitable for implementation</li>
<li>SARSA is on-policy learning: policy maps each state to the action with highest Q value, initialize <span class="math inline">Q</span>, <span class="math inline">s</span>, <span class="math inline">a</span>, take action <span class="math inline">a</span> then record <span class="math inline">s&#39;</span> and choose <span class="math inline">a&#39;</span> from policy, update Q values per <span class="math inline">Q(s, a) \leftarrow Q(s, a) + \alpha(r + \gamma Q(s&#39;, a&#39;) - Q(s, a))</span>, set <span class="math inline">a = a&#39;, s = s&#39;</span></li>
<li>Q-learning is off-policy learning: at each step do arbitrary action <span class="math inline">a</span>, update <span class="math inline">Q</span> with same formula as SARSA, go to new state <span class="math inline">s = s&#39;</span> (with boltzmann exploration, this converges as we visit states infinitely often and temperature and learning rate go to 0)</li>
</ul></li>
</ul></li>
<li>bayes nets:
<ul>
<li>bayes rule: <span class="math inline">P(A \mid B) = P(B \mid A) P(A) / P(B)</span>, aka posterior = likelihood * prior / normalization_constant (for Bayesian inference, posterior is P(hypothesis evidence))</li>
<li>posterior joint distribution: table of <span class="math inline">P(Y \mid E_1, \ldots, E_n)</span> values where each dimension <span class="math inline">i</span> of the table is possible values of <span class="math inline">E_i</span></li>
<li>conditional independence: <span class="math inline">P(A \mid B, C) = P(A \mid C)</span> means <span class="math inline">A</span> and <span class="math inline">B</span> are conditionally independent under <span class="math inline">C</span> - knowing <span class="math inline">B</span> doesn't change our belief in <span class="math inline">A</span> if we already know <span class="math inline">C</span></li>
<li>independence: <span class="math inline">P(A \land B) = P(A) P(B)</span> means <span class="math inline">A</span> and <span class="math inline">B</span> are independent (this is used to make joint probability tables smaller, since we only need to know <span class="math inline">P(A)</span> and <span class="math inline">P(B)</span>, rather than every combination of values for <span class="math inline">P(A \land B)</span>)</li>
<li>bayes net: digraph where nodes are events and edges <span class="math inline">A \to B</span> are conditional dependence &quot;<span class="math inline">B</span> depends on the value of <span class="math inline">A</span>&quot; (each node is directly dependent only on the nodes that are directly connected into it!), also every node includes a conditional probability table P(the node first parent of this node, second parent of this node, ...)</li>
<li>sibling and cousin nodes are conditionally independent of each other given their shared ancestor, non-connected nodes are independent</li>
<li>nodes are conditionally independent of every non-decendent given its parents</li>
<li>markov blanket: nodes are conditionally independent of all other nodes given its parents, children, and children's parents</li>
<li>factor: function over a set of random variables, or a table for each combination of values of those variables, where last variable is conditional on rest (<span class="math inline">f(a, b, c, d)</span> is table for <span class="math inline">P(D \mid A, B, C)</span>)</li>
<li>a set of nodes <span class="math inline">E</span> d-separates nodes <span class="math inline">X</span> and <span class="math inline">Y</span> if removing every node in <span class="math inline">E</span> from the bayes net disconnects <span class="math inline">X</span> and <span class="math inline">Y</span> - <span class="math inline">X</span> and <span class="math inline">Y</span> are conditionally independent given <span class="math inline">E</span></li>
<li>factor rule: <span class="math inline">f(A, B) \times g(B, C)</span> is table where each row's value is the product of the corresponding rows in <span class="math inline">f</span> and <span class="math inline">g</span> - <span class="math inline">abc = f(a, b) g(b, c)</span></li>
<li>relevant nodes: given evidence (nodes whose values are known) and a query (nodes whose values we're trying to compute), the nodes that can affect the query node's value if we knew their values</li>
</ul></li>
<li>stochastic processes:
<ul>
<li>used to represent probabilistic inference over time, where distributions of variables can change over time</li>
<li>one-variable stochastic process can be represented with a bayes net where each node represents the variable at a certain time-slice and nodes are pointed to by all older nodes</li>
<li>stationary assumption: state probabilities don't change over time</li>
<li>markov assumption: state probabilities depend on a fixed number <span class="math inline">k</span> of past states (a <span class="math inline">k</span>-order markov process is one where state probabilities depend on the last <span class="math inline">k</span> states), this lets us specify the entire process using observations from a finite number of time slices</li>
<li>we generally can't directly observe our state (hidden states), sensors reduce uncertainty about current state</li>
</ul></li>
<li>hidden markov model (of order <span class="math inline">k</span>):
<ul>
<li>set of states <span class="math inline">S</span>, set of observations <span class="math inline">O</span>, transition model <span class="math inline">P(s_t \mid s_{t - 1}, \ldots, s_{t - k})</span>, observation model <span class="math inline">P(o_t \mid s_t)</span>, prior <span class="math inline">P(s_0)</span></li>
<li>example of first-order hidden markov model: <span class="math inline">S</span> is robot's coordinates, <span class="math inline">O</span> is distances to nearby objects, transition model is movements of robot (including uncertainty about where we'll end up), observation model is distance measurements (including uncertainly from sensor noise), model is trying to compute <span class="math inline">P(s_t \mid o_1, \ldots, o_t)</span> (position given all sensor measurements)</li>
<li>common tasks: monitoriing <span class="math inline">P(s_t \mid o_1, \ldots, o_t)</span> (e.g., robot localization), prediction <span class="math inline">P(s_t \mid o_1, \ldots, o_t)</span> (stock trading), hindsignt <span class="math inline">P(s_{t - k} \mid o_1, \ldots, o_t)</span> (crime scene investigation), explanation <span class="math inline">\argmax_{s_1, \ldots, s_t} P(s_1, \ldots, s_t \mid o_1, \ldots, o_t)</span> (speech recognition, autocorrect)</li>
<li>for all use cases, solved using variations of variable elimination over the corresponding bayes net, for explanation we can use viterbi algorithm</li>
<li>dynamic bayes nets: represent states/observations using multiple random variables, rather than just one state variable and one observation variable, which allows us to exploit conditional dependence for better time complexity</li>
<li>for non-markovian or non-stationary processes, just add state components until it's markovian and stationary again (e.g., if state depends on history length, add history length to the state tuples)</li>
</ul></li>
<li>decision networks:
<ul>
<li>networks consisting of random variable nodes (like in bayes nets, includes conditional probability tables, oval shape), choice nodes (where choices are made, based entirely on information available from parents, rectangle shape), and value nodes (contains utility of every combination of parents' values, diamond shape, usually only one in a network)</li>
<li>decision networks make decisions always in the same order and don't forget previously available info (parents of a decision are also parents of all subsequent decisions)</li>
<li>policy: a set of mappings from parent assignments (values of parent nodes) to decision values for each decision node</li>
<li>expected utility of policy: expected utility for all possible assignments of the random variables (average utility for those assignments weighted by probability of those assignments occurring)</li>
<li>optimal policy can be computed using dynamic programming, just like for bayes nets - at each decision node just pick the highest-expected-utility choice</li>
</ul></li>
<li>game theory:
<ul>
<li>a rational agent has beliefs about what other players will do, will perform the action that maximizes its own utility under those beliefs - we study how rational agents behave</li>
<li>normal form game: set of agents <span class="math inline">I</span>, set of actions <span class="math inline">A</span>, action profile/outcomes <span class="math inline">\tup{a_1, \ldots, a_n}</span> (decisions made by each player), outcome utility <span class="math inline">U</span></li>
<li>dominant strategy: a way of choosing actions that has better expected utility than any other strategy, regardless of other players' actions (necessary but insufficient condition for rational strategy)</li>
<li>nash equilibrium: an outcome profile <span class="math inline">\tup{a_1, \ldots, a_n}</span> such that no single agent would change their decision if they knew every other agent was going to choose what they did</li>
<li>mixed strategy: probability distribution over set of actions</li>
<li>strategy profile: tuple of probability distributions over set of actions, one entry per player</li>
<li>mixed nash equilibrium: a strategy profile such that no single agent would change their strategy profile if they knew every other agent was going to use the strategy profile they did</li>
<li>every game with a finite action set has a mixed nash equilibrium</li>
<li>we can find nash equilibria of two player zero sum games using linear programming, but is PPAD hard problem in general</li>
<li>extensive form game: set of agents <span class="math inline">I</span>, set of actions <span class="math inline">A</span>, choice nodes <span class="math inline">H</span> (with associated valid actions/resulting node, player), terminal nodes <span class="math inline">Z</span>, and utility function (this can be represented as a game tree)</li>
<li>extensive form game vs. normal form: normal form game is where players all play their moves at the same time, extensive form game is where they take turns, can be written as game tree</li>
<li>any extensive form game can be written as a normal form game: each player precommits to playing a certain sequence of games</li>
<li>subgame perfect equilibria: nash equilibria in extensive form game that are also nash equilibria in all subtrees (this exists in every finite extensive form game, find them by computing all equilibria bottom-up)</li>
</ul></li>
<li>mechanism design:
<ul>
<li>designing games to account for rational agents to make certain outcomes occur</li>
<li>mechanism: set of outcomes <span class="math inline">O</span>, set of agents <span class="math inline">N</span> (each with private information <span class="math inline">\theta_i</span> and utility function <span class="math inline">u_i(o, \theta)</span> and strategy space <span class="math inline">S_i</span>), outcome function <span class="math inline">g: S_1 \times \ldots \times S_n \to O</span></li>
<li>social choice function: <span class="math inline">f: \theta_1 \times \ldots \theta_n \to O</span> (e.g., accepts agent votes and outputs winning vote, accepts bids and outputs auction winner)</li>
<li>mechanism implements social choice function iff there exists a nash equilibrium such that <span class="math inline">g(s_1(\theta_1), \ldots) = f(\theta_1, \ldots)</span> (equilibria at social choice function's outcome given rational agents)</li>
<li>direct mechanism: each agent reports one of its possible private information values as its action</li>
<li>incentive compatible direct mechanism: direct mechanism where there's a nash equilibrium for every agent to tell the truth about its private information</li>
<li>strategy proof direct mechanism: incentive compatible direct mechanism where that nash equilibrium is equilibrium of the dominant strategy</li>
<li>revelation principle: if mechanism implements social choice function in dominant strategies, we can transform it into a strategy proof direct mechanism by moving the agents' transformation of <span class="math inline">\theta_i</span> into the mechanism itself</li>
<li>gibbard-satterthwaite theorem: under pretty common conditions, social cost functions can only be implemented if they're dictatorial (one agent controls the rest)</li>
<li>to get around having to be dictatorial, we design mechanisms to make it computationally hard to avoid social cost function outcomes rather than just relying on rationality, and restrict agent preferences</li>
<li>quasilinear preferences: outcomes are vote result and transfers of money, where money linearly influences utility</li>
<li>groves mechanisms: mechanisms for quasilinear preferences where money is used to pay agents who didn't get their top choice, and agents pay to get their top choice, results in efficient and strategy-proof mechanisms</li>
<li>vickrey mechanisms: special case of groves mechanisms, where equilibrium utility of each agent is how much utility they add to the system by being there vs. not being there (e.g., vickrey auction: highest bidder gets item by paying the second-highest bid value to system)</li>
</ul></li>
<li>machine learning:
<ul>
<li>inductive learning hypothesis: hypothesis that approximates training set well will also approximate testing set well</li>
<li>realizable target function: target function (true function that is being approximated) is contained in hypothesis space</li>
<li>binary decision trees can represent any boolean function</li>
<li>implementing decision trees: output mode of examples if no attributes left or all examples have same class, otherwise choose best attribute, partition examples along that attribute, and recurse on the two partitions and the remaining attributes</li>
<li>entropy: <span class="math inline">\sum_i -P(v_i) \log_2 P(v_i)</span>, for binary training set we have <span class="math inline">P(v_1) = \frac{p}{p + n}</span> and <span class="math inline">P(v_2) = \frac{n}{p + n}</span></li>
<li>choosing best attribute: pick the attribute that has higest information gain - lowers the sum of entropies in the two partitions the most</li>
<li>overfitting: finding patterns that aren't there (avoid this using regularization and cross validation)</li>
<li>bagging: train a bunch of models on training set (sampled with replacement) and take majority vote</li>
<li>boosting: each training set point has a weight, repeatedly train models on training set, but after each one, weight the misclassified examples higher so future classifiers will focus on getting those right more (e.g., adaboost)</li>
</ul></li>
<li>neural nets:
<ul>
<li>feedforward, recurrent</li>
<li>perceptrons: single-layer feedforward network with threshold activation, only one weights matrix, can only learn linear boundary, train by adding or subtracting <span class="math inline">\alpha \vec x_i</span> whenever <span class="math inline">\vec x_i</span> is misclassified, as appropriate</li>
<li>backpropagation: technique for training deeper networks, compute gradient of loss function with respect to weights, subtract gradient from weights to get weights that give lower loss function value</li>
<li>deep learning: neural networks with many layers (tends to overfit due to huge hypothesis space, hard to interpret, but can learn very complex hypotheses given a lot of data)</li>
<li>neural network with single hidden layer can approximate any function, but requires fewer neurons if we use more layers</li>
</ul></li>
<li>bayesian learning:
<ul>
<li><span class="math inline">P(x \mid d) = \sum_i P(x \mid h_i) P(h_i \mid d)</span>, as we get more values of <span class="math inline">d</span> some hypotheses will get better predictions and <span class="math inline">P(h_i \mid d)</span> will increase - prediction is prediction of every hypothesis, weighted by our belief in that hypothesis being true</li>
<li>optimal given prior: no other prediction will be correct more often (and prior can be used to regularize - complex hypotheses get lower prior)</li>
<li>true bayesian learning is usually intractable - instead we might use maximum a posteriori learning, where we only use prediction of the hypothesis with highest <span class="math inline">P(h_i \mid d)</span> value (this is less accurate but converges to same predictions with enough data, but can still be intractable)</li>
<li>maximum a posteriori learning: <span class="math inline">\argmax_h P(h \mid d) = \argmax_h P(h \land d) = \argmax_h P(h) P(d \mid h) = \argmax_h P(h) \prod P(d_i \mid h) = \argmax_h \ln P(h) + \sum \ln P(d_i \mid h)</span>, can then use linear programming to compute the argmax value</li>
<li>maximum likelihood learning: maximum a posteriori learning, but with uniform priors (all <span class="math inline">P(h)</span> are equal, so we just have <span class="math inline">\argmax_h \prod P(d_i \mid h)</span>), still converges to same values as MAP and Bayesian but less accurate before converging, and faster to compute because no need for priors</li>
<li>laplace smoothing: when we have observation <span class="math inline">\vec x</span> from multinomial distribution, we get maximum likelihood estimate for multinomial distribution means as <span class="math inline">\hat \theta = \frac{x_i}{n}</span>, but if we laplace smooth <span class="math inline">\vec x</span>, we get the estimate <span class="math inline">\hat \theta_i = \frac{x_i + \alpha}{n + \alpha d}</span></li>
<li>naive bayes model: assume all <span class="math inline">x_i</span> are independent of each other, <span class="math inline">P(C \mid x_1, \ldots, x_n)</span> is proportional to <span class="math inline">P(C) P(x_1 \mid C) \cdots P(x_n \mid C)</span>, then pick <span class="math inline">C</span> that maximizes <span class="math inline">P(C \mid x_1, \ldots, x_n)</span> (even though the assumption is rarely true, scales and performs well in practice)</li>
<li>naive bayes example: document classification, assume each word is independent, P(class_i document_i) is proportional to <span class="math inline">\prod P(word_i in document \mid class_i) / P(class_i)</span></li>
</ul></li>
<li>expectation maximization:
<ul>
<li>used when some attribute values missing or when learning directly is infeasible</li>
<li>guess maximum likelihood hypothesis, then repeatedly predict missing attributes values using hypothesis, and use those predicted values to compute new maximum likelihood hypothesis, repeat until hypothesis stops changing</li>
<li>formally, repeatedly compute: <span class="math inline">h_{i + 1} = \argmax_h \sum_Z P(Z \mid h_i, e) \ln P(Z \land e \mid h_i)</span> where <span class="math inline">Z</span> is missing attribute values and <span class="math inline">e</span> is known attribute values</li>
<li>monotonically increases <span class="math inline">P(e \mid h_i)</span> as <span class="math inline">i</span> increases</li>
</ul></li>
</ul>
<div class="status-banner" style="display: none; position: fixed; bottom: 0; left: 0; right: 0; text-align: center;">
    <div style="display: inline-block; padding: 0.8em 2em 0.5em 2em; background: black; color: white; font-size: 2em;">
        Rendering <svg xmlns="http://www.w3.org/2000/svg" height="1.4em" viewbox="0 0 1200 500" style="vertical-align: text-bottom"><title>LaTeX logo</title><g transform="matrix(45 0 0 45 40 40)" fill="white"><path d="M5.5 4.4C5.5 4.4 5.2 4.4 5.2 4.4 5.1 5.4 5 6.7 3.2 6.7 3.2 6.7 2.4 6.7 2.4 6.7 1.9 6.7 1.9 6.6 1.9 6.3 1.9 6.3 1.9 1 1.9 1 1.9 0.6 1.9 0.5 2.9 0.5 2.9 0.5 3.2 0.5 3.2 0.5 3.2 0.5 3.2 0.2 3.2 0.2 2.8 0.2 1.9 0.2 1.5 0.2 1.1 0.2 0.3 0.2 0 0.2 0 0.2 0 0.5 0 0.5 0 0.5 0.2 0.5 0.2 0.5 1 0.5 1 0.6 1 0.9 1 0.9 1 6.2 1 6.2 1 6.6 1 6.7 0.2 6.7 0.2 6.7 0 6.7 0 6.7 0 6.7 0 7 0 7 0 7 5.2 7 5.2 7 5.2 7 5.5 4.4 5.5 4.4z"/><path d="M5.3 0.2C5.3 0 5.2 0 5.1 0 5 0 4.9 0 4.9 0.2 4.9 0.2 3.3 4.2 3.3 4.2 3.2 4.4 3.1 4.7 2.5 4.7 2.5 4.7 2.5 5 2.5 5 2.5 5 4 5 4 5 4 5 4 4.7 4 4.7 3.7 4.7 3.5 4.6 3.5 4.4 3.5 4.3 3.5 4.3 3.6 4.2 3.6 4.2 3.9 3.4 3.9 3.4 3.9 3.4 5.9 3.4 5.9 3.4 5.9 3.4 6.3 4.4 6.3 4.4 6.3 4.4 6.3 4.5 6.3 4.5 6.3 4.7 5.9 4.7 5.8 4.7 5.8 4.7 5.8 5 5.8 5 5.8 5 7.7 5 7.7 5 7.7 5 7.7 4.7 7.7 4.7 7.7 4.7 7.6 4.7 7.6 4.7 7.1 4.7 7.1 4.7 7 4.5 7 4.5 5.3 0.2 5.3 0.2zM4.9 0.9C4.9 0.9 5.8 3.1 5.8 3.1 5.8 3.1 4 3.1 4 3.1 4 3.1 4.9 0.9 4.9 0.9z"/><path d="M13.3 0.2C13.3 0.2 7.2 0.2 7.2 0.2 7.2 0.2 7 2.5 7 2.5 7 2.5 7.3 2.5 7.3 2.5 7.4 0.9 7.6 0.5 9.1 0.5 9.3 0.5 9.5 0.5 9.6 0.6 9.8 0.6 9.8 0.7 9.8 0.9 9.8 0.9 9.8 6.2 9.8 6.2 9.8 6.5 9.8 6.7 8.8 6.7 8.8 6.7 8.4 6.7 8.4 6.7 8.4 6.7 8.4 7 8.4 7 8.8 6.9 9.8 6.9 10.3 6.9 10.7 6.9 11.7 6.9 12.2 7 12.2 7 12.2 6.7 12.2 6.7 12.2 6.7 11.8 6.7 11.8 6.7 10.7 6.7 10.7 6.5 10.7 6.2 10.7 6.2 10.7 0.9 10.7 0.9 10.7 0.7 10.7 0.6 10.9 0.6 11 0.5 11.3 0.5 11.5 0.5 13 0.5 13.1 0.9 13.2 2.5 13.2 2.5 13.5 2.5 13.5 2.5 13.5 2.5 13.3 0.2 13.3 0.2z"/><path d="M18.7 6.7C18.7 6.7 18.4 6.7 18.4 6.7 18.2 8.2 17.9 8.9 16.2 8.9 16.2 8.9 14.9 8.9 14.9 8.9 14.4 8.9 14.4 8.8 14.4 8.5 14.4 8.5 14.4 5.9 14.4 5.9 14.4 5.9 15.3 5.9 15.3 5.9 16.3 5.9 16.4 6.2 16.4 7 16.4 7 16.6 7 16.6 7 16.6 7 16.6 4.4 16.6 4.4 16.6 4.4 16.4 4.4 16.4 4.4 16.4 5.2 16.3 5.5 15.3 5.5 15.3 5.5 14.4 5.5 14.4 5.5 14.4 5.5 14.4 3.2 14.4 3.2 14.4 2.8 14.4 2.8 14.9 2.8 14.9 2.8 16.2 2.8 16.2 2.8 17.7 2.8 18 3.3 18.1 4.7 18.1 4.7 18.4 4.7 18.4 4.7 18.4 4.7 18.1 2.5 18.1 2.5 18.1 2.5 12.5 2.5 12.5 2.5 12.5 2.5 12.5 2.8 12.5 2.8 12.5 2.8 12.7 2.8 12.7 2.8 13.5 2.8 13.5 2.9 13.5 3.2 13.5 3.2 13.5 8.4 13.5 8.4 13.5 8.8 13.5 8.9 12.7 8.9 12.7 8.9 12.5 8.9 12.5 8.9 12.5 8.9 12.5 9.2 12.5 9.2 12.5 9.2 18.2 9.2 18.2 9.2 18.2 9.2 18.7 6.7 18.7 6.7z"/><path d="M21.7 3.1C21.7 3.1 23 1.1 23 1.1 23.3 0.8 23.6 0.5 24.5 0.5 24.5 0.5 24.5 0.2 24.5 0.2 24.5 0.2 22.1 0.2 22.1 0.2 22.1 0.2 22.1 0.5 22.1 0.5 22.5 0.5 22.7 0.7 22.7 0.9 22.7 1 22.7 1.1 22.6 1.2 22.6 1.2 21.5 2.8 21.5 2.8 21.5 2.8 20.2 0.9 20.2 0.9 20.2 0.9 20.1 0.8 20.1 0.8 20.1 0.7 20.4 0.5 20.8 0.5 20.8 0.5 20.8 0.2 20.8 0.2 20.4 0.2 19.7 0.2 19.3 0.2 19 0.2 18.4 0.2 18 0.2 18 0.2 18 0.5 18 0.5 18 0.5 18.2 0.5 18.2 0.5 18.8 0.5 19 0.5 19.2 0.8 19.2 0.8 21 3.6 21 3.6 21 3.6 19.4 6 19.4 6 19.2 6.2 18.9 6.7 17.9 6.7 17.9 6.7 17.9 7 17.9 7 17.9 7 20.3 7 20.3 7 20.3 7 20.3 6.7 20.3 6.7 19.8 6.7 19.7 6.4 19.7 6.2 19.7 6.1 19.7 6.1 19.8 6 19.8 6 21.2 3.9 21.2 3.9 21.2 3.9 22.8 6.3 22.8 6.3 22.8 6.3 22.8 6.3 22.8 6.4 22.8 6.5 22.6 6.7 22.2 6.7 22.2 6.7 22.2 7 22.2 7 22.5 6.9 23.2 6.9 23.6 6.9 24 6.9 24.5 7 24.9 7 24.9 7 24.9 6.7 24.9 6.7 24.9 6.7 24.7 6.7 24.7 6.7 24.2 6.7 24 6.6 23.8 6.3 23.8 6.3 21.7 3.1 21.7 3.1z"/></g></svg> math...
    </div>
</div>
<div class="license">
  <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-sa/4.0/80x15.png" /></a> This work by <a xmlns:cc="http://creativecommons.org/ns#" href="https://uberi.github.io/" property="cc:attributionName" rel="cc:attributionURL">Anthony Zhang</a> is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.
  Copyright 2013-2017 Anthony Zhang.
</div>
</body>
</html>
